<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="built-on" content="2024-08-16T16:47:12.180608"><title>Troubleshooting | hms-mirror</title><script type="application/json" id="virtual-toc-data">[{"id":"application-doesn-t-seem-to-be-making-progress","level":0,"title":"Application doesn\u0027t seem to be making progress","anchor":"#application-doesn-t-seem-to-be-making-progress"},{"id":"application-won-t-start-noclassdeffounderror","level":0,"title":"Application won\u0027t start NoClassDefFoundError","anchor":"#application-won-t-start-noclassdeffounderror"},{"id":"cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect","level":0,"title":"CDP Hive Standalone Driver for CDP 7.1.8 CHF x (Cummulative Hot Fix) won\u0027t connect","anchor":"#cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect"},{"id":"failed-avro-table-creation","level":0,"title":"Failed AVRO Table Creation","anchor":"#failed-avro-table-creation"},{"id":"table-processing-completed-with-error","level":0,"title":"Table processing completed with ERROR.","anchor":"#table-processing-completed-with-error"},{"id":"connecting-to-hs2-via-kerberos","level":0,"title":"Connecting to HS2 via Kerberos","anchor":"#connecting-to-hs2-via-kerberos"},{"id":"auto-partition-discovery-not-working","level":0,"title":"Auto Partition Discovery not working","anchor":"#auto-partition-discovery-not-working"},{"id":"hive-sql-exception-hdfs-permissions-issues","level":0,"title":"Hive SQL Exception / HDFS Permissions Issues","anchor":"#hive-sql-exception-hdfs-permissions-issues"},{"id":"yarn-submission-stuck-in-accepted-phase","level":0,"title":"YARN Submission stuck in ACCEPTED phase","anchor":"#yarn-submission-stuck-in-accepted-phase"},{"id":"spark-dfs-access","level":0,"title":"Spark DFS Access","anchor":"#spark-dfs-access"},{"id":"permission-issues","level":0,"title":"Permission Issues","anchor":"#permission-issues"},{"id":"must-use-hiveinputformat-to-read-acid-tables","level":0,"title":"Must use HiveInputFormat to read ACID tables","anchor":"#must-use-hiveinputformat-to-read-acid-tables"},{"id":"acl-issues-across-cross-while-using-lower-clusters-storage","level":0,"title":"ACL issues across cross while using LOWER clusters storage","anchor":"#acl-issues-across-cross-while-using-lower-clusters-storage"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b408/app.css" rel="stylesheet"><meta name="msapplication-TileColor" content="#000000"><link rel="apple-touch-icon" sizes="180x180" href="https://jetbrains.com/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://jetbrains.com/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://jetbrains.com/favicon-16x16.png"><meta name="msapplication-TileImage" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-144x144.png"><meta name="msapplication-square70x70logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-70x70.png"><meta name="msapplication-square150x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x150.png"><meta name="msapplication-square310x310logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x310.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="Troubleshooting | hms-mirror"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="hms-mirror Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="writerside-documentation/hms-mirror/v2.2.0.x/troubleshooting.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="Troubleshooting | hms-mirror"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "writerside-documentation/hms-mirror/v2.2.0.x/troubleshooting.html#webpage",
    "url": "writerside-documentation/hms-mirror/v2.2.0.x/troubleshooting.html",
    "name": "Troubleshooting | hms-mirror",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "writerside-documentation/hms-mirror/#website",
    "url": "writerside-documentation/hms-mirror/",
    "name": "hms-mirror Help"
}</script><!-- End Schema.org --></head><body data-id="troubleshooting" data-main-title="Troubleshooting" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs=""><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>hms-mirror v2.2.0.x Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="troubleshooting" id="troubleshooting.md">Troubleshooting</h1><section class="chapter"><h2 id="application-doesn-t-seem-to-be-making-progress" data-toc="application-doesn-t-seem-to-be-making-progress">Application doesn't seem to be making progress</h2><p id="-3m9ty3_16">All the counters for table processing aren't moving (review the hms-mirror.log) or (1.6.1.0+) the on screen logging of what tables are being added and metadata collected for has stopped.</p><p id="-3m9ty3_17"><span class="control" id="-3m9ty3_22">Solution</span></p><p id="-3m9ty3_18">The application creates a pool of connection to the HiveServer2 instances on the LEFT and RIGHT to be used for processing. When the HiveServer2 doesn't support or doesn't have available the number of connections being requested from <code class="code" id="-3m9ty3_23">hms-mirror</code>, the application will 'wait' forever on getting the connections requested.</p><p id="-3m9ty3_19">Stop the application and lower the concurrency value to a value that can be supported.</p><div class="code-block" data-lang="yaml">
transfer:
  concurrency: 4
</div><p id="-3m9ty3_21">Or, you could modify the HiveServer2 instance to handle the number of connections being requested.</p></section><section class="chapter"><h2 id="application-won-t-start-noclassdeffounderror" data-toc="application-won-t-start-noclassdeffounderror">Application won't start <code class="code" id="-3m9ty3_30">NoClassDefFoundError</code></h2><p id="-3m9ty3_25">Error</p><div class="code-block" data-lang="none">
Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError:
java/sql/Driver at java.base/java.lang.ClassLoader.defineClass1
</div><p id="-3m9ty3_27"><code class="code" id="-3m9ty3_31">hms-mirror</code> uses a classloader to separate the various jdbc classes (and versions) used to manage migrations between two different clusters. The application also has a requirement to run on older platforms, so the most common denominator is Java 8. Our method of loading and separating these libraries doesn't work in Java 9+.</p><p id="-3m9ty3_28"><span class="control" id="-3m9ty3_32">Solution</span></p><p id="-3m9ty3_29">Please use Java 8 to run <code class="code" id="-3m9ty3_33">hms-mirror</code>.</p></section><section class="chapter"><h2 id="cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect" data-toc="cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect">CDP Hive Standalone Driver for CDP 7.1.8 CHF x (Cummulative Hot Fix) won't connect</h2><p id="-3m9ty3_34">If you are attempting to connect to a CDP 7.1.8 clusters Hive Server 2 with the CDP Hive Standalone Driver identified in the clusters <code class="code" id="-3m9ty3_44">jarFile</code> property, you may not be able to connect. A security item addressed in these drivers changed the required classes.</p><p id="-3m9ty3_35">If you see:</p><div class="code-block" data-lang="none">
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/log4j/Level
</div><p id="-3m9ty3_37">You will need to include additional jars in the <code class="code" id="-3m9ty3_45">jarFile</code> property. The following jars are required:</p><div class="code-block" data-lang="none">
log4j-1.2-api-2.18.0.ja
log4j-api-2.18.0.jar
log4j-core-2.18.0.jar
</div><p id="-3m9ty3_39">The feature enhancement that allows multiple jars to be specified in the <code class="code" id="-3m9ty3_46">jarFile</code> property is available in <code class="code" id="-3m9ty3_47">hms-mirror</code> 1.6.0.0 and later. See <a href="https://github.com/cloudera-labs/hms-mirror/issues/67" id="-3m9ty3_48" data-external="true" rel="noopener noreferrer">Issue #47</a></p><p id="-3m9ty3_40"><span class="control" id="-3m9ty3_49">Solution</span></p><p id="-3m9ty3_41">Using <code class="code" id="-3m9ty3_50">hms-mirror</code> v1.6.0.0 or later, specify the additional jars in the <code class="code" id="-3m9ty3_51">jarFile</code> property. For example: <code class="code" id="-3m9ty3_52">jarFile: &quot;&lt;absolute_path_to&gt;/hive-jdbc-3.1.3000.7.1.8.28-1-standalone.jar:&lt;absolute_path_to&gt;/log4j-1.2-api-2.18.0.jar:&lt;absolute_path_to&gt;/log4j-api-2.18.0.jar:&lt;absolute_path_to&gt;/log4j-core-2.18.0.jar&quot;</code></p><p id="-3m9ty3_42">These jar files can be found on the CDP edge node in <code class="code" id="-3m9ty3_53">/opt/cloudera/parcels/CDH/jars/</code>.</p><p id="-3m9ty3_43">Ensure that the standalone driver is list 'FIRST' in the <code class="code" id="-3m9ty3_54">jarFile</code> property.</p></section><section class="chapter"><h2 id="failed-avro-table-creation" data-toc="failed-avro-table-creation">Failed AVRO Table Creation</h2><div class="code-block" data-lang="none">
Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem: Unable to read schema from given path: /user/dstreev/test.avsc)
</div><p id="-3m9ty3_56"><span class="control" id="-3m9ty3_58">Solution</span></p><p id="-3m9ty3_57">Validate that the 'schema' file has been copied over to the new cluster. If that has been done, check the permissions. In a non-impersonation environment (doas=false), the <code class="code" id="-3m9ty3_59">hive</code> user must have access to the file.</p></section><section class="chapter"><h2 id="table-processing-completed-with-error" data-toc="table-processing-completed-with-error">Table processing completed with <code class="code" id="-3m9ty3_65">ERROR.</code></h2><p id="-3m9ty3_61">We make various checks as we perform the migrations, and when those checks don't pass, the result is an error.</p><p id="-3m9ty3_62"><span class="control" id="-3m9ty3_66">Solution</span></p><p id="-3m9ty3_63">In <a href="hms-mirror-tips.html" id="-3m9ty3_67" data-tooltip="This process can be a long-running process. It depends on how much you've asked it to do. Having the application terminated because the ssh session to the edgenode timed out and your computer went to sleep will be very disruptive.">tips</a> we suggest running with <code class="code" id="-3m9ty3_68">dry-run</code> first (default). This will catch the potential issues first, without taking a whole lot of time. Use this to remediate issues before executing.</p><p id="-3m9ty3_64">If the scenario that causes the <code class="code" id="-3m9ty3_69">ERROR</code> is known, a remediation summary will be in the output report under <span class="control" id="-3m9ty3_70">Issues</span> for that table. Follow those instructions, then rerun the process with <code class="code" id="-3m9ty3_71">--retry.</code> NOTE: <code class="code" id="-3m9ty3_72">--retry</code> is currently tech preview and not thoroughly tested.</p></section><section class="chapter"><h2 id="connecting-to-hs2-via-kerberos" data-toc="connecting-to-hs2-via-kerberos">Connecting to HS2 via Kerberos</h2><p id="-3m9ty3_73">Connecting to an HDP cluster running 2.6.5 with Binary protocol and Kerberos triggers an incompatibility issue: <code class="code" id="-3m9ty3_78">Unrecognized Hadoop major version number: 3.1.1.7.1....</code></p><p id="-3m9ty3_74"><span class="control" id="-3m9ty3_79">Solution</span></p><p id="-3m9ty3_75">The application is built with CDP libraries (excluding the Hive Standalone Driver). When <code class="code" id="-3m9ty3_80">Kerberos is the</code>auth` protocol to connect to <span class="control" id="-3m9ty3_81">Hive 1</span>, it will get the application libs which will NOT be compatible with the older cluster.</p><p id="-3m9ty3_76">Kerberos connections are only supported to the CDP cluster.</p><p id="-3m9ty3_77">When connecting via <code class="code" id="-3m9ty3_82">Kerberos, you will need to include the</code>--hadoop-classpath<code class="code" id="-3m9ty3_83">when launching</code>hms-mirror`.</p></section><section class="chapter"><h2 id="auto-partition-discovery-not-working" data-toc="auto-partition-discovery-not-working">Auto Partition Discovery not working</h2><p id="-3m9ty3_84">I've set the <code class="code" id="-3m9ty3_89">partitionDiscovery:auto</code> to <code class="code" id="-3m9ty3_90">true,</code> but the partitions aren't getting discovered.</p><p id="-3m9ty3_85"><span class="control" id="-3m9ty3_91">Solution</span></p><p id="-3m9ty3_86">In CDP Base/PVC versions &lt; 7.1.6 have not set the housekeeping thread that runs to activate this feature.</p><p id="-3m9ty3_87">In the Hive metastore configuration in Cloudera Manager, set <code class="code" id="-3m9ty3_92">metastore.housekeeping.threads.on=true</code> in the <span class="emphasis" id="-3m9ty3_93">Hive Service Advanced Configuration Snippet (Safety Valve) for hive-site.xml</span></p><figure id="-3m9ty3_88"><img alt="pic" src="images/hms_housekeeping_thread.png" title="pic" width="1544" height="432"></figure></section><section class="chapter"><h2 id="hive-sql-exception-hdfs-permissions-issues" data-toc="hive-sql-exception-hdfs-permissions-issues">Hive SQL Exception / HDFS Permissions Issues</h2><div class="code-block" data-lang="none">
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException:Permission denied: user [dstreev] does not have [ALL] privilege on [hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_site]
</div><p id="-3m9ty3_95">This error is a permission error to HDFS. For HYBRID, EXPORT_IMPORT, SQL, and SCHEMA_ONLY (with <code class="code" id="-3m9ty3_100">-ams</code> enabled), this could be an issue with cross-cluster HDFS access.</p><p id="-3m9ty3_96">Review the output report for details of where this error occurred (LEFT or RIGHT cluster).</p><p id="-3m9ty3_97">When dealing with CREATE DDL statements submitted through HS2 with a <code class="code" id="-3m9ty3_101">LOCATION</code> element in them, the submitting <span class="emphasis" id="-3m9ty3_102">user</span> <span class="control" id="-3m9ty3_103">AND</span> the HS2 <span class="emphasis" id="-3m9ty3_104">service account</span> must have permissions to the directory. Remember, with cross-cluster access, the user identity will originate on the RIGHT cluster and will be <span class="control" id="-3m9ty3_105">EVALUATED</span> on the LEFT clusters storage layer.</p><p id="-3m9ty3_98">For migrations, the <code class="code" id="-3m9ty3_106">hms-mirror</code> running user (JDBC) and keytab user (HDFS) should be privileged users.</p><section class="chapter"><h3 id="example-and-ambari-hints" data-toc="example-and-ambari-hints">Example and Ambari Hints</h3><p id="-3m9ty3_107">After checking permissions of 'dstreev': Found that the 'dstreev' user was NOT the owner of the files in these directories on the LEFT cluster. The user running the process needs to be in 'dfs.permissions.superusergroup' for the lower clusters 'hdfs' service. Ambari 2.6 has issues setting this property: https://jira.cloudera.com/browse/EAR-7805</p><p id="-3m9ty3_108">Follow the workaround above or add the user to the 'hdfs' group. Or use Ranger to allow all access. On my cluster, with no Ranger, I had to use '/var/lib/ambari-server/resources/scripts/configs.py' to set it manually for Ambari.</p><p id="-3m9ty3_109"><code class="code" id="-3m9ty3_110">sudo ./configs.py --host=k01.streever.local --port=8080 -u admin -p admin -n hdp50 -c hdfs-site -a set -k dfs.permissions.superusergroup -v hdfs_admin</code></p></section></section><section class="chapter"><h2 id="yarn-submission-stuck-in-accepted-phase" data-toc="yarn-submission-stuck-in-accepted-phase">YARN Submission stuck in ACCEPTED phase</h2><p id="-3m9ty3_111">The process uses a connection pool to hive. If the concurrency value for the cluster is too high, you may have reached the maximum ratio of AM (Application Masters) for the YARN queue.</p><p id="-3m9ty3_112">Review the ACCEPTED jobs and review the jobs <span class="emphasis" id="-3m9ty3_116">Diagnostics</span> status for details on <span class="emphasis" id="-3m9ty3_117">why</span> the jobs is stuck.</p><p id="-3m9ty3_113"><span class="control" id="-3m9ty3_118">Solution</span></p><p id="-3m9ty3_114">Either of:</p><ol class="list _decimal" id="-3m9ty3_115" type="1"><li class="list__item" id="-3m9ty3_119"><p>Reduce the concurrency in the configuration file for <code class="code" id="-3m9ty3_121">hms-mirror</code></p></li><li class="list__item" id="-3m9ty3_120"><p>Increase the AM ratio or Queue size to allow the jobs to be submitted. This can be done while the process is running.</p></li></ol></section><section class="chapter"><h2 id="spark-dfs-access" data-toc="spark-dfs-access">Spark DFS Access</h2><p id="-3m9ty3_122">If you have problems accessing HDFS from <code class="code" id="-3m9ty3_124">spark-shell</code> or <code class="code" id="-3m9ty3_125">spark-submit</code> try adding the following configuration to spark:</p><div class="code-block" data-lang="none">
--conf spark.yarn.access.hadoopFileSystems=hdfs://&lt;NEW_NAMESPACE&gt;,hdfs://&lt;OLD_NAMESPACE&gt;
</div></section><section class="chapter"><h2 id="permission-issues" data-toc="permission-issues">Permission Issues</h2><p id="-3m9ty3_126"><code class="code" id="-3m9ty3_132">HiveAccessControlException Permission denied user: [xxxx] does not have [ALL] privileges on ['location'] [state=42000,code=40000]</code></p><p id="-3m9ty3_127">and possibly</p><p id="-3m9ty3_128">In HS2 Logs: <code class="code" id="-3m9ty3_133">Unauthorized connection for super-user</code></p><p id="-3m9ty3_129"><span class="control" id="-3m9ty3_134">Solution</span></p><p id="-3m9ty3_130">Caused by the following:</p><ul class="list _bullet" id="-3m9ty3_131"><li class="list__item" id="-3m9ty3_135"><p>The 'user' doesn't have access to the location as indicated in the message. Verify through 'hdfs' that this is true or not. If the user does NOT have access, grant them access and try again.</p></li><li class="list__item" id="-3m9ty3_136"><p>The 'hive' service account running HS2 does NOT have access to the location. The message will mask this and present it as a 'user' issue, when it is in fact an issue with the 'hive' service account. Grant the account the appropriate access.</p></li><li class="list__item" id="-3m9ty3_137"><p>The 'hive' service does NOT have proxy permissions to the storage layer. </p><ul class="list _bullet" id="-3m9ty3_138"><li class="list__item" id="-3m9ty3_139"><p>Check the <code class="code" id="-3m9ty3_140">hadoop.proxyuser.hive.hosts|groups</code> setting in <code class="code" id="-3m9ty3_141">core-site.xml</code>. If you are running into this <code class="code" id="-3m9ty3_142">super-user</code> error on the RIGHT cluster, while trying to access a storage location on the <span class="emphasis" id="-3m9ty3_143">LEFT</span> cluster, ensure the proxy settings include the rights values in the RIGHT clusters <code class="code" id="-3m9ty3_144">core-site.xml</code>, since that is where HS2 will pick it up from.</p></li></ul></li></ul></section><section class="chapter"><h2 id="must-use-hiveinputformat-to-read-acid-tables" data-toc="must-use-hiveinputformat-to-read-acid-tables">Must use HiveInputFormat to read ACID tables</h2><p id="-3m9ty3_145">We've seen this while attempting to migrate ACID tables from older clusters (HDP 2.6). The error occurs when we try to extract the ACID table data to a 'transfer' external table on the LEFT cluster, which is 'legacy'.</p><p id="-3m9ty3_146"><span class="control" id="-3m9ty3_149">Solution</span></p><p id="-3m9ty3_147">HDP 2.6.5, the lowest supported cluster version intended for this process, should be using the 'tez' execution engine <code class="code" id="-3m9ty3_150">set hive.execution.engine=tez</code>. If the cluster has been upgraded from an older HDP version OR they've simply decided NOT to use the <code class="code" id="-3m9ty3_151">tez</code> execution engine', you may get this error.</p><p id="-3m9ty3_148">In <code class="code" id="-3m9ty3_152">hms-mirror</code> releases 1.3.0.5 and above, we will explicitly run <code class="code" id="-3m9ty3_153">set hive.execution.engine=tez</code> on the LEFT cluster when identified as a 'legacy' cluster. For version 1.3.0.4 (the first version to support ACID transfers), you'll need to set the hive environment for the HS2 instance you're connecting to use <code class="code" id="-3m9ty3_154">tez</code> as the execution engine.</p></section><section class="chapter"><h2 id="acl-issues-across-cross-while-using-lower-clusters-storage" data-toc="acl-issues-across-cross-while-using-lower-clusters-storage">ACL issues across cross while using LOWER clusters storage</h2><p id="-3m9ty3_155">Are you seeing something like this?</p><div class="code-block" data-lang="none">
org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=hive, access=WRITE, inode=&quot;/apps/hive/warehouse/merge_files.db/merge_files_part_a_small_replacement&quot;:dstreev:hadoop:drwxr-xr-x at
</div><p id="-3m9ty3_157">This is caused when trying to <code class="code" id="-3m9ty3_160">CREATE</code> a table on the <span class="control" id="-3m9ty3_161">RIGHT</span> cluster that references data on the <span class="control" id="-3m9ty3_162">LEFT</span> cluster. When the LEFT cluster is setup differently with regard to impersonation (doas) than the RIGHT, transfer tables are created with POSIX permissions that may not allow the RIGHT cluster/user to access that location.</p><p id="-3m9ty3_158"><span class="control" id="-3m9ty3_163">Solution</span></p><p id="-3m9ty3_159">Using Ranger on the LEFT cluster, open up the permissions to allow the requesting user access as identified.</p></section><div class="last-modified">Last modified: 16 August 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="storage-migration.html" class="navigation-links__prev">Storage Migration</a><a href="license.html" class="navigation-links__next">License APLv2</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b408/app.js"></script></body></html>