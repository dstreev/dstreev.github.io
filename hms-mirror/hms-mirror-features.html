<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="built-on" content="2024-10-08T10:57:58.829335"><title>Features | hms-mirror</title><script type="application/json" id="virtual-toc-data">[{"id":"iceberg-table-migration-via-hive","level":0,"title":"Iceberg Table Migration via Hive","anchor":"#iceberg-table-migration-via-hive"},{"id":"file-system-stats","level":0,"title":"File System Stats","anchor":"#file-system-stats"},{"id":"create-external-table-if-not-exists-option","level":0,"title":"CREATE [EXTERNAL] TABLE IF NOT EXISTS Option","anchor":"#create-external-table-if-not-exists-option"},{"id":"auto-gathering-stats-disabled-by-default","level":0,"title":"Auto Gathering Stats (disabled by default)","anchor":"#auto-gathering-stats-disabled-by-default"},{"id":"non-standard-partition-locations","level":0,"title":"Non-Standard Partition Locations","anchor":"#non-standard-partition-locations"},{"id":"optimizations","level":0,"title":"Optimizations","anchor":"#optimizations"},{"id":"hdp3-managedlocation-database-property","level":0,"title":"HDP3 MANAGEDLOCATION Database Property","anchor":"#hdp3-managedlocation-database-property"},{"id":"compress-text-output","level":0,"title":"Compress Text Output","anchor":"#compress-text-output"},{"id":"views","level":0,"title":"VIEWS","anchor":"#views"},{"id":"acid-tables","level":0,"title":"ACID Tables","anchor":"#acid-tables"},{"id":"intermediate-common-storage-options","level":0,"title":"Intermediate/Common Storage Options","anchor":"#intermediate-common-storage-options"},{"id":"non-native-hive-tables-hbase-kafka-jdbc-druid-etc","level":0,"title":"Non-Native Hive Tables (Hbase, KAFKA, JDBC, Druid, etc..)","anchor":"#non-native-hive-tables-hbase-kafka-jdbc-druid-etc"},{"id":"avro-tables","level":0,"title":"AVRO Tables","anchor":"#avro-tables"},{"id":"table-translations","level":0,"title":"Table Translations","anchor":"#table-translations"},{"id":"distcp-planning-workbook-and-scripts","level":0,"title":"distcp Planning Workbook and Scripts","anchor":"#distcp-planning-workbook-and-scripts"},{"id":"acid-table-downgrades","level":0,"title":"ACID Table Downgrades","anchor":"#acid-table-downgrades"},{"id":"reset-to-default-locations","level":0,"title":"Reset to Default Locations","anchor":"#reset-to-default-locations"},{"id":"legacy-row-serde-translations","level":0,"title":"Legacy Row Serde Translations","anchor":"#legacy-row-serde-translations"},{"id":"filtering-tables-to-process","level":0,"title":"Filtering Tables to Process","anchor":"#filtering-tables-to-process"},{"id":"migrations-between-clusters-without-line-of-site","level":0,"title":"Migrations between Clusters WITHOUT line of Site","anchor":"#migrations-between-clusters-without-line-of-site"},{"id":"shared-storage-models-isilon-spectrum-scale-etc","level":0,"title":"Shared Storage Models (Isilon, Spectrum-Scale, etc.)","anchor":"#shared-storage-models-isilon-spectrum-scale-etc"},{"id":"disconnected-mode","level":0,"title":"Disconnected Mode","anchor":"#disconnected-mode"},{"id":"no-purge-option","level":0,"title":"No-Purge Option","anchor":"#no-purge-option"},{"id":"property-overrides","level":0,"title":"Property Overrides","anchor":"#property-overrides"},{"id":"global-location-map","level":0,"title":"Global Location Map","anchor":"#global-location-map"},{"id":"force-external-locations","level":0,"title":"Force External Locations","anchor":"#force-external-locations"},{"id":"hdp-3-hive","level":0,"title":"HDP 3 Hive","anchor":"#hdp-3-hive"},{"id":"schema-fix-features","level":0,"title":"Schema Fix Features","anchor":"#schema-fix-features"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b419/app.css" rel="stylesheet"><meta name="msapplication-TileColor" content="#000000"><link rel="apple-touch-icon" sizes="180x180" href="https://jetbrains.com/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://jetbrains.com/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://jetbrains.com/favicon-16x16.png"><meta name="msapplication-TileImage" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-144x144.png"><meta name="msapplication-square70x70logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-70x70.png"><meta name="msapplication-square150x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x150.png"><meta name="msapplication-square310x310logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x310.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="Features | hms-mirror"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="hms-mirror Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="writerside-documentation/hms-mirror/v2.2.0.x/hms-mirror-features.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="Features | hms-mirror"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "writerside-documentation/hms-mirror/v2.2.0.x/hms-mirror-features.html#webpage",
    "url": "writerside-documentation/hms-mirror/v2.2.0.x/hms-mirror-features.html",
    "name": "Features | hms-mirror",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "writerside-documentation/hms-mirror/#website",
    "url": "writerside-documentation/hms-mirror/",
    "name": "hms-mirror Help"
}</script><!-- End Schema.org --></head><body data-id="hms-mirror-features" data-main-title="Features" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs=""><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>hms-mirror v2.2.0.x Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="hms-mirror-features" id="hms-mirror-features.md">Features</h1><p id="-qps45w_3"><code class="code" id="-qps45w_33">hms-mirror</code> is designed to migrate schema definitions from one cluster to another or simply provide an extract of the schemas via <code class="code" id="-qps45w_34">-d DUMP</code>.</p><p id="-qps45w_4">Under certain conditions, <code class="code" id="-qps45w_35">hms-mirror</code> will 'move' data too. Using the data strategies <code class="code" id="-qps45w_36">-d SQL|EXPORT_IMPORT|HYBRID</code> well use a combination of SQL temporary tables and <a href="linking-cluster-storage-layers.html" id="-qps45w_37" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">Linking Clusters Storage Layers</a> to facilitate this.</p><section class="chapter"><h2 id="iceberg-table-migration-via-hive" data-toc="iceberg-table-migration-via-hive">Iceberg Table Migration via Hive</h2><p id="-qps45w_38">See <a href="hms-mirror-iceberg-migration.html" id="-qps45w_39" data-tooltip="This process will look at Hive tables, evaluate if the table is a candidate for migration to Iceberg, and then migrate the table to Iceberg.">Iceberg Migration</a> for details.</p></section><section class="chapter"><h2 id="file-system-stats" data-toc="file-system-stats">File System Stats</h2><p id="-qps45w_40">SQL based operations, <code class="code" id="-qps45w_43">hms-mirror</code> will attempt to gather file system stats for the tables being migrated. This is done by running <code class="code" id="-qps45w_44">hdfs dfs -count</code> on the table location. This is done to help determine the best strategy for moving data and allows us to set certain hive session values and distribution strategies in SQL to optimize the data movement.</p><p id="-qps45w_41">But some FileSystems may not be very efficient at gathering stats. For example, S3. In these cases, you can disable the stats gathering by adding <code class="code" id="-qps45w_45">-ssc|--skip-stats-collection</code> to your command line.</p><p id="-qps45w_42">When you have a LOT of tables, collecting stats can have a significant impact on the time it takes to run <code class="code" id="-qps45w_46">hms-mirror</code> and the general pressure on the FileSystem to gather this information. In this case, you have to option to disable stats collection through <code class="code" id="-qps45w_47">-scc</code>.</p></section><section class="chapter"><h2 id="create-external-table-if-not-exists-option" data-toc="create-external-table-if-not-exists-option">CREATE [EXTERNAL] TABLE IF NOT EXISTS Option</h2><p id="-qps45w_49">Default behavior for <code class="code" id="-qps45w_53">hms-mirror</code> is to NOT include the <code class="code" id="-qps45w_54">IF NOT EXISTS</code> clause in the <code class="code" id="-qps45w_55">CREATE TABLE</code> statements. This is because we want to ensure that the table is created and that the schema is correct. If the table already exists, we want to fail.</p><p id="-qps45w_50">But there are some scenarios where the table is present and we don't want the process to fail on the CREATE statement to ensure the remaining SQL statements are executed. In this case, you can modify add the commandline option <code class="code" id="-qps45w_56">-cine</code> or add to the configuration:</p><div class="code-block" data-lang="yaml">
  clusters:
    RIGHT|LEFT:
      createIfNotExists: &quot;true&quot;
</div><p id="-qps45w_52">Using this option has the potential to create a table with a different schema than the source. This is not recommended. This option is applied when using the <code class="code" id="-qps45w_57">SCHEMA_ONLY</code> data strategy.</p></section><section class="chapter"><h2 id="auto-gathering-stats-disabled-by-default" data-toc="auto-gathering-stats-disabled-by-default">Auto Gathering Stats (disabled by default)</h2><p id="-qps45w_58">CDP Default settings have enabled <code class="code" id="-qps45w_62">hive.stats.autogather</code> and <code class="code" id="-qps45w_63">hive.stats.column.autogather</code>. This impacts the speed of INSERT statements (used by <code class="code" id="-qps45w_64">hms-mirror</code> to migrate data) and for large/numerous tables, the impact can be significant.</p><p id="-qps45w_59">The default configuration for <code class="code" id="-qps45w_65">hms-mirror</code> is to disable these settings. You can re-enable this in <code class="code" id="-qps45w_66">hms-mirror</code> for each side (LEFT|RIGHT) separately. Add the following to your configuration.</p><div class="code-block" data-lang="yaml">
clusters:
  LEFT|RIGHT:
    enableAutoTableStats: true
    enableAutoColumnStats: true
</div><p id="-qps45w_61">To disable, remove the <code class="code" id="-qps45w_67">enableAutoTableStats</code> and <code class="code" id="-qps45w_68">enableAutoColumnStats</code> entries or set them to <code class="code" id="-qps45w_69">false</code>.</p></section><section class="chapter"><h2 id="non-standard-partition-locations" data-toc="non-standard-partition-locations">Non-Standard Partition Locations</h2><p id="-qps45w_70">Partitions created by 'hive' with default locations follow a file system naming convention that allows other partitions of 'hive' to discovery/manage those location and partition associations.</p><p id="-qps45w_71">The standard is for partitions to exist as sub-directories of the table location. For example: Table Location is <code class="code" id="-qps45w_78">hdfs://my-cluster/warehouse/tablespace/external/hive/my_test.db/my_table</code> and the partition location is <code class="code" id="-qps45w_79">hdfs://my-cluster/warehouse/tablespace/external/hive/my_test.db/my_table/dt=2020-01-01</code>, assuming the partition column name is <code class="code" id="-qps45w_80">dt</code>.</p><p id="-qps45w_72">When this convention is not followed, additional steps are required to build the partition metadata. You can't use <code class="code" id="-qps45w_81">MSCK REPAIR</code> because it will not find the partitions. You can use <code class="code" id="-qps45w_82">ALTER TABLE ADD PARTITION</code> but you'll need to provide the location of the partition. <code class="code" id="-qps45w_83">hms-mirror</code> will do this for you when using the data strategies <code class="code" id="-qps45w_84">-d DUMP|SCHEMA_ONLY</code> and the commandline flag <code class="code" id="-qps45w_85">-epl|--evaluate-partition-location</code>.</p><p id="-qps45w_73">In order to make this evaluation efficient, we do NOT use standard HiveSQL to discover the partition details. It is possible to use HiveSQL for this, it's just not meant for operations at scale or tables with a lot of partitions.</p><p id="-qps45w_74">Hence, we tap directly into the hive metastore database. In order to use this feature, you will need to add the following configuration definition to your hms-mirror configuration file (default.yaml).</p><div class="code-block" data-lang="yaml">
clusters:
  LEFT|RIGHT:
    ...
    metastore_direct:
      uri: &quot;&lt;db_url&gt;&quot;
      type: MYSQL|POSTGRES|ORACLE
      connectionProperties:
        user: &quot;&lt;db_user&gt;&quot;
        password: &quot;&lt;db_password&gt;&quot;
      connectionPool:
        min: 3
        max: 5
</div><p id="-qps45w_76">You will also need to place a copy of the RDBMS JDBC driver in <code class="code" id="-qps45w_86">$HOME/.hms-mirror/aux_libs</code>. The driver must match the <code class="code" id="-qps45w_87">type</code> defined in the configuration file.</p><p id="-qps45w_77"><span class="control" id="-qps45w_88">Note: Non-Standard Partition Location will affect other strategies like <code class="code" id="-qps45w_89">SQL</code> where the LEFT clusters storage is accessible to the RIGHT and is used by the RIGHT to source data. The 'mirror' table used for the transfer will NOT discover the partitions and will NOT transfer data. See: <a href="https://github.com/cloudera-labs/hms-mirror/issues/63" id="-qps45w_90" data-external="true" rel="noopener noreferrer">Issue #63</a> for updates on addressing this scenario. If this is affecting you, I highly recommend you comment on the issue to help us set priorities.</span></p></section><section class="chapter"><h2 id="optimizations" data-toc="optimizations">Optimizations</h2><p id="-qps45w_91">The following configuration settings control the various optimizations taken by <code class="code" id="-qps45w_96">hms-mirror</code>. These settings are mutually exclusive.</p><ul class="list _bullet" id="-qps45w_92"><li class="list__item" id="-qps45w_97"><p><code class="code" id="-qps45w_100">-at|--auto-tune</code></p></li><li class="list__item" id="-qps45w_98"><p><code class="code" id="-qps45w_101">-so|--skip-optimizations</code></p></li><li class="list__item" id="-qps45w_99"><p><code class="code" id="-qps45w_102">-sdpi|--sort-dynamic-partition-inserts</code></p></li></ul><section class="chapter"><h3 id="auto-tune" data-toc="auto-tune">Auto-Tune</h3><p id="-qps45w_103"><code class="code" id="-qps45w_108">-at|--auto-tune</code></p><p id="-qps45w_104">Auto-tuning will use some basic file level statistics about tables/partitions to provide overrides for the following settings:</p><ul class="list _bullet" id="-qps45w_105"><li class="list__item" id="-qps45w_109"><p><code class="code" id="-qps45w_112">tez.grouping.max-size</code></p></li><li class="list__item" id="-qps45w_110"><p><code class="code" id="-qps45w_113">hive.exec.max.dynamic.partitions</code></p></li><li class="list__item" id="-qps45w_111"><p><code class="code" id="-qps45w_114">hive.exec.reducers.max</code></p></li></ul><p id="-qps45w_106">in addition to these session level setting, we'll use those basic file statistics to construct migration scripts that address things like 'small-files' and 'large' partition datasets.</p><p id="-qps45w_107">We'll set <code class="code" id="-qps45w_115">hive.optimize.sort.dynamic.partition.threshold=-1</code> and append <code class="code" id="-qps45w_116">DISTRIBUTE BY</code> to the SQL migration sql statement, just like we do with <code class="code" id="-qps45w_117">-sdpi</code>. But we'll go one step further and review the average partition size and add an additional 'grouping' element to the SQL to ensure we get efficient writers to a partition. The means that tables with large partition datasets will have more than the standard single writer per partition, preventing the LONG running hanging task that is trying to write a very large partition.</p></section><section class="chapter"><h3 id="sort-dynamic-partition-inserts" data-toc="sort-dynamic-partition-inserts">Sort Dynamic Partition Inserts</h3><p id="-qps45w_118"><code class="code" id="-qps45w_121">-sdpi|--sort-dynamic-partition-inserts</code></p><p id="-qps45w_119">This will set the session property <code class="code" id="-qps45w_122">hive.optimize.sort.dynamic.partition.threshold=0</code>, which will enable plans to distribute multi partition inserts by the partition key, therefore reducing partitions writes to a single 'writer/reducer'.</p><p id="-qps45w_120">When this isn't set, we set <code class="code" id="-qps45w_123">hive.optimize.sort.dynamic.partition.threshold=-1</code>, and append <code class="code" id="-qps45w_124">DISTRIBUTE BY</code> to the SQL migration sql statement to ensure the same behavior of grouping reducers by partition values.</p></section><section class="chapter"><h3 id="skip-optimizations" data-toc="skip-optimizations">Skip Optimizations</h3><p id="-qps45w_125"><code class="code" id="-qps45w_129">-so</code></p><p id="-qps45w_126"><a href="https://github.com/cloudera-labs/hms-mirror/issues/23" id="-qps45w_130" data-external="true" rel="noopener noreferrer">Feature Request #23</a> was introduced in v1.5.4.2 and give an option to <span class="control" id="-qps45w_131">Skip Optimizations</span>.</p><p id="-qps45w_127">When migrating data via SQL with partitioned tables (OR downgrading an ACID table), there are optimizations that we apply to help hive distribute data more efficiently. One method is to use <code class="code" id="-qps45w_132">hive.optimize.sort.dynamic.partition=true</code> which will &quot;DISTRIBUTE&quot; data along the partitions via a Reduction task. Another is to declare this in SQL with a <code class="code" id="-qps45w_133">DISTRIBUTE BY</code> clause.</p><p id="-qps45w_128">But there is a corner case where these optimizations can get in the way and cause long-running tasks. If the source table has already been organized into large files (which would be within the partitions already), adding the optimizations above force a single reducer per partition. If the partitions are large and already have good file sizes, we want to skip these optimizations and let hive run the process with only a map task.</p></section></section><section class="chapter"><h2 id="hdp3-managedlocation-database-property" data-toc="hdp3-managedlocation-database-property">HDP3 MANAGEDLOCATION Database Property</h2><p id="-qps45w_134"><a href="https://github.com/cloudera-labs/hms-mirror/issues/52" id="-qps45w_136" data-external="true" rel="noopener noreferrer">HDP3 doesn't support MAANGEDLOCATION</a> so we've added a property to the cluster configuration to allow the system to <span class="emphasis" id="-qps45w_137">SKIP</span> setting the <code class="code" id="-qps45w_138">MANAGEDLOCATION</code> database property in HDP 3 / Hive 3 environments.</p><div class="code-block" data-lang="yaml">
clusters:
  LEFT:
    platformType: 'HDP3'
</div></section><section class="chapter"><h2 id="compress-text-output" data-toc="compress-text-output">Compress Text Output</h2><p id="-qps45w_139"><code class="code" id="-qps45w_140">-cto</code> will control the session level setting for `hive.exec.compress.output'.</p></section><section class="chapter"><h2 id="views" data-toc="views">VIEWS</h2><p id="-qps45w_141"><code class="code" id="-qps45w_146">hms-mirror</code> now supports the migration of VIEWs between two environments. Use the <code class="code" id="-qps45w_147">-v|--views-only</code> option to execute this path. VIEW creation requires dependent tables to exist.</p><p id="-qps45w_142">Run <code class="code" id="-qps45w_148">hms-mirror</code> to create all the target tables before running it with the <code class="code" id="-qps45w_149">-v</code> option.</p><p id="-qps45w_143">This flag is an <code class="code" id="-qps45w_150">OR</code> for processing VIEW's <code class="code" id="-qps45w_151">OR</code> TABLE's. They are NOT processed together.</p><p id="-qps45w_144"><span class="control" id="-qps45w_152">Requirements</span></p><ul class="list _bullet" id="-qps45w_145"><li class="list__item" id="-qps45w_153"><p>The dependent tables must exist in the RIGHT cluster</p></li><li class="list__item" id="-qps45w_154"><p>When using <code class="code" id="-qps45w_155">-dbp|--db-prefix</code> option, VIEW definitions are NOT modified and will most likely cause VIEW creation to fail.</p></li></ul></section><section class="chapter"><h2 id="acid-tables" data-toc="acid-tables">ACID Tables</h2><p id="-qps45w_156"><code class="code" id="-qps45w_162">hms-mirror</code> supports the migration of ACID tables using the <code class="code" id="-qps45w_163">-d HYBRID|SQL|EXPORT_IMPORT</code> data strategy in combination with the <code class="code" id="-qps45w_164">-ma|--migrate-acid</code> or <code class="code" id="-qps45w_165">-mao|--migrate-acid-only</code> flag. You can also simply 'replay' the schema definition (without data) using <code class="code" id="-qps45w_166">-d SCHEMA_ONLY -ma|-mao</code>. The <code class="code" id="-qps45w_167">-ma|-mao</code> flag takes an <span class="emphasis" id="-qps45w_168">optional</span> integer value that sets an 'Artificial Bucket Threshold'. When no parameter is specified, the default is <code class="code" id="-qps45w_169">2</code>.</p><p id="-qps45w_157">Use this value to set a bucket limit where we'll <span class="emphasis" id="-qps45w_170">remove</span> the bucket definition during the translation. This is helpful for legacy ACID tables which <span class="emphasis" id="-qps45w_171">required</span> a bucket definition but weren't a part of the intended design. The migration provides an opportunity to correct this artificial design element.</p><p id="-qps45w_158">With the default value <code class="code" id="-qps45w_172">2</code>, we will <span class="emphasis" id="-qps45w_173">remove</span> CLUSTERING from any ACID table definitions with <code class="code" id="-qps45w_174">2</code> or fewer buckets defined. If you wish to keep ALL CLUSTERED definitions, regardless of size, set this value to <code class="code" id="-qps45w_175">0</code>.</p><p id="-qps45w_159">There is now an option to 'downgrade' ACID tables to EXTERNAL/PURGE during migration using the <code class="code" id="-qps45w_176">-da</code> option.</p><section class="chapter"><h3 id="the-acid-migration-process" data-toc="the-acid-migration-process">The ACID Migration Process</h3><p id="-qps45w_177">The ACID migration builds a 'transfer' table on the LEFT cluster, a 'legacy' managed table (when the LEFT is a legacy cluster), or an 'EXTERNAL/PURGE' table. Data is copied to this transfer table from the original ACID table via SQL.</p><p id="-qps45w_178">Since the clusters are <a href="linking-cluster-storage-layers.html" id="-qps45w_182" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">linked</a>, we build a 'shadow' table that is 'EXTERNAL' on the 'RIGHT' cluster that uses the data in the 'LEFT' cluster. Similar to the LINKED data strategy. If the data is partitioned, we run <code class="code" id="-qps45w_183">MSCK</code> on this 'shadow' table in the 'RIGHT' cluster to discover all the partitions.</p><p id="-qps45w_179">The final ACID table is created in the 'RIGHT' cluster, and SQL is used to copy data from the 'LEFT' cluster via the 'shadow' table.</p><p id="-qps45w_180"><span class="control" id="-qps45w_184">Requirements</span></p><ul class="list _bullet" id="-qps45w_181"><li class="list__item" id="-qps45w_185"><p>Data Strategy: <code class="code" id="-qps45w_193">HYBRID</code>, <code class="code" id="-qps45w_194">SQL</code>, or <code class="code" id="-qps45w_195">EXPORT_IMPORT</code></p></li><li class="list__item" id="-qps45w_186"><p>Activate Migrate ACID: <code class="code" id="-qps45w_196">-ma|-mao</code></p></li><li class="list__item" id="-qps45w_187"><p><a href="linking-cluster-storage-layers.html" id="-qps45w_197" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">Link Clusters</a>, unless using the <code class="code" id="-qps45w_198">-is|--intermediate-storage</code> option.</p></li><li class="list__item" id="-qps45w_188"><p>This is a 'ONE' time transfer. It is not an incremental update process.</p></li><li class="list__item" id="-qps45w_189"><p>Adequate Storage on LEFT to make an 'EXTERNAL' copy of the ACID table.</p></li><li class="list__item" id="-qps45w_190"><p>Permissions: </p><ul class="list _bullet" id="-qps45w_199"><li class="list__item" id="-qps45w_200"><p>From the RIGHT cluster, the submitting user WILL need access to the LEFT cluster's storage layer (HDFS) to create the shadow table (with location) that points across clusters.</p></li><li class="list__item" id="-qps45w_201"><p>doas will have a lot to do with the permissions requirements.</p></li><li class="list__item" id="-qps45w_202"><p>The 'hive' service account on the RIGHT cluster will need elevated privileges to the LEFT storage LAYER (HDFS). For example: If the hive service accounts on each cluster DO NOT share the same identity, like <code class="code" id="-qps45w_203">hive</code>, then the RIGHT hive identity MUST also have privileged access to the LEFT clusters HDFS layer.</p></li></ul></li><li class="list__item" id="-qps45w_191"><p>Partitioned tables must have data that is 'discoverable' via <code class="code" id="-qps45w_204">MSCK</code>. NOTE: The METADATA activity and REDUCER restrictions to the number of BUCKETs can dramatically affect this.- The number of partitions in the source ACID tables must be below the <code class="code" id="-qps45w_205">partitionLimit</code> (default 500). This strategy may not be successful when the partition count is above this, and we won't even attempt the conversion. Check YARN for the progress of jobs with a large number of partitions/buckets. Progress many appear stalled from 'hms-mirror'.</p></li><li class="list__item" id="-qps45w_192"><p>ACID table migration to Hive 1/2 is NOT supported due to the lack of support for &quot;INSERT OVERWRITE&quot; on transactional tables. Hive 1/2 to Hive 3 IS support and the target of this implementation. Hive 3 to Hive 3 is also supported.</p></li></ul></section><section class="chapter"><h3 id="replace-acid-r-or-replace" data-toc="replace-acid-r-or-replace">Replace ACID <code class="code" id="-qps45w_208">-r</code> or <code class="code" id="-qps45w_209">--replace</code></h3><p id="-qps45w_207">When downgrading ACID tables during migration, the <code class="code" id="-qps45w_210">-r</code> option will give you the option to 'replace' the original ACID table with the a table that is no longer ACID. This option is only available along with the <code class="code" id="-qps45w_211">-da</code> and <code class="code" id="-qps45w_212">SQL</code> data strategy options.</p></section></section><section class="chapter"><h2 id="intermediate-common-storage-options" data-toc="intermediate-common-storage-options">Intermediate/Common Storage Options</h2><p id="-qps45w_213">When bridging the gap between two clusters, you may find they can't share/link storage. In this case, using one of these options will help you with the transfer.</p><p id="-qps45w_214">The <code class="code" id="-qps45w_216">-is</code> or <code class="code" id="-qps45w_217">--intermediate-storage</code> option is consider a transient location that both cluster can share, see, and have access to. The strategies for transferring data (EXPORT_IMPORT, SQL, HYBRID) will use this location to facilitate the transfer. This is a common strategy when migrating from on-prem environments to the cloud.</p><p id="-qps45w_215">The <code class="code" id="-qps45w_218">-cs</code> or <code class="code" id="-qps45w_219">--common-storage</code> option is similar to <code class="code" id="-qps45w_220">-is</code> but this option ends up being the final resting place for the data, not just the transfer location. And with this option, we can streamline the jumps required to migrate data. Again, this location needs to be accessible to both clusters.</p></section><section class="chapter"><h2 id="non-native-hive-tables-hbase-kafka-jdbc-druid-etc" data-toc="non-native-hive-tables-hbase-kafka-jdbc-druid-etc">Non-Native Hive Tables (Hbase, KAFKA, JDBC, Druid, etc..)</h2><p id="-qps45w_221">Any table definition without a <code class="code" id="-qps45w_225">LOCATION</code> element is typically a reference to an external system like: HBase, Kafka, Druid, and/or (but not limited to) JDBC.</p><p id="-qps45w_222"><span class="control" id="-qps45w_226">Requirements</span></p><p id="-qps45w_223">These references require the environment to be:</p><ul class="list _bullet" id="-qps45w_224"><li class="list__item" id="-qps45w_227"><p>Correctly configured to use these resources</p></li><li class="list__item" id="-qps45w_228"><p>Include the required libraries in the default hive environment.</p></li><li class="list__item" id="-qps45w_229"><p>The referenced resource must exist already BEFORE the 'hive' DDL will successfully run.</p></li></ul></section><section class="chapter"><h2 id="avro-tables" data-toc="avro-tables">AVRO Tables</h2><p id="-qps45w_230">AVRO tables can be designed with a 'reference' to a schema file in <code class="code" id="-qps45w_236">TBLPROPERTIES</code> with <code class="code" id="-qps45w_237">avro.schema.url</code>. The referenced file needs to be 'copied' to the <span class="emphasis" id="-qps45w_238">RIGHT</span> cluster BEFORE the <code class="code" id="-qps45w_239">CREATE</code> statement for the AVRO table will succeed.</p><p id="-qps45w_231">Add the <code class="code" id="-qps45w_240">-asm|--avro-schema-move</code> option at the command line to <span class="emphasis" id="-qps45w_241">copy</span> the file from the LEFT cluster to the RIGHT cluster.</p><p id="-qps45w_232">As long as the clusters are <a href="linking-cluster-storage-layers.html" id="-qps45w_242" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">linked</a> and the cluster <code class="code" id="-qps45w_243">hcfsNamespace</code> values are accurate, the user's credentials running <code class="code" id="-qps45w_244">hms-mirror</code> will attempt to copy the schema file to the <span class="emphasis" id="-qps45w_245">RIGHT</span> cluster BEFORE executing the <code class="code" id="-qps45w_246">CREATE</code> statement.</p><p id="-qps45w_233"><span class="control" id="-qps45w_247">Requirements</span></p><ul class="list _bullet" id="-qps45w_234"><li class="list__item" id="-qps45w_248"><p><a href="linking-cluster-storage-layers.html" id="-qps45w_252" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">Link Clusters</a> for Data Strategies: <code class="code" id="-qps45w_253">SCHEMA_ONLY</code>, <code class="code" id="-qps45w_254">SQL</code>, <code class="code" id="-qps45w_255">EXPORT_IMPORT</code>, and <code class="code" id="-qps45w_256">HYBRID</code></p></li><li class="list__item" id="-qps45w_249"><p>Running user must have 'namespace' access to the directories identified in the <code class="code" id="-qps45w_257">TBLPROPERTIES</code> key <code class="code" id="-qps45w_258">avro.schema.url</code>.</p></li><li class="list__item" id="-qps45w_250"><p>The user running <code class="code" id="-qps45w_259">hms-mirror</code> will need enough storage level permissions to copy the file.</p></li><li class="list__item" id="-qps45w_251"><p>When hive is running with <code class="code" id="-qps45w_260">doas=false</code>, <code class="code" id="-qps45w_261">hive</code> will need access to this file.</p></li></ul><section class="chapter"><h3 id="warnings" data-toc="warnings">Warnings</h3><ul class="list _bullet" id="-qps45w_262"><li class="list__item" id="-qps45w_263"><p>With the <code class="code" id="-qps45w_264">EXPORT_IMPORT</code> strategy, the <code class="code" id="-qps45w_265">avro.schema.url</code> location will NOT be converted. It may lead to an issue reading the table if the location includes a prefix of the cluster's namespace OR the file doesn't exist in the new cluster.</p></li></ul></section></section><section class="chapter"><h2 id="table-translations" data-toc="table-translations">Table Translations</h2><section class="chapter"><h3 id="legacy-managed-tables" data-toc="legacy-managed-tables">Legacy Managed Tables</h3><p id="-qps45w_267"><code class="code" id="-qps45w_268">hms-mirror</code> will convert 'legacy' managed tables in Hive 1 or 2 to EXTERNAL tables in Hive 3. It relies on the <code class="code" id="-qps45w_269">legacyHive</code> setting in the cluster configurations to accurately make this conversion. So make sure you've set this correctly.</p></section></section><section class="chapter"><h2 id="distcp-planning-workbook-and-scripts" data-toc="distcp-planning-workbook-and-scripts"><code class="code" id="-qps45w_277">distcp</code> Planning Workbook and Scripts</h2><p id="-qps45w_271"><code class="code" id="-qps45w_278">hms-mirror</code> will create source files and a shell script that can be used as the basis for the 'distcp' job(s) used to support the databases and tables requested in <code class="code" id="-qps45w_279">-db</code>. <code class="code" id="-qps45w_280">hms-mirror</code> will NOT run these jobs. It will provide the basic job constructs that match what it did for the schemas. Use these constructs to build your execution plan and run these separately.</p><p id="-qps45w_272">The constructs created are intended as a <span class="emphasis" id="-qps45w_281">one-time</span> transfer. If you are using <span class="emphasis" id="-qps45w_282">SNAPSHOTS</span> or <code class="code" id="-qps45w_283">--update</code> flags in <code class="code" id="-qps45w_284">distcp</code> to support incremental updates, you will have to make additional modifications to the scripts/process. Note: For these scenarios, <code class="code" id="-qps45w_285">hms-mirror</code> supports options like <code class="code" id="-qps45w_286">-ro|--read-only</code> and <code class="code" id="-qps45w_287">-sync</code>.</p><p id="-qps45w_273">Each time <code class="code" id="-qps45w_288">hms-mirror</code> is run, <span class="emphasis" id="-qps45w_289">source</span> files for each database are created. These source files need to be copied to the distributed filesystem and reference with an <code class="code" id="-qps45w_290">-f</code> option in <code class="code" id="-qps45w_291">distcp</code>. We also create a <span class="emphasis" id="-qps45w_292">basic</span> shell script that can be used as a template to run the actual <code class="code" id="-qps45w_293">distcp</code> jobs.</p><p id="-qps45w_274">Depending on the job size and operational expectations, you may want to use <span class="emphasis" id="-qps45w_294">SNAPSHOTS</span> to ensure an immutable source or use a <code class="code" id="-qps45w_295">diff</code> strategy for more complex migrations. Regardless, you'll need to make modifications to these scripts to suit your purposes.</p><p id="-qps45w_275">If your process requires the data to exist BEFORE you migrate the schemas, run <code class="code" id="-qps45w_296">hms-mirror</code> in the <code class="code" id="-qps45w_297">dry-run</code> mode (default) and use the distcp planning workbook and scripts to transfer the datasets. Then run <code class="code" id="-qps45w_298">hms-mirror</code> with the <code class="code" id="-qps45w_299">-e|--execute</code> option to migrate the schemas.</p><p id="-qps45w_276">These workbooks will NOT include elements for ACID/Transactional tables. Simply copying the dataset for transactional tables will NOT work. Use the <code class="code" id="-qps45w_300">HYBRID</code> data strategy migration transactional table schemas and datasets.</p></section><section class="chapter"><h2 id="acid-table-downgrades" data-toc="acid-table-downgrades">ACID Table Downgrades</h2><p id="-qps45w_301">The default table creation scenario for Hive 3 (CDP and HDP 3) installations is to create ACID transactional tables. If you're moving from a legacy platform like HDP 2 or CDH, this may have caught you off guard and resulted in a lot of ACID tables you did NOT intend on.</p><p id="-qps45w_302">The <code class="code" id="-qps45w_304">-da|--downgrade-acid</code> option can be used to convert these ACID tables to <span class="emphasis" id="-qps45w_305">EXTERNAL/PURGE</span> tables.</p><p id="-qps45w_303">If you have ACID tables on the current platform and would like to <span class="emphasis" id="-qps45w_306">downgrade</span> them, but you're not doing a migration, try the <code class="code" id="-qps45w_307">-ip|--in-place</code> option. This will archive to existing ACID table and build a new table (with the original table name) that is <span class="emphasis" id="-qps45w_308">EXTERNAL/PURGE</span>.</p></section><section class="chapter"><h2 id="reset-to-default-locations" data-toc="reset-to-default-locations">Reset to Default Locations</h2><p id="-qps45w_309">Migrations present an opportunity to clean up a lot of history. While <code class="code" id="-qps45w_312">hms-mirror</code> was originally designed to migration data and maintain the <span class="control" id="-qps45w_313">relative</span> locations of the data, some may want to reorganize the data during the migration.</p><p id="-qps45w_310">The option <code class="code" id="-qps45w_314">-rdl|--reset-default-location</code> will overwrite the locations originally used and place the datasets in the 'default' locations as defined by <code class="code" id="-qps45w_315">-wd|--warehouse-directory</code> and <code class="code" id="-qps45w_316">-ewd|--external-warehouse-directory</code>.</p><p id="-qps45w_311">The <code class="code" id="-qps45w_317">-rdl</code> option requires <code class="code" id="-qps45w_318">-wd</code> and <code class="code" id="-qps45w_319">-ewd</code> to be specified. These locations will be used to <code class="code" id="-qps45w_320">ALTER</code> the databases <code class="code" id="-qps45w_321">LOCATION</code> and <code class="code" id="-qps45w_322">MANAGEDLOCATION</code> values. After which, all new <code class="code" id="-qps45w_323">CREATE \[EXTERNAL\] TABLE</code> definitions don't specify a <code class="code" id="-qps45w_324">LOCATION</code>, which means table locations will use the default.</p></section><section class="chapter"><h2 id="legacy-row-serde-translations" data-toc="legacy-row-serde-translations">Legacy Row Serde Translations</h2><p id="-qps45w_325">By default, tables using old row <span class="emphasis" id="-qps45w_327">serde</span> classes will be converted to the newer serde as the definition is processed by <code class="code" id="-qps45w_328">hms-mirror</code>. See <a href="https://github.com/cloudera-labs/hms-mirror/blob/6f6c309f24fbb8133e5bd52e5b18274094ff5be8/src/main/java/com/cloudera/utils/hadoop/hms/mirror/feature/LegacyTranslations.java#L28" id="-qps45w_329" data-external="true" rel="noopener noreferrer">here</a> for a list of serdes we look for.</p><p id="-qps45w_326">If you do NOT want to apply this translation, add the option <code class="code" id="-qps45w_330">-slt|--skip-legacy-translation</code> to the commandline.</p></section><section class="chapter"><h2 id="filtering-tables-to-process" data-toc="filtering-tables-to-process">Filtering Tables to Process</h2><p id="-qps45w_331">There are options to filter tables included in <code class="code" id="-qps45w_335">hms-mirror</code> process. You can select <code class="code" id="-qps45w_336">-tf|--table-filter</code> to &quot;include&quot; only tables that match this 'regular-expression'. Inversely, use <code class="code" id="-qps45w_337">-etf|--exclude-table-filter</code> to omit tables from the list. These options are mutually exclusive.</p><p id="-qps45w_332">The filters for <code class="code" id="-qps45w_338">-tf</code> and <code class="code" id="-qps45w_339">-tef</code> are expressed as a 'regular-expression'. Complex expressions should be enclosed in quotes to ensure the commandline interpreter doesn't split them.</p><p id="-qps45w_333">Additional table filters (<code class="code" id="-qps45w_340">-tfs|--table-filter-size-limit</code> and <code class="code" id="-qps45w_341">-tfp|--table-filter-partition-count-limit</code>) that check a tables data size and partition count limits can also be applied to narrow the range of tables you'll process.</p><p id="-qps45w_334">The filter does NOT override the requirement for options like <code class="code" id="-qps45w_342">-ma|-mao</code>. It is used as an additional filter.</p></section><section class="chapter"><h2 id="migrations-between-clusters-without-line-of-site" data-toc="migrations-between-clusters-without-line-of-site">Migrations between Clusters WITHOUT line of Site</h2><p id="-qps45w_343">There will be cases where clusters can't be <a href="linking-cluster-storage-layers.html" id="-qps45w_345" data-tooltip="For the hms-mirror process to work, it relies on the RIGHT clusters' ability to SEE and ACCESS data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.">linked</a>. And without this line of sight, data movement needs to happen through some other means.</p><section class="chapter"><h3 id="on-prem-to-cloud" data-toc="on-prem-to-cloud">On-Prem to Cloud</h3><p id="-qps45w_346">This scenario is most common with &quot;on-prem&quot; to &quot;cloud&quot; migrations. Typically, <code class="code" id="-qps45w_348">hms-mirror</code> is run from the <span class="control" id="-qps45w_349">RIGHT</span> cluster, but in this case the <span class="control" id="-qps45w_350">RIGHT</span> cloud cluster doesn't have line of site to the <span class="control" id="-qps45w_351">LEFT</span> on-prem cluster. But the on-prem cluster will have limited line of site to the cloud environment. In this case, the only option is to run <code class="code" id="-qps45w_352">hms-mirror</code> from the on-prem cluster. The on-prem cluster will need access to either an <code class="code" id="-qps45w_353">-is|--intermediate-storage</code> location that both clusters can access or a <code class="code" id="-qps45w_354">-cs|--common-storage</code> location that each cluster can see, but can be considered the final resting place for the cloud environment. The <code class="code" id="-qps45w_355">-cs</code> option usually means there are fewer data hops required to complete the migration.</p><p id="-qps45w_347">You'll run <code class="code" id="-qps45w_356">hms-mirror</code> from a <span class="control" id="-qps45w_357">LEFT</span> cluster edgenode. This node will require line of site to the Hive Server 2 endpoint in the <span class="control" id="-qps45w_358">RIGHT</span> cloud environment. The <span class="control" id="-qps45w_359">LEFT</span> cluster will also need to be configured with the appropriate libraries and configurations to write to the location defined in <code class="code" id="-qps45w_360">-is|-cs</code>. For example: For S3, the <span class="control" id="-qps45w_361">LEFT</span> cluster will need the AWS S3 libraries configured and the appropriate keys for S3 access setup to complete the transfer.</p></section></section><section class="chapter"><h2 id="shared-storage-models-isilon-spectrum-scale-etc" data-toc="shared-storage-models-isilon-spectrum-scale-etc">Shared Storage Models (Isilon, Spectrum-Scale, etc.)</h2><p id="-qps45w_362">There are cases where 'HDFS' isn't the primary data source. So the only thing the cluster share is storage in these 'common' storage units. You want to transfer the schema, but the data doesn't need to move (at least for 'EXTERNAL' (non-transactional) tables). In this case, try the <code class="code" id="-qps45w_363">-d|--data-strategy</code> COMMON. The schema's will go through all the needed conversions while the data remains in the same location.</p></section><section class="chapter"><h2 id="disconnected-mode" data-toc="disconnected-mode">Disconnected Mode</h2><p id="-qps45w_364">Use the <code class="code" id="-qps45w_370">-rid|--right-is-disconnected</code> mode when you need to build (and/or) transfer schema/datasets from one cluster to another, but you can't connect to both at the same time. See the issues log for details regarding the cases here <a href="https://github.com/cloudera-labs/hms-mirror/issues/17" id="-qps45w_371" data-external="true" rel="noopener noreferrer">issue #17</a>.</p><p id="-qps45w_365">Use cases:</p><ul class="list _bullet" id="-qps45w_366"><li class="list__item" id="-qps45w_372"><p>Schema Only Transfers</p></li><li class="list__item" id="-qps45w_373"><p>SQL, EXPORT_IMPORT, and HYBRID only when -is or -cs is used. This might be the case when the clusters are secure (kerberized), but don't share a common kerberos domain/user auth. So an intermediate or common storage location will be used to migrate the data.</p></li><li class="list__item" id="-qps45w_374"><p>Both clusters (and HS2 endpoints) are Kerberized, but the clusters are NOT the same major hadoop version. In this case, hms-mirror doesn't support connecting to both of these endpoints at the same time. Running in the disconnected mode will help push through with the conversion.</p></li></ul><p id="-qps45w_367">hms-mirror will run as normal, with the exception of examining and running scripts against the right cluster. It will be assumed that the RIGHT cluster elements do NOT exist.</p><p id="-qps45w_368">The RIGHT_ 'execution' scripts and distcp commands will need to be run MANUALLY via Beeline on the RIGHT cluster.</p><p id="-qps45w_369">Note: This will be know as the &quot;right-is-disconnected&quot; option. Which means the process should be run from a node that has access to the &quot;left&quot; cluster. This is 'counter' to our general recommendation that the process should be run from the 'right' cluster.</p></section><section class="chapter"><h2 id="no-purge-option" data-toc="no-purge-option">No-Purge Option</h2><p id="-qps45w_375"><code class="code" id="-qps45w_377">-np</code></p><p id="-qps45w_376"><a href="https://github.com/cloudera-labs/hms-mirror/issues/25" id="-qps45w_378" data-external="true" rel="noopener noreferrer">Feature Request #25</a> was introduced in v1.5.4.2 and gives the user to option to remove the <code class="code" id="-qps45w_379">external.table.purge</code> option that is added when converting legacy managed tables to external table (Hive 1/2 to 3). This does affect the behavior of the table from the older platforms.</p></section><section class="chapter"><h2 id="property-overrides" data-toc="property-overrides">Property Overrides</h2><p id="-qps45w_380"><code class="code" id="-qps45w_387">-po[l|r] &lt;key=value&gt;[,&lt;key=value&gt;]...</code></p><p id="-qps45w_381"><a href="https://github.com/cloudera-labs/hms-mirror/issues/27" id="-qps45w_388" data-external="true" rel="noopener noreferrer">Feature Request #27</a> introduced in v1.5.4.2 provides the ability to set a hive properties at the beginning of each migration part. This is a comma separated list of key=value pairs with no space. If spaces are needed, quote the parameter on the commandline.</p><p id="-qps45w_382">You can use <code class="code" id="-qps45w_389">-po</code> to set the properties for BOTH clusters or <code class="code" id="-qps45w_390">-pol</code>|<code class="code" id="-qps45w_391">-por</code> to set them specifically for the 'left' and/or 'right' cluster.</p><p id="-qps45w_383">For example: <code class="code" id="-qps45w_392">-po hive.exec.orc.split.strategy=BI,hive.compute.query.using.stats=false</code></p><p id="-qps45w_384">To provide a consistent list of settings for ALL jobs, add/modify the following section in the configuration file ie: <code class="code" id="-qps45w_393">default.yaml</code> used for processing. In this case you do NOT need to use the commandline option. Although, you can set basic values in the configuration file and add other via the commandline.</p><p id="-qps45w_385">Notice that there are setting for the LEFT and RIGHT clusters.</p><div class="code-block" data-lang="yaml">
optimization:
  overrides:
    left:
      tez.queue.name: &quot;compaction&quot;
    right:
      tez.queue.name: &quot;migration&quot;
      
</div></section><section class="chapter"><h2 id="global-location-map" data-toc="global-location-map">Global Location Map</h2><p id="-qps45w_394"><code class="code" id="-qps45w_401">-glm|--global-location-map &lt;from=to&gt;[,...]</code></p><p id="-qps45w_395">This is an opportunity to make some specific directory mappings during the migration. You can supply a comma separated list of directory pairs to be use for evaluation.</p><p id="-qps45w_396"><code class="code" id="-qps45w_402">-glm /data/my_original_location=/corp/finance_new_loc,/user/hive=/warehouse/hive</code></p><p id="-qps45w_397">These directory mappings ONLY apply to 'EXTERNAL' and 'DOWNGRADED ACID' tables. You can supply 'n' number of mappings to review through the commandline interface as describe above. To provide a consistent set of mappings to ALL jobs, add/modify the following section in the configuration file ie: <code class="code" id="-qps45w_403">default.yaml</code> used for processing.</p><div class="code-block" data-lang="yaml">
translator:
  globalLocationMap:
    /data/my_original_location: &quot;/corp/finance_new_loc&quot;
    /user/hive: &quot;/warehouse/hive&quot;
</div><p id="-qps45w_399">The list will be sorted by the length of the string, then alpha-numerically. This will ensure the deepest nested paths are evaluated FIRST. When a path is matched during evaluation, the process will NOT look and any more paths in the list. Therefore, avoiding possible double evaluations that may result when there are nested paths in the list.</p><p id="-qps45w_400">Paths are evaluated with 'startsWith' on the original path (minus the original namespace). When a match is found, the path 'part' will be replaced with the value specified. The remaining path will remain intact and regardless of the <code class="code" id="-qps45w_404">-rdl</code> setting, the LOCATION element will be included in the tables new CREATE statement.</p></section><section class="chapter"><h2 id="force-external-locations" data-toc="force-external-locations">Force External Locations</h2><p id="-qps45w_405"><code class="code" id="-qps45w_409">-fel|--force-external-location</code></p><p id="-qps45w_406">Under some conditions, the default warehouse directory hierarchy is not honored. We've seen this in HDP 3. The <code class="code" id="-qps45w_410">-rdl</code> option collects the external tables in the default warehouse directory by omitting the LOCATION element in the CREATE statement, relying on the default location. The default location is set at the DATABASE level by <code class="code" id="-qps45w_411">hms-mirror</code>.</p><p id="-qps45w_407">In HDP3, the CREATE statement doesn't honor the 'database' LOCATION element and reverts to the system wide warehouse directory configurations. The <code class="code" id="-qps45w_412">-fel</code> flag will simply include the 'properly' adjusted LOCATION element in the CREATE statement to ensure the tables are created in the desired location. This setting overrides the effects intended by the <code class="code" id="-qps45w_413">-rdl</code> option which intend to place the tables under the stated warehouse locations by omitting the location from the tables definition and relying on the default location specified in the database.</p><p id="-qps45w_408"><code class="code" id="-qps45w_414">-fel</code> will use the original location as a starting point. If <code class="code" id="-qps45w_415">-wd|-ewd</code> are specified, they aren't not used in the translation, but warnings may be issued if the final location doesn't align with the warehouse directory. The effect change in the location when using <code class="code" id="-qps45w_416">-fel</code>, add mappings via <code class="code" id="-qps45w_417">-glm</code>.</p></section><section class="chapter"><h2 id="hdp-3-hive" data-toc="hdp-3-hive">HDP 3 Hive</h2><p id="-qps45w_418">HDP 3 (Hive 3) was an incomplete implementation with regards to complex table 'location' management. When you are working in this environment, add to the cluster configuration (LEFT or RIGHT) the setting: <code class="code" id="-qps45w_425">hdpHive3: true</code>. There is NOT a commandline switch for this. JUst add it to the configuration file you're using to run the application.</p><div class="code-block" data-lang="yaml">
clusters:
  LEFT:
    environment: &quot;LEFT&quot;
    legacyHive: false
    hdpHive3: true
</div><p id="-qps45w_420">HDP3 Hive did NOT have a MANAGEDLOCATION attribute for Databases.</p><p id="-qps45w_421">The LOCATION element tracked the Manage ACID tables and will control where they go.</p><p id="-qps45w_422">This LOCATION will need to be transferred to MANAGEDLOCATION 'after' upgrading to CDP to ensure ACID tables maintain the same behavior. EXTERNAL tables will explicity set there LOCATION element to match the setting in <code class="code" id="-qps45w_426">-ewd</code>.</p><p id="-qps45w_423">Future external tables, when no location is specified, will be created in the <code class="code" id="-qps45w_427">hive.metastore.warehouse.external.dir</code>. This value is global in HDP Hive3 and can NOT be set for individual databases.</p><p id="-qps45w_424">Post upgrade to CDP, you should add a specific directory value at the database level for better control.</p></section><section class="chapter"><h2 id="schema-fix-features" data-toc="schema-fix-features">Schema Fix Features</h2><p id="-qps45w_428">Schema Fix Features are a way to inject special considerations into the replay of a schema between clusters. Each schema is automatically check is a particular 'feature' applies.</p><p id="-qps45w_429">If you find that this features check is causing issues, add the flag <code class="code" id="-qps45w_433">-sf</code> to the application parameters and the feature checks will be skipped.</p><section class="chapter"><h3 id="bad-orc-def" data-toc="bad-orc-def">BAD_ORC_DEF</h3><p id="-qps45w_434"><code class="code" id="-qps45w_442">BAD_ORC_DEF</code> is a feature that corrects poorly executed schema definitions in legacy Hive 1/2 that don't translate into a functioning table in Hive 3. In this case, the legacy definition was defined with:</p><div class="code-block" data-lang="none">
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
</div><p id="-qps45w_436">when it should have been created with:</p><div class="code-block" data-lang="none">
STORED AS ORC
</div><p id="-qps45w_438">The result, when not modified and replayed in Hive 3 is a table that isn't functional. The <code class="code" id="-qps45w_443">BAD_ORC_DEF</code> feature will replace:</p><div class="code-block" data-lang="none">
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n'
</div><p id="-qps45w_440">with:</p><div class="code-block" data-lang="none">
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
</div></section><section class="chapter"><h3 id="bad-rc-def" data-toc="bad-rc-def">BAD_RC_DEF</h3><p id="-qps45w_444"><code class="code" id="-qps45w_452">BAD_RC_DEF</code> is a feature that corrects poorly executed schema definitions in legacy Hive 1/2 that doesn't translate into a functioning table in Hive 3. In this case, the legacy definition was defined with:</p><div class="code-block" data-lang="none">
ROW FORMAT DELIMITED,
    FIELDS TERMINATED BY '|'
 STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
 OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
</div><p id="-qps45w_446">when it should have been created with:</p><div class="code-block" data-lang="none">
STORED AS RCFILE
</div><p id="-qps45w_448">The result, when not modified and replayed in Hive 3 is a table that isn't functional. The <code class="code" id="-qps45w_453">BAD_RC_DEF</code> feature will replace:</p><div class="code-block" data-lang="none">
ROW FORMAT DELIMITED,                              
    FIELDS TERMINATED BY '|'                       
 STORED AS INPUTFORMAT                             
</div><p id="-qps45w_450">with:</p><div class="code-block" data-lang="none">
STORED AS RCFILE
</div></section><section class="chapter"><h3 id="bad-textfile-def" data-toc="bad-textfile-def">BAD_TEXTFILE_DEF</h3><p id="-qps45w_454">Older Textfile schemas somehow are corrupted through subsequent ALTER statements that get the table into a state where you can NOT re-run the contents of <code class="code" id="-qps45w_458">SHOW CREATE TABLE</code>. In this case, the issue is that there is a declaration for <code class="code" id="-qps45w_459">WITH SERDEPROPERTIES</code> along with a <code class="code" id="-qps45w_460">ROW FORMAT DELIMITED</code> clause. These two can NOT exist together. Here is an example of this:</p><div class="code-block" data-lang="none">
ROW FORMAT DELIMITED
     FIELDS TERMINATED BY '|'
     LINES TERMINATED BY '\n'
WITH SERDEPROPERTIES (
     'escape.delim'='\\')
STORED AS INPUTFORMAT
     'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
</div><p id="-qps45w_456">In this case, we need to convert the <code class="code" id="-qps45w_461">ROW FORMAT DELIMITED * TERMINATED BY *</code> values into the <code class="code" id="-qps45w_462">SERDEPROPERTIES</code> and replace it with</p><div class="code-block" data-lang="none">
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
</div></section></section><div class="last-modified">Last modified: 08 October 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="hive-conversions.html" class="navigation-links__prev">Hive Conversions</a><a href="strategies.html" class="navigation-links__next">Strategies</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b419/app.js"></script></body></html>