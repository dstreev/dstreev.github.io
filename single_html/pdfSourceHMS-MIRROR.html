<html><head><meta charset="UTF-8"><script src="https://cdn.jsdelivr.net/npm/prismjs/prism.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-none.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-yaml.min.js"></script><style>
        .article {
            position: relative;
            padding-bottom: 24px;
        }
        

        .bordered-element {
            border: 1px solid #c4c4c4;
            overflow: hidden;
        }
        

        .bordered-element-rounded {
            border: 1px solid #c4c4c4;
            border-radius: 7px;
            overflow: hidden;
        }
        

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        

        .center-text {
            text-align: center;
        }
        

        .code-block {
            overflow: hidden;
            position: relative;
            padding: 0;
            border-radius: 8px;
            font-variant-ligatures: none;
            background-color: rgba(25, 25, 28, .05);
            word-break: break-all;
        }
        

        .container {
            max-width: 100%;
            overflow: hidden;
            page-break-inside: avoid;
            display: block;
        }
        

        .control {
            color: #19191c;
            font-size: 16px;
            font-weight: 670;
        }
        

        .detached {
            margin-block-start: 0;
            margin-block-end: 0;
            margin-bottom: 8px;
        }
        

        .emphasis {
            color: inherit;
            font-weight: inherit;
            font-style: italic;
        }
        

        .flex {
            display: flex;
        }
        

        .header-row {
            background: #e6e6e6;
        }
        

        .image {
            max-width: 100%;
            max-height: 90vh;
            width: auto;
        }
        

        .image-container {
            max-width: 100vw;
        }
        

        .image-size {
            height: auto;
        }
        

        .inline-code {
            border-radius: 4px;
            display: inline;
            padding: 2px 1px;
            font-family: JetBrains Sans,monospace;
            background: #e6e6e6;
        }
        

        .inline-icon {
            vertical-align: middle;
        }
        

        .list {
            list-style-type: disc;
            padding-left: 0;
            margin-left: 15px;
        }
        

        .list-decimal {
            list-style-type: decimal;
        }
        

        .list-item {
            margin-top: 6px;
            margin-bottom: 6px;
            margin-left: 4px;
        }
        

        .main-title {
            padding-bottom: 24px;
            margin-top: 0;
            font-size: 40px;
            margin-block-start: 0;
            margin-block-end: 0;
        }
        

        .note {
            background: rgba(77, 187, 95, .2);
        }
        

        .prism {
            page-break-inside: avoid;
        }
        
        /* PrismJS 1.29.0
https://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript+abap+abnf+actionscript+ada+agda+al+antlr4+apacheconf+apex+apl+applescript+aql+arduino+arff+armasm+arturo+asciidoc+aspnet+asm6502+asmatmel+autohotkey+autoit+avisynth+avro-idl+awk+bash+basic+batch+bbcode+bbj+bicep+birb+bison+bnf+bqn+brainfuck+brightscript+bro+bsl+c+csharp+cpp+cfscript+chaiscript+cil+cilkc+cilkcpp+clojure+cmake+cobol+coffeescript+concurnas+csp+cooklang+coq+crystal+css-extras+csv+cue+cypher+d+dart+dataweave+dax+dhall+diff+django+dns-zone-file+docker+dot+ebnf+editorconfig+eiffel+ejs+elixir+elm+etlua+erb+erlang+excel-formula+fsharp+factor+false+firestore-security-rules+flow+fortran+ftl+gml+gap+gcode+gdscript+gedcom+gettext+gherkin+git+glsl+gn+linker-script+go+go-module+gradle+graphql+groovy+haml+handlebars+haskell+haxe+hcl+hlsl+hoon+http+hpkp+hsts+ichigojam+icon+icu-message-format+idris+ignore+inform7+ini+io+j+java+javadoc+javadoclike+javastacktrace+jexl+jolie+jq+jsdoc+js-extras+json+json5+jsonp+jsstacktrace+js-templates+julia+keepalived+keyman+kotlin+kumir+kusto+latex+latte+less+lilypond+liquid+lisp+livescript+llvm+log+lolcode+lua+magma+makefile+markdown+markup-templating+mata+matlab+maxscript+mel+mermaid+metafont+mizar+mongodb+monkey+moonscript+n1ql+n4js+nand2tetris-hdl+naniscript+nasm+neon+nevod+nginx+nim+nix+nsis+objectivec+ocaml+odin+opencl+openqasm+oz+parigp+parser+pascal+pascaligo+psl+pcaxis+peoplecode+perl+php+phpdoc+php-extras+plant-uml+plsql+powerquery+powershell+processing+prolog+promql+properties+protobuf+pug+puppet+pure+purebasic+purescript+python+qsharp+q+qml+qore+r+racket+cshtml+jsx+tsx+reason+regex+rego+renpy+rescript+rest+rip+roboconf+robotframework+ruby+rust+sas+sass+scss+scala+scheme+shell-session+smali+smalltalk+smarty+sml+solidity+solution-file+soy+sparql+splunk-spl+sqf+sql+squirrel+stan+stata+iecst+stylus+supercollider+swift+systemd+t4-templating+t4-cs+t4-vb+tap+tcl+tt2+textile+toml+tremor+turtle+twig+typescript+typoscript+unrealscript+uorazor+uri+v+vala+vbnet+velocity+verilog+vhdl+vim+visual-basic+warpscript+wasm+web-idl+wgsl+wiki+wolfram+wren+xeora+xml-doc+xojo+xquery+yaml+yang+zig&plugins=highlight-keywords */
code[class*=language-],pre[class*=language-]{color:#000;background:0 0;text-shadow:0 1px #fff;font-family:Consolas,Monaco,'Andale Mono','Ubuntu Mono',monospace;font-size:1em;text-align:left;white-space:pre-wrap;word-spacing:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{text-shadow:none;background:#b3d4fc}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#b3d4fc}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{padding:16px;margin:0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{color:#9a6e3a;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}

        

        .prompt {
            flex-direction: row;
            letter-spacing: .0015em;
            font-size: 16px;
            font-weight: 400;
            line-height: 24px;
            margin-left: 0;
            margin-right: 0;
        }
        

        .prompt-content {
            padding: 15px;
            overflow: hidden;
            flex: 1 1 auto;
        }
        

        .prompt-content-p p {
            margin-block-start: 0;
            margin-block-end: 0;
        }
        

        .prompt-icon {
            flex: 0 0 auto;
            margin-left: 15px;
            margin-top: 15px;
            fill: currentcolor;
            width: 24px;
            height: 24px;
        }
        

        :root {
            width: 95%;
            max-width: 95vw;
            padding: 0 0 0 30px;
        }
        
        body {
            font-family: JetBrains Sans,serif;
        }
        
        a {
            overflow-wrap: anywhere;
            width: 100vw;
        }
        
        
        @font-face {
            font-family: JetBrains Sans;
            src: url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-Light.woff2) format("woff2"), url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-Light.woff) format("woff");
            font-weight: 300;
            font-style: normal;
        }

        @font-face {
            font-family: JetBrains Sans;
            src: url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-Regular.woff2) format("woff2"), url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-Regular.woff) format("woff");
            font-weight: 400;
            font-style: normal;
        }

        @font-face {
            font-family: JetBrains Sans;
            src: url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-SemiBold.woff2) format("woff2"), url(https://resources.jetbrains.com/storage/jetbrains-sans/JetBrainsSans-SemiBold.woff) format("woff");
            font-weight: 600;
            font-style: normal;
        }
        
        
        code {
            display: inline;
            word-break: break-word;
            font-size: 15px;
            line-height: inherit;
            font-variant-ligatures: none;
            font-family: JetBrains Sans,monospace;
            white-space: pre-line;
            overflow-wrap: break-word;
        }
        
        figcaption {
            margin-top: 5px;
        }
        
        h2 {
            padding-top: 16px;
            padding-bottom: 8px;
            margin-block-start: 0;
            margin-block-end: 0;
        }
        
        h3 {
            padding-top: 8px;
            padding-bottom: 8px;
            margin-block-start: 0;
            margin-block-end: 0;
        }
        
        h4 {
            padding-top: 4px;
            padding-bottom: 8px;
            margin-block-start: 0;
            margin-block-end: 0;
        }
        
        p {
            padding: 0;
            border: 0;
            line-height: 25px;
            margin-block-start: 0;
            margin-block-end: 0;
            padding-bottom: 8px;
        }
        
        div {
            display: block;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            page-break-inside: avoid;
        }
        
        th, td {
            border: 1px solid #c4c4c4;
            padding: 10px;
            text-align: left;
            word-break: break-all;
        }
        
        .entry {
            display: grid;
            grid-template-columns: auto max-content;
            grid-template-areas: "chapter page";
            align-items: end;
            gap: 0 .25rem;
            line-height: 25px;
        }
        
        .toc-link-container{
            grid-area: chapter;
            position: relative;
            overflow: hidden;
        }
        
        .toc-link{
            text-decoration: none;
            color: black;
        }
        
        .toc-link-container::after {
            position: absolute;
            padding-left: .25ch;
            content: " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ";
            text-align: right;
        }
        
        .page {
            grid-area: page;
            width: 30px;
            text-align: right;
        }
        

        .tab-content {
            padding: 16px;
        }
        
        .tab-content > *  {
            border: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }
        

        .tab-title  {
            font-size: 20px;
            margin-bottom: 10px;
        }
        

        .table-wrapper {
            overflow: hidden;
            box-sizing: border-box;
            font: inherit;
        }
        

        .tip {
            background-color: rgba(25, 25, 28, .05);
        }
        

        .topic {
            page-break-before: always;
        }
        

        .warning {
            background: rgba(244, 92, 74, .2);
        }
        </style></head><body><div><section class="topic"><div><article class="article"><h1 class="main-title">Table of contents</h1><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#951673673">Introduction</a></div><div class="page">3</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1464374981">Release Notes</a></div><div class="page">4</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#2111190712">Getting Started</a></div><div class="page">11</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1567475278">Setup</a></div><div class="page">12</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-2018830280">Limits</a></div><div class="page">15</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-150235217">Runtime Interfaces</a></div><div class="page">17</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1150912653">Concurrency</a></div><div class="page">19</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1652928064">Web Interface</a></div><div class="page">20</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1332792180">CLI Interface</a></div><div class="page">23</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1805241073">Commandline Help (Options)</a></div><div class="page">24</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1247348560">Output</a></div><div class="page">41</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-2108749216">REST Interface (WIP)</a></div><div class="page">47</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1535710817">Reports</a></div><div class="page">48</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1938912956">Quick Start Scenarios</a></div><div class="page">49</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-2134352060">Pre-Requisites</a></div><div class="page">50</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1124446108">Warning</a></div><div class="page">52</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#970891157">Permissions</a></div><div class="page">54</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1958103884">Linking Cluster Storage Layers</a></div><div class="page">55</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1489882229">Configuration</a></div><div class="page">59</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1461399276">Hive JDBC Drivers and Configuration</a></div><div class="page">61</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#870733106">Example URL's</a></div><div class="page">65</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#703441149">Metastore JDBC Drivers and Configuration</a></div><div class="page">67</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1700610744">Storage Systems (Distributed File Systems)</a></div><div class="page">68</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1116233166">Password Security</a></div><div class="page">70</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#893562735">Memory Settings</a></div><div class="page">73</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#28311084">Default Configuration Template</a></div><div class="page">74</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1624968880">Running</a></div><div class="page">78</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1566128265">Optimizations</a></div><div class="page">87</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1057781433">Tips</a></div><div class="page">91</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-356146741">Location Alignment</a></div><div class="page">92</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1642806840">Databases</a></div><div class="page">100</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1685164704">Warehouse Plans</a></div><div class="page">102</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1477024918">Global Warehouse Plans</a></div><div class="page">104</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1930923410">Database Warehouse Plans</a></div><div class="page">105</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1414560951">Environment Warehouse</a></div><div class="page">106</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-17525952">Hive Conversions</a></div><div class="page">107</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#45251276">Features</a></div><div class="page">110</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#2080784241">Strategies</a></div><div class="page">129</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1171882890">SCHEMA_ONLY</a></div><div class="page">130</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#2109940">DUMP</a></div><div class="page">134</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-415535163">Options</a></div><div class="page">135</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-2049336807">LINKED</a></div><div class="page">139</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1487760827">CONVERT_LINKED</a></div><div class="page">140</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#82350">SQL</a></div><div class="page">141</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#846013616">Options</a></div><div class="page">142</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1900672368">EXPORT_IMPORT</a></div><div class="page">145</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-308799310">Options</a></div><div class="page">147</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#2145539580">HYBRID</a></div><div class="page">148</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1993481707">COMMON</a></div><div class="page">149</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#745346762">STORAGE_MIGRATION</a></div><div class="page">150</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1035902883">ICEBERG_MIGRATION</a></div><div class="page">151</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1731590434">Index of Settings</a></div><div class="page">152</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1862306058">Filter</a></div><div class="page">157</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-332740970">Cluster</a></div><div class="page">158</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1345526795">Transfer</a></div><div class="page">159</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#605563258">Storage Migration</a></div><div class="page">160</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1272712614">Troubleshooting</a></div><div class="page">161</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#166757441">License APLv2</a></div><div class="page">168</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1632746627">Use Cases</a></div><div class="page">173</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#-1522123162">On Prem Legacy Hive to Non-Legacy Hive</a></div><div class="page">174</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#728268470">Scenarios</a></div><div class="page">176</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#1005763858">Cloud to Cloud DR (AWS)</a></div><div class="page">181</div></div><div class="entry"><div class="toc-link-container"><a class="toc-link" href="#672024080">Hybrid Data LakeHouse</a></div><div class="page">185</div></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="951673673">Introduction</h1><p id="951673673#-dyg4s9_3"> is a utility used to bridge the gap between two clusters and migrate <span class="inline-code" id="951673673#-dyg4s9_10">hive</span> <span class="emphasis" id="951673673#-dyg4s9_11">metadata</span>. HMS-Mirror is distributed under the APLv2 (<a href="#166757441">License APLv2</a>) license.</p><p id="951673673#-dyg4s9_4">The application will migrate hive metastore data (metadata) between two clusters. With SQL (<a href="#82350">SQL</a>) and EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>) data strategies, we can move data between the two clusters using Hive SQL.</p><p id="951673673#-dyg4s9_5">As an alternative to using Hive SQL to move data between two clusters, <span class="inline-code" id="951673673#-dyg4s9_15">hms-mirror</span> can build <span class="inline-code" id="951673673#-dyg4s9_16">distcp</span> plans and scripts that can be run in concert with the metadata scripts to complete the migration.</p><p id="951673673#-dyg4s9_6">You start by picking a 'Data Strategy' that fits your needs. A <span class="control" id="951673673#-dyg4s9_17">Data Strategy</span> defines how you're choosing to migrate 'metadata' and 'data' between two clusters. Some of the strategies are 'metadata' only, and some are 'metadata' and 'data'. Others are used to migrate data 'within' a cluster (STORAGE_MIGRATION) to facilitate changes in the storage layer. For example: Move data into an encrypted zone or to a different storage layer, like Ozone.</p><p id="951673673#-dyg4s9_7">There are two interface (working on a third) for <span class="inline-code" id="951673673#-dyg4s9_18">hms-mirror</span>: CLI (<a href="#-1332792180">CLI Interface</a>) and WebUI (<a href="#-1652928064">Web Interface</a>)</p><p id="951673673#-dyg4s9_8">From both interfaces, reports are generated that detail the actions taken by the application. You can direct it to run the conversion scripts automatically or just generate the scripts for you to run later.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1464374981">Release Notes</h1><section><h2 id="-1464374981#known-issues" data-toc="known-issues#Release-Notes.md-known-issues">Known Issues</h2><p id="-1464374981#vruz4l_12">The latest set of known issues can be found here (<a href="https://github.com/cloudera-labs/hms-mirror/issues?q=is%3Aissue+is%3Aopen+label%3Abug">https://github.com/cloudera-labs/hms-mirror/issues?q=is%3Aissue+is%3Aopen+label%3Abug</a>)</p></section><section><h2 id="-1464374981#enhancement-requests" data-toc="enhancement-requests#Release-Notes.md-enhancement-requests">Enhancement Requests</h2><p id="-1464374981#vruz4l_14">The latest set of enhancement requests can be found here (<a href="https://github.com/cloudera-labs/hms-mirror/issues?q=is%3Aissue+is%3Aopen+label%3Aenhancement">https://github.com/cloudera-labs/hms-mirror/issues?q=is%3Aissue+is%3Aopen+label%3Aenhancement</a>).</p><p id="-1464374981#vruz4l_15">If there is something you'd like to see, add a new issue here (<a href="https://github.com/cloudera-labs/hms-mirror/issues">https://github.com/cloudera-labs/hms-mirror/issues</a>)</p></section><section><h2 id="-1464374981#2-2-0-10" data-toc="2-2-0-10#Release-Notes.md-2-2-0-10">2.2.0.10</h2><p id="-1464374981#vruz4l_18"><span class="control" id="-1464374981#vruz4l_24">What's New</span></p><section><h3 id="-1464374981#hive-4-db-owner-ddl-syntax-for-altering-db-onwer-requires-user" data-toc="hive-4-db-owner-ddl-syntax-for-altering-db-onwer-requires-user#Release-Notes.md-hive-4-db-owner-ddl-syntax-for-altering-db-onwer-requires-user">Hive 4 DB OWNER DDL syntax for ALTERing DB ONWER requires 'USER'</h3><p id="-1464374981#vruz4l_26">This changed resulted in a simplification of how we determine what the cluster platform is. Previously we used two attributes (<span class="inline-code" id="-1464374981#vruz4l_30">legacyHive</span> and <span class="inline-code" id="-1464374981#vruz4l_31">hdpHive3</span>) to determine the platform. This information would direct logic around translations and other features.</p><p id="-1464374981#vruz4l_27">Unfortunately, this isn't enough for us to determine all the scenarios we're encountering. These attributes have been replaced with a new attribute call <span class="inline-code" id="-1464374981#vruz4l_32">platformType</span>. A list of the platform types can be found here.</p><p id="-1464374981#vruz4l_28">We will make automatic translations of legacy configurations to the new <span class="inline-code" id="-1464374981#vruz4l_34">platformType</span> attribute. The translation will be pretty basic and result in either the platform type being defined as <span class="inline-code" id="-1464374981#vruz4l_35">HDP2</span> or <span class="inline-code" id="-1464374981#vruz4l_36">CDP_7.1</span>. If you have a more complex configuration, you'll need to adjust the <span class="inline-code" id="-1464374981#vruz4l_37">platformType</span> attribute manually. Future persisted configurations will use the new <span class="inline-code" id="-1464374981#vruz4l_38">platformType</span> attribute and drop the <span class="inline-code" id="-1464374981#vruz4l_39">legacyHive</span> and <span class="inline-code" id="-1464374981#vruz4l_40">hdpHive3</span> attributes.</p></section><section><h3 id="-1464374981#add-property-overrides-to-web-interface" data-toc="add-property-overrides-to-web-interface#Release-Notes.md-add-property-overrides-to-web-interface">Add &quot;Property Overrides&quot; to Web Interface</h3><p id="-1464374981#vruz4l_42">A feature that was late in making it into the Web UI is now here.</p></section><section><h3 id="-1464374981#for-web-ui-service-default-to-prefer-ipv4" data-toc="for-web-ui-service-default-to-prefer-ipv4#Release-Notes.md-for-web-ui-service-default-to-prefer-ipv4">For Web UI Service, default to prefer IPV4</h3><p id="-1464374981#vruz4l_45">To ensure the right IP stack is used when the Web UI starts up, we're forcing this JDK configuration with the Web UI.</p></section><section><h3 id="-1464374981#forcibly-set-java-home-via-duser-home" data-toc="forcibly-set-java-home-via-duser-home#Release-Notes.md-forcibly-set-java-home-via-duser-home">Forcibly set Java Home via -Duser.home</h3><p id="-1464374981#vruz4l_48">We had a few requests and issues with implementations were the target environment isn't always setup with normal user 'home' standards that we can rely on. This change allows us to set the 'home' directory for the user running the application and ensure its translated correctly in hms-mirror for storing and reading configurations, reports, and logs.</p><p id="-1464374981#vruz4l_49">If you are in an environment that doesn't follow user <span class="inline-code" id="-1464374981#vruz4l_51">$HOME</span> standards, you can set the <span class="inline-code" id="-1464374981#vruz4l_52">HOME</span> environment variable to a custom directory <span class="control" id="-1464374981#vruz4l_53">BEFORE</span> starting <span class="inline-code" id="-1464374981#vruz4l_54">hms-mirror</span> to alter the default behavior.</p></section><section><h3 id="-1464374981#cleanup-sql-has-been-added-to-web-reporting-ui" data-toc="cleanup-sql-has-been-added-to-web-reporting-ui#Release-Notes.md-cleanup-sql-has-been-added-to-web-reporting-ui">Cleanup SQL has been added to Web Reporting UI</h3><p id="-1464374981#vruz4l_55">We've added a 'Cleanup SQL' tab to the Web Reporting UI. This will show you the SQL that was generated to clean up the source cluster after the migration. This is useful to see what will be done before you execute the migration.</p><p id="-1464374981#vruz4l_56"><span class="control" id="-1464374981#vruz4l_58">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_57" start="1"><li class="list-item" id="-1464374981#vruz4l_59"><p>DATABASE set OWNER ALTER statement is incorrect (<a href="https://github.com/cloudera-labs/hms-mirror/issues/135">https://github.com/cloudera-labs/hms-mirror/issues/135</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_60"><p>SQL ACID Migrations from HDPHive3 cluster Ordering (<a href="https://github.com/cloudera-labs/hms-mirror/issues/138">https://github.com/cloudera-labs/hms-mirror/issues/138</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_61"><p>DB Location for HDP3 migrations is flipped (<a href="https://github.com/cloudera-labs/hms-mirror/issues/140">https://github.com/cloudera-labs/hms-mirror/issues/140</a>)</p></li></ul></section></section><section><h2 id="-1464374981#2-2-0-9" data-toc="2-2-0-9#Release-Notes.md-2-2-0-9">2.2.0.9</h2><p id="-1464374981#vruz4l_65"><span class="control" id="-1464374981#vruz4l_69">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_66" start="1"><li class="list-item" id="-1464374981#vruz4l_70"><p>Allow process path that doesn't require Metastore Direct Connection (<a href="https://github.com/cloudera-labs/hms-mirror/issues/128">https://github.com/cloudera-labs/hms-mirror/issues/128</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_71"><p>Fix Kerberos Connection Issues to HS2 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/129">https://github.com/cloudera-labs/hms-mirror/issues/129</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_72"><p>Job Progress getting stuck at the end while writing reports (<a href="https://github.com/cloudera-labs/hms-mirror/issues/130">https://github.com/cloudera-labs/hms-mirror/issues/130</a>)</p></li></ul><p id="-1464374981#vruz4l_67"><span class="control" id="-1464374981#vruz4l_76">Enhancements</span></p><p id="-1464374981#vruz4l_68">Increase build dependencies to CDP 7.1.9 SP1. Rework Pass Key Management. Additional details in Connection Validation.</p></section><section><h2 id="-1464374981#2-2-0-8" data-toc="2-2-0-8#Release-Notes.md-2-2-0-8">2.2.0.8</h2><p id="-1464374981#vruz4l_77"><span class="control" id="-1464374981#vruz4l_79">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_78" start="1"><li class="list-item" id="-1464374981#vruz4l_80"><p>Not parsing abfs protocol locations correctly (<a href="https://github.com/cloudera-labs/hms-mirror/issues/124">https://github.com/cloudera-labs/hms-mirror/issues/124</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_81"><p>Database input duplication (<a href="https://github.com/cloudera-labs/hms-mirror/issues/125">https://github.com/cloudera-labs/hms-mirror/issues/125</a>)</p></li></ul></section><section><h2 id="-1464374981#2-2-0-7" data-toc="2-2-0-7#Release-Notes.md-2-2-0-7">2.2.0.7</h2><p id="-1464374981#vruz4l_84"><span class="control" id="-1464374981#vruz4l_86">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_85" start="1"><li class="list-item" id="-1464374981#vruz4l_87"><p>Non Configs blocking WEB UI Create (<a href="https://github.com/cloudera-labs/hms-mirror/issues/122">https://github.com/cloudera-labs/hms-mirror/issues/122</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_88"><p>SCHEMA_ONLY with ALIGNED, DISTCP and Partitions is too granular on partition distcp (<a href="https://github.com/cloudera-labs/hms-mirror/issues/123">https://github.com/cloudera-labs/hms-mirror/issues/123</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_89"><p>Expose rid in web ui (<a href="https://github.com/cloudera-labs/hms-mirror/issues/121">https://github.com/cloudera-labs/hms-mirror/issues/121</a>)</p></li></ul></section><section><h2 id="-1464374981#2-2-0-5" data-toc="2-2-0-5#Release-Notes.md-2-2-0-5">2.2.0.5</h2><p id="-1464374981#vruz4l_93"><span class="control" id="-1464374981#vruz4l_95">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_94" start="1"><li class="list-item" id="-1464374981#vruz4l_96"><p>CLI Setup Issues (<a href="https://github.com/cloudera-labs/hms-mirror/issues/120">https://github.com/cloudera-labs/hms-mirror/issues/120</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_97"><p>Stale Config after several runs (<a href="https://github.com/cloudera-labs/hms-mirror/issues/118">https://github.com/cloudera-labs/hms-mirror/issues/118</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_98"><p>SQL Strategy Transfer Fixes (<a href="https://github.com/cloudera-labs/hms-mirror/issues/117">https://github.com/cloudera-labs/hms-mirror/issues/117</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_99"><p>Partition Reductions for Distcp (<a href="https://github.com/cloudera-labs/hms-mirror/issues/116">https://github.com/cloudera-labs/hms-mirror/issues/116</a>)</p></li></ul></section><section><h2 id="-1464374981#2-2-0-4" data-toc="2-2-0-4#Release-Notes.md-2-2-0-4">2.2.0.4</h2><p id="-1464374981#vruz4l_104"><span class="control" id="-1464374981#vruz4l_106">Bugs (Fixed)</span></p><ul class="list" id="-1464374981#vruz4l_105" start="1"><li class="list-item" id="-1464374981#vruz4l_107"><p>Report Output Directory Issue (<a href="https://github.com/cloudera-labs/hms-mirror/issues/115">https://github.com/cloudera-labs/hms-mirror/issues/115</a>)</p></li><li class="list-item" id="-1464374981#vruz4l_108"><p>Filters Optimizations and Fixes (<a href="https://github.com/cloudera-labs/hms-mirror/issues/116">https://github.com/cloudera-labs/hms-mirror/issues/116</a>)</p></li></ul></section><section><h2 id="-1464374981#2-2-0-2" data-toc="2-2-0-2#Release-Notes.md-2-2-0-2">2.2.0.2</h2><p id="-1464374981#vruz4l_111">This is a big release for <span class="inline-code" id="-1464374981#vruz4l_122">hms-mirror</span>. We've added a Web interface to <span class="inline-code" id="-1464374981#vruz4l_123">hms-mirror</span> that makes it easier to configure and run varies scenarios.</p><p id="-1464374981#vruz4l_112">Along with the Web interface, we've made some significant adjustments to the <span class="inline-code" id="-1464374981#vruz4l_124">hms-mirror</span> engine which is much more complete than the previous release. The engine now supports a wider range of strategies and has a more robust configuration system.</p><p id="-1464374981#vruz4l_113">We do our best to guide you through configurations that make sense, help you build plans and manage complex scenarios.</p><section><h3 id="-1464374981#automatic-configuration-adjustments" data-toc="automatic-configuration-adjustments#Release-Notes.md-automatic-configuration-adjustments">Automatic Configuration Adjustments</h3><p id="-1464374981#vruz4l_125">To ensure that configuration settings are properly set, the application will automatically adjust the configuration settings to match a valid scenario. These changes will be recorded in the 'run status config messages' section and can be seen on reports or the web interface.</p><p id="-1464374981#vruz4l_126">Changes are mostly related to the acceptable strategy configurations. See Location Alignment (<a href="#-356146741">Location Alignment</a>) for more details.</p></section><section><h3 id="-1464374981#property-overrides" data-toc="property-overrides#Release-Notes.md-property-overrides">Property Overrides</h3><p id="-1464374981#vruz4l_128">Not yet available in Web UI. Coming soon issue 111 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/111">https://github.com/cloudera-labs/hms-mirror/issues/111</a>).</p><p id="-1464374981#vruz4l_129">This feature, introduced in the CLI, allows you to add/override Hive properties on the LEFT, RIGHT, or BOTH clusters for custom control of running Hive jobs. Most commonly used with SQL migration strategies.</p></section><section><h3 id="-1464374981#evaluate-partition-locations-and-reset-to-default-location" data-toc="evaluate-partition-locations-and-reset-to-default-location#Release-Notes.md-evaluate-partition-locations-and-reset-to-default-location">Evaluate Partition Locations and Reset to Default Location</h3><p id="-1464374981#vruz4l_131">These properties are no longer valid. An added property called 'translationType' is used to determine this functionality.</p><p id="-1464374981#vruz4l_132">Before the <span class="inline-code" id="-1464374981#vruz4l_134">epl|evaluate-partition-locations</span> would gather partition location information from the Metastore Direct connection to ensure they were aligned. We've adjusted/simplified the concept with <span class="inline-code" id="-1464374981#vruz4l_135">translationType</span>, which defined either <span class="inline-code" id="-1464374981#vruz4l_136">RELATIVE</span> or <span class="inline-code" id="-1464374981#vruz4l_137">ALIGNED</span> strategy types.</p><p id="-1464374981#vruz4l_133">See Location Alignment (<a href="#-356146741">Location Alignment</a>) for more details.</p></section><section><h3 id="-1464374981#concurrency" data-toc="concurrency#Release-Notes.md-concurrency">Concurrency</h3><p id="-1464374981#vruz4l_139">In previous releases using the CLI, concurrency could be set through the configuration files <span class="inline-code" id="-1464374981#vruz4l_141">transfer:concurrency</span> setting. The default was 4 if no setting was provided. This setting in the control file is NO LONGER supported and will be ignored. The new default concurrency setting is <span class="inline-code" id="-1464374981#vruz4l_142">10</span> and can be overridden only during the application startup.</p><p id="-1464374981#vruz4l_140">See Concurrency (<a href="#-1150912653">Concurrency</a>) for more details.</p></section><section><h3 id="-1464374981#global-location-maps" data-toc="global-location-maps#Release-Notes.md-global-location-maps">Global Location Maps</h3><p id="-1464374981#vruz4l_144">Previous releases had a fairly basic implementation of 'Global Location Maps'. These could be supplied through the cli option <span class="inline-code" id="-1464374981#vruz4l_154">-glm</span>, which is still supported, but limited in functionality. The improved implementation work from the concept of building 'Warehouse Plans' which are then used to build the 'Global Location Maps'.</p><p id="-1464374981#vruz4l_145">See Warehouse Plans for more details.</p><p id="-1464374981#vruz4l_146">The <span class="inline-code" id="-1464374981#vruz4l_156">-glm</span> option can take an addition element to identify the mapping for a particular table type. As a result, any configuration files save with this setting will not be loaded and will need to be updated.</p><p id="-1464374981#vruz4l_147">While the <span class="inline-code" id="-1464374981#vruz4l_157">-glm</span> option will still honor the old format of <span class="inline-code" id="-1464374981#vruz4l_158">source_dir=target_dir</span>, the new format is <span class="inline-code" id="-1464374981#vruz4l_159">source_dir:&lt;table_type&gt;:target_dir</span>. The <span class="inline-code" id="-1464374981#vruz4l_160">table_type</span> is a new addition to the configuration and is required for the new implementation. When omitted, the mapping will be created for both EXTERNAL and MANAGED tables.</p><p id="-1464374981#vruz4l_148"><span class="inline-code" id="-1464374981#vruz4l_161">&lt;table_type&gt;</span> can be one of: <span class="inline-code" id="-1464374981#vruz4l_162">EXTERNAL_TABLE</span> or <span class="inline-code" id="-1464374981#vruz4l_163">MANAGED_TABLE</span>.</p><p id="-1464374981#vruz4l_149"><span class="inline-code" id="-1464374981#vruz4l_164">-glm /tpsds_base_dir=EXTERNAL_TABLE:/alt/ext/location</span></p><p id="-1464374981#vruz4l_150"><span class="control" id="-1464374981#vruz4l_165">Old Format</span></p><div class="detached code-block" id="-1464374981#vruz4l_151"><pre><code class="language-yaml">globalLocationMap:
  /tpcds_base_dir: &quot;/alt/ext/location&quot;
  /tpcds_base_dir2/web: &quot;/alt/ext/location&quot;</code></pre></div><p id="-1464374981#vruz4l_152"><span class="control" id="-1464374981#vruz4l_166">New Format</span></p><div class="detached code-block" id="-1464374981#vruz4l_153"><pre><code class="language-yaml">userGlobalLocationMap:
  /tpcds_base_dir:
    EXTERNAL_TABLE: &quot;/alt/ext/location&quot;
  /tpcds_base_dir2/web:
    EXTERNAL_TABLE: &quot;/alt/ext/location&quot;</code></pre></div></section><section><h3 id="-1464374981#jdk-11-support" data-toc="jdk-11-support#Release-Notes.md-jdk-11-support">JDK 11 Support</h3><p id="-1464374981#vruz4l_167">The application now supports JDK 11, as well as JDK 8.</p></section><section><h3 id="-1464374981#kerberos-support-and-platform-libraries" data-toc="kerberos-support-and-platform-libraries#Release-Notes.md-kerberos-support-and-platform-libraries">Kerberos Support and Platform Libraries</h3><p id="-1464374981#vruz4l_168">We are still working to replicate the options available in previous release with regard to Kerberos connections. Currently, <span class="inline-code" id="-1464374981#vruz4l_171">hms-mirror</span> can only support a single Kerberos connection. This is the same as it was previously. <span class="inline-code" id="-1464374981#vruz4l_172">hms-mirror</span> packaging includes the core Hadoop classes required for Kerberos connections pulled from the latest CDP release.</p><p id="-1464374981#vruz4l_169">In the past, we 'could' support kerberos connections to lower versions of Hadoop clusters (HDP and CDH) by running <span class="inline-code" id="-1464374981#vruz4l_173">hms-mirror</span> on a cluster with those hadoop libraries installed and specifying <span class="inline-code" id="-1464374981#vruz4l_174">--hadoop-classpath</span> on the commandline. This is no longer supported, as the packaging required to support the Web and REST interfaces is now different.</p><p id="-1464374981#vruz4l_170">We are investigating the possibility of supporting kerberos connections to lower clusters in the future.</p></section><section><h3 id="-1464374981#metastore-direct-access" data-toc="metastore-direct-access#Release-Notes.md-metastore-direct-access">Metastore Direct Access</h3><p id="-1464374981#vruz4l_175">In later 1.6 releases we introduced a 'Metastore Direct' connection type when defining a LEFT(source) cluster. To help build a more complete picture of locations in the metadata, we found it necessary to gather detailed location information for each partition of the datasets being inspected. Because Hive was so configurable regarding location preferences and the ability to set locations at the partition level, we needed to ensure that the locations were aligned. The only sure way to get this complete picture was to connect directly to the Metastore backend database. We currently support 'MYSQL' and 'POSTGRES' metastore backends. 'Oracle' coming soon.</p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="2111190712">Getting Started</h1><p id="2111190712#-9at6o8_3">Check out the sub-sections to get up and running quickly.</p><section><h2 id="2111190712#video-series" data-toc="video-series#Getting-Started.md-video-series">Video Series</h2><section><h3 id="2111190712#installation" data-toc="installation#Getting-Started.md-installation">Installation</h3></section><section><h3 id="2111190712#introduction" data-toc="introduction#Getting-Started.md-introduction">Introduction</h3></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1567475278">Setup</h1><section><h2 id="1567475278#binary-package" data-toc="binary-package#hms-mirror-setup.md-binary-package">Binary Package</h2><p id="1567475278#-ky5238_7">Download the latest version of <span class="inline-code" id="1567475278#-ky5238_9">hms-mirror-&lt;version&gt;-dist.tar.gz</span> (<a href="https://github.com/cloudera-labs/hms-mirror/releases">https://github.com/cloudera-labs/hms-mirror/releases</a>)</p></section><section><h2 id="1567475278#hms-mirror-setup-from-binary-distribution" data-toc="hms-mirror-setup-from-binary-distribution#hms-mirror-setup.md-hms-mirror-setup-from-binary-distribution">HMS-Mirror Setup from Binary Distribution</h2><p id="1567475278#-ky5238_10">On the edgenode:</p><ul class="list" id="1567475278#-ky5238_11" start="1"><li class="list-item" id="1567475278#-ky5238_14"><p id="1567475278#-ky5238_18">Remove previous install directory <span class="inline-code" id="1567475278#-ky5238_20">rm -rf hms-mirror-install-&lt;version&gt;</span></p><ul class="list" id="1567475278#-ky5238_19" start="1"><li class="list-item" id="1567475278#-ky5238_21"><p>If you don't remove the previous install directory, the default <span class="inline-code" id="1567475278#-ky5238_22">tar</span> behaviour will NOT overwrite the existing directory, hence you won't get the new version.</p></li></ul></li><li class="list-item" id="1567475278#-ky5238_15"><p id="1567475278#-ky5238_23">Expand the tarball <span class="inline-code" id="1567475278#-ky5238_25">tar zxvf hms-mirror-&lt;version&gt;-dist.tar.gz</span>.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="1567475278#-ky5238_26">This produces a child <span class="inline-code" id="1567475278#-ky5238_27">hms-mirror-install-&lt;version&gt;</span> directory.</p></div>
</blockquote>
</li><li class="list-item" id="1567475278#-ky5238_16"><p id="1567475278#-ky5238_28">Make sure you <span class="control" id="1567475278#-ky5238_30">STOP</span> any previous release of <span class="inline-code" id="1567475278#-ky5238_31">hms-mirror</span> before attempting install.</p><ul class="list" id="1567475278#-ky5238_29" start="1"><li class="list-item" id="1567475278#-ky5238_32"><p><span class="inline-code" id="1567475278#-ky5238_33">hms-mirror --stop</span></p></li></ul></li><li class="list-item" id="1567475278#-ky5238_17"><p id="1567475278#-ky5238_34">Two options for installation:</p><ul class="list" id="1567475278#-ky5238_35" start="1"><li class="list-item" id="1567475278#-ky5238_36"><p>As the root user (or <span class="inline-code" id="1567475278#-ky5238_38">sudo</span>), run <span class="inline-code" id="1567475278#-ky5238_39">hms-mirror-install-&lt;version&gt;/setup.sh</span>. This will install the <span class="inline-code" id="1567475278#-ky5238_40">hms-mirror</span> packages in <span class="inline-code" id="1567475278#-ky5238_41">/usr/local/hms-mirror</span> and create symlinks for the executables in <span class="inline-code" id="1567475278#-ky5238_42">/usr/local/bin</span>. At this point, <span class="inline-code" id="1567475278#-ky5238_43">hms-mirror</span> should be available to all user and in the default path.</p></li><li class="list-item" id="1567475278#-ky5238_37"><p>As the local user, run <span class="inline-code" id="1567475278#-ky5238_44">hms-mirror-install-&lt;version&gt;/setup.sh</span>. This will install the <span class="inline-code" id="1567475278#-ky5238_45">hms-mirror</span> packages in <span class="inline-code" id="1567475278#-ky5238_46">$HOME/.hms-mirror</span> and create symlink in <span class="inline-code" id="1567475278#-ky5238_47">$HOME/bin</span>. Ensure <span class="inline-code" id="1567475278#-ky5238_48">$HOME/bin</span> is in the users path and run <span class="inline-code" id="1567475278#-ky5238_49">hms-mirror</span>.</p></li></ul></li></ul><p id="1567475278#-ky5238_12"><span class="emphasis" id="1567475278#-ky5238_50">DO NOT RUN <span class="inline-code" id="1567475278#-ky5238_51">hms-mirror</span> from the installation directory.</span></p><p id="1567475278#-ky5238_13">If you install both options, your environment PATH will determine which one is run. Make note of this because an upgrade may not be reachable.</p></section><section><h2 id="1567475278#quick-start" data-toc="quick-start#hms-mirror-setup.md-quick-start">Quick Start</h2><p id="1567475278#-ky5238_52">After installation (above), run <span class="inline-code" id="1567475278#-ky5238_55">hms-mirror</span> for the particular interface that interests you.</p><div id="1567475278#-ky5238_53"><div class="detached" id="1567475278#-ky5238_56"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><p>Run: </p><p id="1567475278#-ky5238_58"><span class="inline-code" id="1567475278#-ky5238_60">hms-mirror --service</span></p><p id="1567475278#-ky5238_59">Open a browser to <span class="inline-code" id="1567475278#-ky5238_61">http://hostname:8090/hms-mirror</span> to access the Web UI.</p></div></div><div class="detached" id="1567475278#-ky5238_57"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p>`hms-mirror` requires a configuration file describing the LEFT (source) and RIGHT (target) cluster connections. There are two ways to create the config: </p><ul class="list" id="1567475278#-ky5238_62" start="1"><li class="list-item" id="1567475278#-ky5238_63"><p>Use the default config template (<a href="#28311084">Default Configuration Template</a>)) as a starting point. Edit and place a copy here <span class="inline-code" id="1567475278#-ky5238_66">$HOME/.hms-mirror/cfg/default.yaml</span>.</p></li><li class="list-item" id="1567475278#-ky5238_64"><p><span class="inline-code" id="1567475278#-ky5238_67">hms-mirror --setup</span> - Prompts a series of questions about the LEFT and RIGHT clusters to build the default configuration file.</p></li></ul></div></div></div><p id="1567475278#-ky5238_54">If either or both clusters are Kerberized, please review the detailed configuration guidance here (<a href="#-1624968880#running-against-a-legacy-non-cdp-kerberized-hiveserver2">&quot;Running Against a LEGACY (Non-CDP) Kerberized HiveServer2&quot; in &quot;Running&quot;</a>) and here (<a href="#-1624968880#kerberized-connections">&quot;Kerberized Connections&quot; in &quot;Running&quot;</a>).</p></section><section><h2 id="1567475278#general-guidance" data-toc="general-guidance#hms-mirror-setup.md-general-guidance">General Guidance</h2><ul class="list" id="1567475278#-ky5238_70" start="1"><li class="list-item" id="1567475278#-ky5238_73"><p>Run <span class="inline-code" id="1567475278#-ky5238_74">hms-mirror</span> from the RIGHT cluster on an Edge Node.</p></li></ul><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="1567475278#-ky5238_75"><span class="inline-code" id="1567475278#-ky5238_76">hms-mirror</span> is built (default setting) with CDP libraries and will connect natively using those libraries. The edge node should have the hdfs client installed and configured for that cluster. The application will use this connection for some migration strategies.</p></div>
</blockquote>
<ul class="list" id="1567475278#-ky5238_72" start="1"><li class="list-item" id="1567475278#-ky5238_77"><p>If running from the LEFT cluster, note that the <span class="inline-code" id="1567475278#-ky5238_80">-ro/--read-only</span> feature examines HDFS on the RIGHT cluster. The HDFS client on the LEFT cluster may not support this access.</p></li><li class="list-item" id="1567475278#-ky5238_78"><p>Connecting to HS2 through KNOX (in both clusters, if possible) reduces the complexities of the connection by removing Kerberos from the picture.</p></li><li class="list-item" id="1567475278#-ky5238_79"><p>The libraries will only support a Kerberos connection to a 'single' version of Hadoop at a time. This is relevant for 'Kerberized' connections to Hive Server 2. The default libraries will support a kerberized connection to a CDP clusters HS2 and HDFS. If the LEFT (source) cluster is Kerberized, including HS2, you will need to make some adjustments. </p><ul class="list" id="1567475278#-ky5238_81" start="1"><li class="list-item" id="1567475278#-ky5238_82"><p>The LEFT clusters HS2 needs to support any auth mechanism BUT Kerberos.</p></li><li class="list-item" id="1567475278#-ky5238_83"><p>Use an Ambari Group to setup an independent HS2 instance for this exercise or use KNOX.</p></li></ul></li></ul></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-2018830280">Limits</h1><p id="-2018830280#nkc11e_3">Let's cover some practical limits of <span class="inline-code" id="-2018830280#nkc11e_12">hms-mirror</span> in the area of scale.</p><p id="-2018830280#nkc11e_4"><span class="inline-code" id="-2018830280#nkc11e_13">hms-mirror</span> is designed to migrate databases. While you can migrate 'every' databases in a schema using the <span class="inline-code" id="-2018830280#nkc11e_14">dbRegEx</span> we don't recommend this. You should construct a plan to migrate databases individually.</p><p id="-2018830280#nkc11e_5">The architecture of <span class="inline-code" id="-2018830280#nkc11e_15">hms-mirror</span> does everything in memory. Each table and partition consumes memory and contributes to the load placed on systems we're using.</p><p id="-2018830280#nkc11e_6">We've done practical tests to do a SCHEMA_ONLY migration of 10k tables between two clusters. On our relatively modest hardware, we accomplished this in about 8 minutes with the default memory settings.</p><p id="-2018830280#nkc11e_7">Writing the reports, which happens at the end takes time. It this case 1-2 minutes.</p><p id="-2018830280#nkc11e_8">If you need to process more than what we've tested, please do a 'Dry-Run' first. Watch the memory of the application on the host where its running. You may need to increase the memory profile of the application and try again. See Memory Settings (<a href="#893562735">Memory Settings</a>)</p><section><h2 id="-2018830280#impact-on-source-and-target-systems" data-toc="impact-on-source-and-target-systems#Limits.md-impact-on-source-and-target-systems">Impact on Source and Target Systems</h2><p id="-2018830280#nkc11e_17"><span class="inline-code" id="-2018830280#nkc11e_19">hms-mirror</span> uses HiveServer2 connections to 'extract' and 'replay' schema's between systems. It also utilizes Metastore Direct connections to pull partition level details that can't be efficiently collected through the HiveServer2 SQL interface.</p><p id="-2018830280#nkc11e_18">The concurrency (default of 10), will act like 10 users making a whole lot of DDL requests. The impact to existing workloads is a possibility. If you have a large amount of data(metadata) to migrate/extract and you're concerned about the impact to the user base, you should 'isolate' the HiveServer2 AND Hive Metastore for this process. That could mean setting up an additional HiveServer2/Metastore pair that is used specifically by 'hms-mirror'.</p></section><section><h2 id="-2018830280#databases-with-a-large-number-of-tables" data-toc="databases-with-a-large-number-of-tables#Limits.md-databases-with-a-large-number-of-tables">Databases with a Large Number of Tables</h2><p id="-2018830280#nkc11e_20">As we mentioned, migrations are a database at a time. If the table in the database exceed some of these limits, consider using various <span class="inline-code" id="-2018830280#nkc11e_21">table filters</span> to process filtered lists of tables in the database at a time.</p></section><section><h2 id="-2018830280#reports" data-toc="reports#Limits.md-reports">Reports</h2><p id="-2018830280#nkc11e_22">Reports are created at the database level as well. When the database has a large number of tables, these reports can be extremely long.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-150235217">Runtime Interfaces</h1><p id="-150235217#z83xm97_3">The classic 'hms-mirror' interface is the command line interface (CLI). This is the interface that most users have used in the past. Support for this is still provided but is quickly being replaced by the Web interface.</p><p id="-150235217#z83xm97_4">The CLI interface requires users to define a configuration file that the is used to control connection endpoints, and cluster attributes. Runtime operations are controlled by command line options that are passed to the application.</p><p id="-150235217#z83xm97_5">The Web interface provides a much more complete experience for users and allows configurations to be built and validated in a more interactive way. The Web interface is the preferred interface for users who are new to the application and we encourage existing users to adopt this interface as well since it's capabilities and usability a vast improvement over the CLI.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>We understand that a big part of the attraction to the CLI was its simple commandline interface and feedback UI. And under many circumstances, the CLI was a natural choice due to security restrictions and port exposure in CDP environments. However, the Web interface offers a much more robust and user-friendly experience. See below for some suggestions on how to gain access to the Web Interface in environments with security restrictions.</p></div>
</blockquote>
<section><h2 id="-150235217#accessing-the-web-interface-in-secure-environments" data-toc="accessing-the-web-interface-in-secure-environments#Runtime-Interfaces.md-accessing-the-web-interface-in-secure-environments">Accessing the Web Interface in Secure Environments</h2><p id="-150235217#z83xm97_8">Ports for the Web Interface may not be available in secure enviroments. By default, the Web Interface is available on port '8090'.</p><section><h3 id="-150235217#option-1-alternate-port" data-toc="option-1-alternate-port#Runtime-Interfaces.md-option-1-alternate-port">Option #1: Alternate Port</h3><p id="-150235217#z83xm97_12">If '8090' isn't available, you can change the port by adding the following to the service start up command:</p><div class="detached code-block" id="-150235217#z83xm97_13"><pre><code class="language-bash">hms-mirror --service --server.port=&lt;new_port&gt;</code></pre></div></section><section><h3 id="-150235217#option-2-ssh-tunnel" data-toc="option-2-ssh-tunnel#Runtime-Interfaces.md-option-2-ssh-tunnel">Option #2: SSH Tunnel</h3><p id="-150235217#z83xm97_14">If you can SSH into the host where the service is running, you can most likely create a tunnel to the service port that will allow you to access the Web Interface. Here's an example:</p><div class="detached code-block" id="-150235217#z83xm97_15"><pre><code class="language-bash">ssh -L 8090:&lt;remote_host&gt;:8090 &lt;user&gt;@&lt;remote_host&gt;</code></pre></div><p id="-150235217#z83xm97_16">This will create a tunnel from your local machine to the remote host. Any traffic you send to <span class="inline-code" id="-150235217#z83xm97_18">localhost:8090</span> will be forwarded to the remote host's port <span class="inline-code" id="-150235217#z83xm97_19">8090</span>. Once the tunnel is established, you can open a browser and navigate to <span class="inline-code" id="-150235217#z83xm97_20">http://localhost:8090/hms-mirror</span> to access the Web Interface.</p><p id="-150235217#z83xm97_17">Here is a good article on SSH Tunnels: SSH Tunneling (<a href="https://www.ssh.com/ssh/tunneling/example">https://www.ssh.com/ssh/tunneling/example</a>)</p></section><section><h3 id="-150235217#option-3-dynamic-port-forwarding" data-toc="option-3-dynamic-port-forwarding#Runtime-Interfaces.md-option-3-dynamic-port-forwarding">Option #3: Dynamic Port Forwarding</h3><p id="-150235217#z83xm97_22">Again, this method relies on SSH access to the remote host. It also requires advanced SOCKS configuration in your browser. You first create a dynamic port forward with SSH:</p><div class="detached code-block" id="-150235217#z83xm97_23"><pre><code class="language-bash">ssh -D &lt;choose_a_port&gt; &lt;user&gt;@&lt;remote_host&gt;</code></pre></div><p id="-150235217#z83xm97_24">Once that tunnel is established, you can configure your browser to use a SOCKS proxy on <span class="inline-code" id="-150235217#z83xm97_26">localhost:&lt;choose_a_port&gt;</span>. This will allow you to access the Web Interface through the tunnel.</p><p id="-150235217#z83xm97_25">To access the Web Interface, you would navigate to <span class="inline-code" id="-150235217#z83xm97_27">http://&lt;remote_host&gt;:8090/hms-mirror</span> (SOCKS proxy will handle the routing).</p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1150912653">Concurrency</h1><p id="-1150912653#-qjdpf_3">This controls how many parallel operations can be performed at once. The default is <span class="inline-code" id="-1150912653#-qjdpf_8">10</span> and can be overridden during application startup.</p><p id="-1150912653#-qjdpf_4">The 'concurrency' setting was previously in the configuration file, but is now only set at startup. The setting in the configuration file will be ignored.</p><div id="-1150912653#-qjdpf_5"><div class="detached" id="-1150912653#WebUI"><div class="tab-title"><div>WebUI</div></div><div class="bordered-element tab-content"><p id="-1150912653#-qjdpf_11">To adjust the concurrency setting for the Web Service, add <span class="inline-code" id="-1150912653#-qjdpf_12">hms-mirror.concurrency.max-threads=n</span> to the startup command.</p></div></div><div class="detached" id="-1150912653#cli"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p id="-1150912653#-qjdpf_13">To adjust the concurrency setting, use the <span class="inline-code" id="-1150912653#-qjdpf_14">-c|--concurrency</span> option when starting the application.</p></div></div></div><p id="-1150912653#-qjdpf_6">This setting dictates the number of connections made to the various endpoints like both Hive Server 2's (LEFT and RIGHT) and the Metastore Direct connections.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-1150912653#-qjdpf_15">These services need to be able to support these connections.</p></div>
</blockquote>
</article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1652928064">Web Interface</h1><section><h2 id="-1652928064#starting" data-toc="starting#Web-Interface.md-starting">Starting</h2><p id="-1652928064#-sss3tc_8">The <span class="inline-code" id="-1652928064#-sss3tc_16">hms-mirror</span> web application is started by running:</p><div class="detached code-block" id="-1652928064#-sss3tc_9"><pre><code class="language-bash">hms-mirror --service</code></pre></div><p id="-1652928064#-sss3tc_10">This will start the web application on the default port of <span class="inline-code" id="-1652928064#-sss3tc_17">8090</span>. The port can be changed by using the <span class="inline-code" id="-1652928064#-sss3tc_18">--server.port</span> option during startup.</p><div class="detached code-block" id="-1652928064#-sss3tc_11"><pre><code class="language-bash">hms-mirror --service --server.port=8080</code></pre></div><p id="-1652928064#-sss3tc_12">Point your browser to <span class="inline-code" id="-1652928064#-sss3tc_19">http://server-host:8090/hms-mirror</span> to access the web application.</p><p id="-1652928064#-sss3tc_13">The web application will use the user home directory to store configuration files, logs, and reports. This isn't as much a concern as it is for the <span class="inline-code" id="-1652928064#-sss3tc_20">cli</span> version since the web application will manage all this from the user's browser.</p><p id="-1652928064#-sss3tc_14">The web application works on the premise of a 'session'. Although the application is stateless, the session is used to store the user's configuration and state while they are using the application. This allows the user to navigate the application and make changes without losing their work. Currently, the session is global throughout the application and is not tied to a specific user browser session. An 'hms-mirror' session is NOT synonymous with a browser session.</p><p id="-1652928064#-sss3tc_15">An instance of the web application can only run ONE session at a time.</p></section><section><h2 id="-1652928064#stopping" data-toc="stopping#Web-Interface.md-stopping">Stopping</h2><p id="-1652928064#-sss3tc_21"><span class="inline-code" id="-1652928064#-sss3tc_22">hms-mirror --stop</span></p></section><section><h2 id="-1652928064#security" data-toc="security#Web-Interface.md-security">Security</h2><p id="-1652928064#-sss3tc_23">Coming Soon...</p></section><section><h2 id="-1652928064#where-to-start" data-toc="where-to-start#Web-Interface.md-where-to-start">Where to Start?</h2><p id="-1652928064#-sss3tc_24">There are three methods to building/loading a configuration. From the main page, select 'Initialize'.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1652928064#-sss3tc_25" alt="web_init_menu" title="web_init_menu" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/web_init_menu.png" width="1150" height="618"><figcaption class="center-text">web_init_menu</figcaption></figure></div><p id="-1652928064#-sss3tc_26">This will bring up the 'Initialize' page where one of the following options can be selected.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1652928064#-sss3tc_27" alt="web_init.png" title="web_init.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/web_init.png" width="936" height="246"><figcaption class="center-text">web_init.png</figcaption></figure></div><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-1652928064#-sss3tc_32">Configurations are specific to a Data Strategy (<a href="#2080784241">Strategies</a>). Once created, you can NOT change the Data Strategy but, you can clone the configuration and change the Data Strategy (last option).</p></div>
</blockquote>
<section><h3 id="-1652928064#pick-an-existing-configuration" data-toc="pick-an-existing-configuration#Web-Interface.md-pick-an-existing-configuration">Pick an Existing Configuration</h3><p id="-1652928064#-sss3tc_34">Load a previously saved configuration. Changes can be made and saved back to the same file or a new file.</p></section><section><h3 id="-1652928064#create-a-new-configuration" data-toc="create-a-new-configuration#Web-Interface.md-create-a-new-configuration">Create a New Configuration</h3><p id="-1652928064#-sss3tc_35">Create a new configuration from scratch.</p></section><section><h3 id="-1652928064#clone-an-existing-configuration" data-toc="clone-an-existing-configuration#Web-Interface.md-clone-an-existing-configuration">Clone an Existing Configuration</h3><p id="-1652928064#-sss3tc_36">Using an existing configuration, clone it and change the Data Strategy. This will allow you to maintain any previously configured endpoints to Hive Server 2 and Metastore Direct connections.</p></section></section><section><h2 id="-1652928064#managing-the-session-configuration" data-toc="managing-the-session-configuration#Web-Interface.md-managing-the-session-configuration">Managing the Session Configuration</h2><p id="-1652928064#-sss3tc_37">There are 2 states for a session. The in-memory state and the persisted state.</p><p id="-1652928064#-sss3tc_38">When you choose to 'Edit' a session configuration, you'll need to 'Save' the configuration for those changes to be applied to the session. 'Saving' the configuration will ONLY update the in-memory state of the session.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1652928064#-sss3tc_39" alt="session_mngt.png" title="session_mngt.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/session_mngt.png" width="970" height="526"><figcaption class="center-text">session_mngt.png</figcaption></figure></div><p id="-1652928064#-sss3tc_40">You will need to 'Persist' the session to save it for future use.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1332792180">CLI Interface</h1><p id="-1332792180#z9wtppg_3">This is the legacy interface and is a Command Line interface that you run from a terminal.</p><p id="-1332792180#z9wtppg_4">First you need to build a configuration file <span class="inline-code" id="-1332792180#z9wtppg_7">default.yaml</span>, that's placed in <span class="inline-code" id="-1332792180#z9wtppg_8">${HOME}/.hms-mirror/cfg</span>. When you follow the above location and name, this configuration is loaded by default upon application start.</p><p id="-1332792180#z9wtppg_5">To control the run behavior of the CLI, you can add many commandline parameters. The application output is describe here (<a href="#1247348560">Output</a>).</p><p id="-1332792180#z9wtppg_6">The CLI and Web versions use the same engine and share the same output/results. Although, the web interface has an online viewer for the results. With the CLI version, you need to collect the output, possibly bring it to you localhost and view it through a 'markdown' viewer.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1805241073">Commandline Help (Options)</h1><div class="detached code-block" id="1805241073#-q0pv8h_3"><pre><code class="language-none">usage: hms-mirror &lt;options&gt;
                  version:1.6.5.0-SNAPSHOT
Hive Metastore Migration Utility
 -accept,--accept                                              Accept ALL confirmations and silence
                                                               prompts
 -ap,--acid-partition-count &lt;limit&gt;                            Set the limit of partitions that the
                                                               ACID strategy will work with. '-1'
                                                               means no-limit.
 -asm,--avro-schema-migration                                  Migrate AVRO Schema Files referenced
                                                               in TBLPROPERTIES by
                                                               'avro.schema.url'.  Without migration
                                                               it is expected that the file will
                                                               exist on the other cluster and match
                                                               the 'url' defined in the schema DDL.
                                                               If it's not present, schema creation
                                                               will FAIL.
                                                               Specifying this option REQUIRES the
                                                               LEFT and RIGHT cluster to be LINKED.
                                                               See docs:
                                                               https://github.com/cloudera-labs/hms-
                                                               mirror#linking-clusters-storage-layer
                                                               s
 -at,--auto-tune                                               Auto-tune Session Settings for
                                                               SELECT's and DISTRIBUTION for
                                                               Partition INSERT's.
 -cfg,--config &lt;filename&gt;                                      Config with details for the
                                                               HMS-Mirror.  Default:
                                                               $HOME/.hms-mirror/cfg/default.yaml
 -cine,--create-if-not-exist                                   CREATE table/partition statements
                                                               will be adjusted to include 'IF NOT
                                                               EXISTS'.  This will ensure all
                                                               remaining sql statements will be run.
                                                               This can be used to sync partition
                                                               definitions for existing tables.
 -cs,--common-storage &lt;storage-path&gt;                           Common Storage used with Data
                                                               Strategy HYBRID, SQL, EXPORT_IMPORT.
                                                               This will change the way these
                                                               methods are implemented by using the
                                                               specified storage location as an
                                                               'common' storage point between two
                                                               clusters.  In this case, the cluster
                                                               do NOT need to be 'linked'.  Each
                                                               cluster DOES need to have access to
                                                               the location and authorization to
                                                               interact with the location.  This may
                                                               mean additional configuration
                                                               requirements for 'hdfs' to ensure
                                                               this seamless access.
 -cto,--compress-test-output                                   Data movement (SQL/STORAGE_MIGRATION)
                                                               of TEXT based file formats will be
                                                               compressed in the new table.
 -d,--data-strategy &lt;strategy&gt;                                 Specify how the data will follow the
                                                               schema. [DUMP, SCHEMA_ONLY, LINKED,
                                                               SQL, EXPORT_IMPORT, HYBRID,
                                                               CONVERT_LINKED, STORAGE_MIGRATION,
                                                               COMMON, ICEBERG_CONVERSION]
 -da,--downgrade-acid                                          Downgrade ACID tables to EXTERNAL
                                                               tables with purge.
 -db,--database &lt;databases&gt;                                    Comma separated list of Databases
                                                               (upto 100).
 -dbo,--database-only                                          Migrate the Database definitions as
                                                               they exist from LEFT to RIGHT
 -dbp,--db-prefix &lt;prefix&gt;                                     Optional: A prefix to add to the
                                                               RIGHT cluster DB Name. Usually used
                                                               for testing.
 -dbr,--db-rename &lt;rename&gt;                                     Optional: Rename target db to ...
                                                               This option is only valid when '1'
                                                               database is listed in `-db`.
 -dbRegEx,--database-regex &lt;regex&gt;                             RegEx of Database to include in
                                                               process.
 -dc,--distcp &lt;flow-direction default:PULL&gt;                    Build the 'distcp' workplans.
                                                               Optional argument (PULL, PUSH) to
                                                               define which cluster is running the
                                                               distcp commands.  Default is PULL.
 -dp,--decrypt-password &lt;encrypted-password&gt;                   Used this in conjunction with '-pkey'
                                                               to decrypt the generated passcode
                                                               from `-p`.
 -ds,--dump-source &lt;source&gt;                                    Specify which 'cluster' is the source
                                                               for the DUMP strategy (LEFT|RIGHT).
 -dtd,--dump-test-data                                         Used to dump a data set that can be
                                                               feed into the process for testing.
 -e,--execute                                                  Execute actions request, without this
                                                               flag the process is a dry-run.
 -ep,--export-partition-count &lt;limit&gt;                          Set the limit of partitions that the
                                                               EXPORT_IMPORT strategy will work
                                                               with.
 -epl,--evaluate-partition-location                            For SCHEMA_ONLY and DUMP
                                                               data-strategies, review the partition
                                                               locations and build partition
                                                               metadata calls to create them is they
                                                               can't be located via 'MSCK'.
 -ewd,--external-warehouse-directory &lt;path&gt;                    The external warehouse directory
                                                               path.  Should not include the
                                                               namespace OR the database directory.
                                                               This will be used to set the LOCATION
                                                               database option.
 -f,--flip                                                     Flip the definitions for LEFT and
                                                               RIGHT.  Allows the same config to be
                                                               used in reverse.
 -fel,--force-external-location                                Under some conditions, the LOCATION
                                                               element for EXTERNAL tables is
                                                               removed (ie: -rdl).  In which case we
                                                               rely on the settings of the database
                                                               definition to control the EXTERNAL
                                                               table data location.  But for some
                                                               older Hive versions, the LOCATION
                                                               element in the database is NOT
                                                               honored.  Even when the database
                                                               LOCATION is set, the EXTERNAL table
                                                               LOCATION defaults to the system wide
                                                               warehouse settings.  This flag will
                                                               ensure the LOCATION element remains
                                                               in the CREATE definition of the table
                                                               to force it's location.
 -glm,--global-location-map &lt;key=value&gt;                        Comma separated key=value pairs of
                                                               Locations to Map. IE:
                                                               /myorig/data/finance=/data/ec/finance
                                                               . This reviews 'EXTERNAL' table
                                                               locations for the path
                                                               '/myorig/data/finance' and replaces
                                                               it with '/data/ec/finance'.  Option
                                                               can be used alone or with -rdl. Only
                                                               applies to 'EXTERNAL' tables and if
                                                               the tables location doesn't contain
                                                               one of the supplied maps, it will be
                                                               translated according to -rdl rules if
                                                               -rdl is specified.  If -rdl is not
                                                               specified, the conversion for that
                                                               table is skipped.
 -h,--help                                                     Help
 -ip,--in-place                                                Downgrade ACID tables to EXTERNAL
                                                               tables with purge.
 -is,--intermediate-storage &lt;storage-path&gt;                     Intermediate Storage used with Data
                                                               Strategy HYBRID, SQL, EXPORT_IMPORT.
                                                               This will change the way these
                                                               methods are implemented by using the
                                                               specified storage location as an
                                                               intermediate transfer point between
                                                               two clusters.  In this case, the
                                                               cluster do NOT need to be 'linked'.
                                                               Each cluster DOES need to have access
                                                               to the location and authorization to
                                                               interact with the location.  This may
                                                               mean additional configuration
                                                               requirements for 'hdfs' to ensure
                                                               this seamless access.
 -itpo,--iceberg-table-property-overrides &lt;key=value&gt;          Comma separated key=value pairs of
                                                               Iceberg Table Properties to
                                                               set/override.
 -iv,--iceberg-version &lt;version&gt;                               Specify the Iceberg Version to use.
                                                               Specify 1 or 2.  Default is 2.
 -ltd,--load-test-data &lt;file&gt;                                  Use the data saved by the `-dtd`
                                                               option to test the process.
 -ma,--migrate-acid &lt;bucket-threshold (2)&gt;                     Migrate ACID tables (if strategy
                                                               allows). Optional:
                                                               ArtificialBucketThreshold count that
                                                               will remove the bucket definition if
                                                               it's below this.  Use this as a way
                                                               to remove artificial bucket
                                                               definitions that were added
                                                               'artificially' in legacy Hive.
                                                               (default: 2)
 -mao,--migrate-acid-only &lt;bucket-threshold (2)&gt;               Migrate ACID tables ONLY (if strategy
                                                               allows). Optional:
                                                               ArtificialBucketThreshold count that
                                                               will remove the bucket definition if
                                                               it's below this.  Use this as a way
                                                               to remove artificial bucket
                                                               definitions that were added
                                                               'artificially' in legacy Hive.
                                                               (default: 2)
 -mnn,--migrate-non-native &lt;arg&gt;                               Migrate Non-Native tables (if
                                                               strategy allows). These include table
                                                               definitions that rely on external
                                                               connection to systems like: HBase,
                                                               Kafka, JDBC
 -mnno,--migrate-non-native-only                               Migrate Non-Native tables (if
                                                               strategy allows). These include table
                                                               definitions that rely on external
                                                               connection to systems like: HBase,
                                                               Kafka, JDBC
 -np,--no-purge                                                For SCHEMA_ONLY, COMMON, and LINKED
                                                               data strategies set RIGHT table to
                                                               NOT purge on DROP
 -o,--output-dir &lt;outputdir&gt;                                   Output Directory (default:
                                                               $HOME/.hms-mirror/reports/&lt;yyyy-MM-dd
                                                               _HH-mm-ss&gt;
 -p,--password &lt;password&gt;                                      Used this in conjunction with '-pkey'
                                                               to generate the encrypted password
                                                               that you'll add to the configs for
                                                               the JDBC connections.
 -pkey,--password-key &lt;password-key&gt;                           The key used to encrypt / decrypt the
                                                               cluster jdbc passwords.  If not
                                                               present, the passwords will be
                                                               processed as is (clear text) from the
                                                               config file.
 -po,--property-overrides &lt;key=value&gt;                          Comma separated key=value pairs of
                                                               Hive properties you wish to
                                                               set/override.
 -pol,--property-overrides-left &lt;key=value&gt;                    Comma separated key=value pairs of
                                                               Hive properties you wish to
                                                               set/override for LEFT cluster.
 -por,--property-overrides-right &lt;key=value&gt;                   Comma separated key=value pairs of
                                                               Hive properties you wish to
                                                               set/override for RIGHT cluster.
 -q,--quiet                                                    Reduce screen reporting output.  Good
                                                               for background processes with output
                                                               redirects to a file
 -rdl,--reset-to-default-location                              Strip 'LOCATION' from all target
                                                               cluster definitions.  This will allow
                                                               the system defaults to take over and
                                                               define the location of the new
                                                               datasets.
 -replay,--replay &lt;report-directory&gt;                           Use to replay process from the report
                                                               output.
 -rid,--right-is-disconnected                                  Don't attempt to connect to the
                                                               'right' cluster and run in this mode
 -ro,--read-only                                               For SCHEMA_ONLY, COMMON, and LINKED
                                                               data strategies set RIGHT table to
                                                               NOT purge on DROP. Intended for use
                                                               with replication distcp strategies
                                                               and has restrictions about existing
                                                               DB's on RIGHT and PATH elements.  To
                                                               simply NOT set the purge flag for
                                                               applicable tables, use -np.
 -rr,--reset-right                                             Use this for testing to remove the
                                                               database on the RIGHT using CASCADE.
 -s,--sync                                                     For SCHEMA_ONLY, COMMON, and LINKED
                                                               data strategies.  Drop and Recreate
                                                               Schema's when different.  Best to use
                                                               with RO to ensure table/partition
                                                               drops don't delete data. When used
                                                               WITHOUT `-tf` it will compare all the
                                                               tables in a database and sync
                                                               (bi-directional).  Meaning it will
                                                               DROP tables on the RIGHT that aren't
                                                               in the LEFT and ADD tables to the
                                                               RIGHT that are missing.  When used
                                                               with `-ro`, table schemas can be
                                                               updated by dropping and recreating.
                                                               When used with `-tf`, only the tables
                                                               that match the filter (on both sides)
                                                               will be considered.
 -sdpi,--sort-dynamic-partition-inserts                        Used to set
                                                               `hive.optimize.sort.dynamic.partition
                                                               ` in TEZ for optimal partition
                                                               inserts.  When not specified, will
                                                               use prescriptive sorting by adding
                                                               'DISTRIBUTE BY' to transfer SQL.
                                                               default: false
 -sf,--skip-features                                           Skip Features evaluation.
 -slc,--skip-link-check                                        Skip Link Check. Use when going
                                                               between or to Cloud Storage to avoid
                                                               having to configure hms-mirror with
                                                               storage credentials and libraries.
                                                               This does NOT preclude your Hive
                                                               Server 2 and compute environment from
                                                               such requirements.
 -slt,--skip-legacy-translation                                Skip Schema Upgrades and Serde
                                                               Translations
 -smn,--storage-migration-namespace &lt;namespace&gt;                Optional: Used with the 'data
                                                               strategy STORAGE_MIGRATION to specify
                                                               the target namespace.
 -so,--skip-optimizations                                      Skip any optimizations during data
                                                               movement, like dynamic sorting or
                                                               distribute by
 -sp,--sql-partition-count &lt;limit&gt;                             Set the limit of partitions that the
                                                               SQL strategy will work with. '-1'
                                                               means no-limit.
 -sql,--sql-output                                             &lt;deprecated&gt;.  This option is no
                                                               longer required to get SQL out in a
                                                               report.  That is the default
                                                               behavior.
 -ssc,--skip-stats-collection                                  Skip collecting basic FS stats for a
                                                               table.  This WILL affect the
                                                               optimizer and our ability to
                                                               determine the best strategy for
                                                               moving data.
 -su,--setup                                                   Setup a default configuration file
                                                               through a series of questions
 -tef,--table-exclude-filter &lt;regex&gt;                           Filter tables (excludes) with name
                                                               matching RegEx. Comparison done with
                                                               'show tables' results.  Check case,
                                                               that's important.  Hive tables are
                                                               generally stored in LOWERCASE. Make
                                                               sure you double-quote the expression
                                                               on the commandline.
 -tf,--table-filter &lt;regex&gt;                                    Filter tables (inclusive) with name
                                                               matching RegEx. Comparison done with
                                                               'show tables' results.  Check case,
                                                               that's important.  Hive tables are
                                                               generally stored in LOWERCASE. Make
                                                               sure you double-quote the expression
                                                               on the commandline.
 -tfp,--table-filter-partition-count-limit &lt;partition-count&gt;   Filter partition tables OUT that are
                                                               have more than specified here. Non
                                                               Partitioned table aren't filtered.
 -tfs,--table-filter-size-limit &lt;size MB&gt;                      Filter tables OUT that are above the
                                                               indicated size.  Expressed in MB
 -to,--transfer-ownership                                      If available (supported) on LEFT
                                                               cluster, extract and transfer the
                                                               tables owner to the RIGHT cluster.
                                                               Note: This will make an 'exta' SQL
                                                               call on the LEFT cluster to determine
                                                               the ownership.  This won't be
                                                               supported on CDH 5 and some other
                                                               legacy Hive platforms. Beware the
                                                               cost of this extra call for EVERY
                                                               table, as it may slow down the
                                                               process for a large volume of tables.
 -v,--views-only                                               Process VIEWs ONLY
 -wd,--warehouse-directory &lt;path&gt;                              The warehouse directory path.  Should
                                                               not include the namespace OR the
                                                               database directory. This will be used
                                                               to set the MANAGEDLOCATION database
                                                               option.</code></pre></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1247348560">Output</h1><p id="1247348560#z3x6zk_3">The output from <span class="inline-code" id="1247348560#z3x6zk_12">hms-mirror</span> will, by default, be sent to <span class="inline-code" id="1247348560#z3x6zk_13">$HOME/.hms-mirror/reports</span>. Each run will be place in a sub-directory with a timestamp. You can choose to redirect the output to a different location with the <span class="inline-code" id="1247348560#z3x6zk_14">-o</span> option. In this case the directory will be created if it doesn't exist and the output will be written to that location (without the timestamp sub-directory).</p><p id="1247348560#z3x6zk_4">If you wish to have the reports written to a different location AND have the timestamp sub-directory, use a symbolic link redirect the <span class="inline-code" id="1247348560#z3x6zk_15">$HOME/.hms-mirror/reports</span> directory to the desired output directory.</p><p id="1247348560#z3x6zk_5">A report for each <span class="control" id="1247348560#z3x6zk_16">database</span> processed will be created in the output directory. Files for a database will be prefixed with the database name. This applies to each of the following report/script types.</p><section><h2 id="1247348560#application-report" data-toc="application-report#hms-mirror-output.md-application-report">Application Report</h2><p id="1247348560#z3x6zk_17">The output report is in markdown format. You can use a markdown renderer to view the report. If you don't have a renderer, you can still read the report, it will just be harder to read.</p><p id="1247348560#z3x6zk_18">The report include various stats regarding the run and details for each tables migration process. In this report, you'll find details on &quot;why&quot; a particular table was skipped, or what actions were taken to migrate the table. The report will even list issue encountered during the process.</p></section><section><h2 id="1247348560#sql-scripts" data-toc="sql-scripts#hms-mirror-output.md-sql-scripts">SQL Scripts</h2><p id="1247348560#z3x6zk_19"><span class="inline-code" id="1247348560#z3x6zk_21">hms-mirror</span> will produce the SQL scripts used to migrate the data. These scripts are written to the output directory. The scripts are prefixed with the <span class="control" id="1247348560#z3x6zk_22">database</span> name.</p><ul class="list" id="1247348560#z3x6zk_20" start="1"><li class="list-item" id="1247348560#z3x6zk_23"><p><span class="inline-code" id="1247348560#z3x6zk_27">&lt;db_name&gt;_LEFT_Clueanup_execute.sql</span> - When present, this scripts represents SQL statements that should be run on the LEFT cluster to cleanup artifacts from the migration process.</p></li><li class="list-item" id="1247348560#z3x6zk_24"><p><span class="inline-code" id="1247348560#z3x6zk_28">&lt;db_name&gt;_LEFT_execute.sql</span> - When present, this scripts represents SQL statements that should be run on the LEFT cluster to migrate the data. If the <span class="inline-code" id="1247348560#z3x6zk_29">-e</span> option was used, the contents of this script will be executed on the LEFT cluster by <span class="inline-code" id="1247348560#z3x6zk_30">hms-mirror</span>. If the <span class="inline-code" id="1247348560#z3x6zk_31">-e</span> option was NOT specified, these script can be verified and executed manually on the LEFT cluster.</p></li><li class="list-item" id="1247348560#z3x6zk_25"><p><span class="inline-code" id="1247348560#z3x6zk_32">&lt;db_name&gt;_RIGHT_execute.sql</span> - When present, this scripts represents SQL statements that should be run on the RIGHT cluster to migrate the data. If the <span class="inline-code" id="1247348560#z3x6zk_33">-e</span> option was used, the contents of this script will be executed on the RIGHT cluster by <span class="inline-code" id="1247348560#z3x6zk_34">hms-mirror</span>. If the <span class="inline-code" id="1247348560#z3x6zk_35">-e</span> option was NOT specified, these script can be verified and executed manually on the RIGHT cluster.</p></li><li class="list-item" id="1247348560#z3x6zk_26"><p><span class="inline-code" id="1247348560#z3x6zk_36">&lt;db_name&gt;_RIGHT_Clueanup_execute.sql</span> - When present, this scripts represents SQL statements that should be run on the RIGHT cluster to cleanup artifacts from the migration process.</p></li></ul></section><section><h2 id="1247348560#yaml-output" data-toc="yaml-output#hms-mirror-output.md-yaml-output">YAML Output</h2><p id="1247348560#z3x6zk_37">The <span class="inline-code" id="1247348560#z3x6zk_38">&lt;db_name&gt;_hms-mirror.yaml</span> file is a full listing of the migration process as a document. Use this file to programmatically determine what actions were taken during the migration process.</p></section><section><h2 id="1247348560#runbook" data-toc="runbook#hms-mirror-output.md-runbook">Runbook</h2><p id="1247348560#z3x6zk_39">The <span class="inline-code" id="1247348560#z3x6zk_40">&lt;db_name&gt;_runbook.md</span> is a markdown file that is a workbook of 'what' to do. It lays out the steps taken and the steps to be taken to complete the migration process.</p></section><section><h2 id="1247348560#distcp-scripts-and-workbook" data-toc="distcp-scripts-and-workbook#hms-mirror-output.md-distcp-scripts-and-workbook">distcp Scripts and Workbook</h2><p id="1247348560#z3x6zk_42">When you include the <span class="inline-code" id="1247348560#z3x6zk_49">-dc|--distcp</span> option when running <span class="inline-code" id="1247348560#z3x6zk_50">hms-mirror</span>, we'll build a template <span class="inline-code" id="1247348560#z3x6zk_51">distcp</span> job for each database that has data to be migrated. The result is a set of <span class="control" id="1247348560#z3x6zk_52">bash scripts</span> and source files listing the contents to be used in the migration.</p><p id="1247348560#z3x6zk_43">Depending other influencing options, there may be a <span class="inline-code" id="1247348560#z3x6zk_53">distcp</span> script for the LEFT and RIGHT clusters. The scripts will be prefixed with the database name.</p><p id="1247348560#z3x6zk_44">The various <span class="control" id="1247348560#z3x6zk_54">distcp</span> reports include:</p><ul class="list" id="1247348560#z3x6zk_45" start="1"><li class="list-item" id="1247348560#z3x6zk_55"><p><span class="inline-code" id="1247348560#z3x6zk_58">&lt;db_name&gt;_RIGHT_n_distcp_source.txt</span> - A list of the source directories to be copied to the RIGHT cluster. The <span class="inline-code" id="1247348560#z3x6zk_59">n</span> will increment for each one of the jobs created for the database being migrated. These files must be copied to the RIGHT clusters HDFS filesystem. When running the distcp bash shell script, set the bash environment variable <span class="inline-code" id="1247348560#z3x6zk_60">$HCFS_BASE_DIR</span> to set the 'directory' these are copied to.</p></li><li class="list-item" id="1247348560#z3x6zk_56"><p><span class="inline-code" id="1247348560#z3x6zk_61">&lt;db_name&gt;_RIGHT_distcp_script.sh</span> - The bash script created that will run the <span class="inline-code" id="1247348560#z3x6zk_62">distcp</span> jobs. This script will be run on the RIGHT cluster. Review the comments in the script for details on how to run it.</p></li><li class="list-item" id="1247348560#z3x6zk_57"><p><span class="inline-code" id="1247348560#z3x6zk_63">&lt;db_name&gt;_RIGHT_distcp_workbook.md</span> - A markdown report table that breakdown what will be moved by the process.</p></li></ul><p id="1247348560#z3x6zk_46">** Example distcp Workbook **</p><div class="table-wrapper detached"><table id="1247348560#z3x6zk_47"><tr class="header-row" id="1247348560#z3x6zk_64"><th id="1247348560#z3x6zk_67"><p>Database</p></th><th id="1247348560#z3x6zk_68"><p>Target</p></th><th id="1247348560#z3x6zk_69"><p>Sources</p></th></tr><tr class="" id="1247348560#z3x6zk_65"><td id="1247348560#z3x6zk_70"><p>tpcds_bin_partitioned_orc_10</p></td><td id="1247348560#z3x6zk_71"></td><td id="1247348560#z3x6zk_72"></td></tr><tr class="" id="1247348560#z3x6zk_66"><td id="1247348560#z3x6zk_73"></td><td id="1247348560#z3x6zk_74"><p>hdfs://HOME90/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db</p></td><td id="1247348560#z3x6zk_75"><p>hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/call_center</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/catalog_page</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/catalog_returns</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/catalog_sales</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/customer</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/customer_address</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/customer_demographics</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/date_dim</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/household_demographics</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/income_band</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/inventory</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/item</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/promotion</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/reason</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/ship_mode</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/store</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/store_returns</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/store_sales</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/time_dim</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/warehouse</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_page</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_returns</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_sales</p><p> hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_site</p></td></tr></table></div></section><section><h2 id="1247348560#logs" data-toc="logs#hms-mirror-output.md-logs">Logs</h2><p id="1247348560#z3x6zk_100">Logs, as of 1.6.5.6 are now in the same output directory as the reports.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-2108749216">REST Interface (WIP)</h1><p id="-2108749216#-f18b8m_3">Start <span class="inline-code" id="-2108749216#-f18b8m_8">hms-mirror</span> with the <span class="inline-code" id="-2108749216#-f18b8m_9">--service</span> option.</p><div class="detached code-block" id="-2108749216#-f18b8m_4"><pre><code class="language-bash">hms-mirror --service</code></pre></div><p id="-2108749216#-f18b8m_5">The REST Swagger documentation is available at <span class="inline-code" id="-2108749216#-f18b8m_10">http://server-host:8090/hms-mirror/swagger-ui/index.html</span>.</p><p id="-2108749216#-f18b8m_6">The REST base endpoint is <span class="inline-code" id="-2108749216#-f18b8m_11">http://server-host:8090/hms-mirror/api/v1</span>.</p><p id="-2108749216#-f18b8m_7">The REST service controls the 'current' session in <span class="inline-code" id="-2108749216#-f18b8m_12">hms-mirror</span>. This session is the same as the session available through the Web Interface.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1535710817">Reports</h1><p id="-1535710817#z9s4uy9_3">Both the Web (<a href="#-1652928064">Web Interface</a>) and  interfaces generate reports. By default, the reports are generated in the $HOME/.hms-mirror/reports directory with the timestamp as the 'name' of the report. The report is directory of several files that include configurations, conversions, and job 'yaml' files of what was done. Each database has their own reports. The database name prefixes the report file name for the various reports.</p><p id="-1535710817#z9s4uy9_4">From the CLI, you'll need to download the report directory to your local machine and use a 'Markdown' viewer to read it easily.</p><p id="-1535710817#z9s4uy9_5">The Web interface has built in report viewing capabilities and makes it much easier to review the reports.</p><section><h2 id="-1535710817#options" data-toc="options#Reports.md-options">Options</h2><p id="-1535710817#z9s4uy9_9">There are several views for the reports that include a detailed view of the process, workbooks for 'distcp', scripts for 'distcp' , job 'yaml' files, the configuration used to run the job, etc.</p><p id="-1535710817#z9s4uy9_10">You have options to view all these reports within the Web Interface. You can further 'download' a zip file of the report contents and view them locally.</p><p id="-1535710817#z9s4uy9_11">To keep things organized, there is an 'archive' option that will move the reports to an 'archive' directory. This is useful to help keep the reports directory clean and organized. If you need to view those reports again, you can always move them back to the reports directory. Although, that process is manual.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-1535710817#z9s4uy9_13">If you run the process via the CLI and are using defaults, you'll be able to view the reports through the Web Interface since they are both using the same location.</p></div>
</blockquote>
</section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1938912956">Quick Start Scenarios</h1><ul class="list" id="-1938912956#-hvv238_3" start="1"><li class="list-item" id="-1938912956#-hvv238_4"><p id="-1938912956#-hvv238_7">On-prem Sidecar Migrations (<a href="#-1522123162">On Prem Legacy Hive to Non-Legacy Hive</a>)</p></li><li class="list-item" id="-1938912956#-hvv238_5"><p id="-1938912956#-hvv238_9">Hybrid Migrations (<a href="#672024080">Hybrid Data LakeHouse</a>)</p></li><li class="list-item" id="-1938912956#-hvv238_6"><p id="-1938912956#-hvv238_11">Cloud to Cloud DR with Datahub (<a href="#1005763858">Cloud to Cloud DR (AWS)</a>)</p></li></ul></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-2134352060">Pre-Requisites</h1><section><h2 id="-2134352060#user-home-directory" data-toc="user-home-directory#Pre-Requisites.md-user-home-directory">User Home Directory</h2><p id="-2134352060#e8hs52_7">A lot of what happens in <span class="inline-code" id="-2134352060#e8hs52_9">hms-mirror</span> depends on a 'user' <svg id="-2134352060#e8hs52_10"  style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="65.272ex" height="2.057ex" role="img" focusable="false" viewBox="0 -704 28850.1 909"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(1651,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(2702,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(3466,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(3986,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4331,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4782,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(5248,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(5681,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(6042,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(6527,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6978,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(7468,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(7912.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(8960.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(9536.7,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(10065.7,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(10426.7,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(10892.7,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(11377.7,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(11843.7,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12294.7,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12866.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(13335.7,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(13801.7,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(14252.7,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(14703.7,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(15275.7,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(15875.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(16344.7,0)"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path></g><g data-mml-node="mi" transform="translate(16619.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(17195.7,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(18073.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(18764.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(19765.1,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(20643.1,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(20988.1,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(21439.1,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(21890.1,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(22375.1,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(22826.1,0)"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path></g><g data-mml-node="mi" transform="translate(23101.1,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(23817.1,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(24283.1,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(24999.1,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(25344.1,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(25642.1,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(25940.1,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(26512.1,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(26981.1,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(27447.1,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(27808.1,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(28384.1,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g></g></g></svg>HOME environment variable to store configurations, logs, and reports.</p><p id="-2134352060#e8hs52_8">If your environment doesn't follow a typical home environment setup, you will NEED to set the $HOME environment variable before running the application to ensure that the application can find and store the necessary files.</p></section><section><h2 id="-2134352060#hive-tez-properties-whitelist-requirements" data-toc="hive-tez-properties-whitelist-requirements#Pre-Requisites.md-hive-tez-properties-whitelist-requirements">Hive/TEZ Properties Whitelist Requirements</h2><p id="-2134352060#e8hs52_11">HiveServer2 has restrictions on what properties can be set by the user in a session. To ensure that <span class="inline-code" id="-2134352060#e8hs52_13">hms-mirror</span> will be able to set the properties it needs, add <span class="inline-code" id="-2134352060#e8hs52_17">hive.security.authorization.sqlstd.confwhitelist.append</span> (<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.security.authorization.sqlstd.confwhitelist.append">https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.security.authorization.sqlstd.confwhitelist.append</a>) property in the HiveServer2 Advanced Configuration Snippet (Safety Valve) for <span class="inline-code" id="-2134352060#e8hs52_15">hive-site.xml</span> with at least the following value(s) so <span class="inline-code" id="-2134352060#e8hs52_16">hms-mirror</span> can set the properties it needs:</p><div class="detached code-block" id="-2134352060#e8hs52_12"><pre><code class="language-none">tez\.grouping\..*</code></pre></div></section><section><h2 id="-2134352060#backups" data-toc="backups#Pre-Requisites.md-backups">Backups</h2><p id="-2134352060#e8hs52_18">DO NOT SKIP THIS!!!</p><p id="-2134352060#e8hs52_19">The  process DOES 'DROP' tables when asked. If those tables <span class="emphasis" id="-2134352060#e8hs52_23">manage</span> data like a <span class="emphasis" id="-2134352060#e8hs52_24">legacy</span> managed, <span class="emphasis" id="-2134352060#e8hs52_25">ACID</span>, or <span class="emphasis" id="-2134352060#e8hs52_26">external.table.purge=true</span> scenario, we do our best NOT to DROP those and ERROR out. But, protect yourself and make backups of the areas you'll be working in.</p><section><h3 id="-2134352060#hdfs-snapshots" data-toc="hdfs-snapshots#Pre-Requisites.md-hdfs-snapshots">HDFS Snapshots</h3><p id="-2134352060#e8hs52_28">Use HDFS Snapshots to make a quick backup of directories you'll be working on. Do this, especially in the <span class="emphasis" id="-2134352060#e8hs52_29">LEFT</span> cluster. We only drop tables, so a snapshot of the database directory is good. BUT, if you are manually doing any <span class="inline-code" id="-2134352060#e8hs52_30">DROP DATABASE &lt;x&gt; CASCADE</span> operations, that will delete the snapshotted directory (and the snapshot). In this case, create the <span class="emphasis" id="-2134352060#e8hs52_31">snapshot</span> one level above the database directory.</p></section><section><h3 id="-2134352060#metastore-backups" data-toc="metastore-backups#Pre-Requisites.md-metastore-backups">Metastore Backups</h3><p id="-2134352060#e8hs52_32">Take a DB backup of your metastore and keep it in a safe place before starting.</p></section></section><section><h2 id="-2134352060#shared-authentication" data-toc="shared-authentication#Pre-Requisites.md-shared-authentication">Shared Authentication</h2><p id="-2134352060#e8hs52_33">The clusters must share a common authentication model to support cross-cluster HDFS access when HDFS is the underlying storage layer for the datasets. This means that a <span class="control" id="-2134352060#e8hs52_36">kerberos</span> ticket used in the RIGHT cluster must be valid for the LEFT cluster.</p><p id="-2134352060#e8hs52_34">For cloud storage, the two clusters must have rights to the target storage bucket.</p><p id="-2134352060#e8hs52_35">If you can <span class="inline-code" id="-2134352060#e8hs52_39">distcp</span> between the clusters (<a href="#1958103884">Linking Cluster Storage Layers</a>), you have the basic connectivity required to start working with <span class="inline-code" id="-2134352060#e8hs52_38">hms-mirror</span>.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1124446108">Warning</h1><section><h2 id="1124446108#building-metadata" data-toc="building-metadata#warning.md-building-metadata">Building METADATA</h2><p id="1124446108#-nbk86k_6">Rebuilding METADATA can be an expensive scenario. Especially when you are trying to reconstruct the entire metastore in a short time period, consider this in your planning. Know the number of partitions and buckets you will be moving and account for this. Test on smaller datasets (volume and metadata elements). Progress to testing higher volumes/partition counts to find limits and make adjustments to your strategy.</p><p id="1124446108#-nbk86k_7">Using the SQL and EXPORT_IMPORT strategies will move metadata AND data, but rebuilding the metastore elements can be pretty expensive. So consider migrating the metadata separately from the data (distcp) and use MSCK on the RIGHT cluster to discover the data. This will be considerably more efficient.</p><p id="1124446108#-nbk86k_8">If you will be doing a lot of metadata work on the RIGHT cluster. That cluster also serves a current user base; consider setting up separate HS2 pods for the migration to minimize the impact on the current user community. Isolate Migration Activities (<a href="#-1566128265#isolate-migration-activities">&quot;Isolate Migration Activities&quot; in &quot;Optimizations&quot;</a>)</p></section><section><h2 id="1124446108#partition-handling-for-data-transfers" data-toc="partition-handling-for-data-transfers#warning.md-partition-handling-for-data-transfers">Partition Handling for Data Transfers</h2><p id="1124446108#-nbk86k_10">There are three settings in the configuration to control how and to what extent we'll attempt to migrate <span class="emphasis" id="1124446108#-nbk86k_18">DATA</span> for tables with partitions.</p><p id="1124446108#-nbk86k_11">For non-ACID/transactional tables the setting in:</p><div class="detached code-block" id="1124446108#-nbk86k_12"><pre><code class="language-yaml">hybrid:
  exportImportPartitionLimit: 100
  sqlPartitionLimit: 500</code></pre></div><p id="1124446108#-nbk86k_13">Control both the <span class="inline-code" id="1124446108#-nbk86k_19">HYBRID</span> strategy for selecting either <span class="inline-code" id="1124446108#-nbk86k_20">EXPORT_IMPORT</span> or <span class="inline-code" id="1124446108#-nbk86k_21">SQL</span> and the <span class="inline-code" id="1124446108#-nbk86k_22">SQL</span> <span class="emphasis" id="1124446108#-nbk86k_23">LIMIT</span> for how many partitions we'll attempt. When the <span class="inline-code" id="1124446108#-nbk86k_24">SQL</span> limit is exceeded, you will need to use <span class="inline-code" id="1124446108#-nbk86k_25">SCHEMA_ONLY</span> to migrate the schema followed by <span class="inline-code" id="1124446108#-nbk86k_26">distcp</span> to move the data.</p><p id="1124446108#-nbk86k_14">For ACID/transactional tables, the setting in:</p><div class="detached code-block" id="1124446108#-nbk86k_15"><pre><code class="language-yaml">migrateACID:
  partitionLimit: 500</code></pre></div><p id="1124446108#-nbk86k_16">Effectively draws the same limit as above.</p><p id="1124446108#-nbk86k_17">Why do we have these limits? Mass migration of datasets via SQL and EXPORT_IMPORT with many partitions is costly and NOT very efficient. It's best that when these limits are reached that you separate the METADATA and DATA migration to DDL and distcp.</p></section><section><h2 id="1124446108#permissions" data-toc="permissions#warning.md-permissions">Permissions</h2><p id="1124446108#-nbk86k_27">We use a cross-cluster technique to back metadata in the RIGHT cluster with datasets in the LEFT cluster for data strategies: LINKED, HYBRID, EXPORT_IMPORT, SQL, and SCHEMA_ONLY (with <span class="inline-code" id="1124446108#-nbk86k_29">-ams</span> AVRO Migrate Schema).</p><p id="1124446108#-nbk86k_28">See Linking Clusters Storage Layers (<a href="#1958103884">Linking Cluster Storage Layers</a>) for details on configuring this state.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="970891157">Permissions</h1><p id="970891157#odlzut_3">In both the METADATA and STORAGE phases of <span class="inline-code" id="970891157#odlzut_11">hms-mirror</span> the RIGHT cluster will reach down into the LEFT clusters storage layer to either <span class="emphasis" id="970891157#odlzut_12">use</span> or <span class="emphasis" id="970891157#odlzut_13">copy</span> the data.</p><p id="970891157#odlzut_4"><span class="inline-code" id="970891157#odlzut_14">hms-mirror</span> access each cluster via JDBC and use the RIGHT cluster for <span class="emphasis" id="970891157#odlzut_15">storage</span> layer access.</p><p id="970891157#odlzut_5">When the RIGHT cluster is using 'non-impersonation' (hive <span class="inline-code" id="970891157#odlzut_16">doas=false</span>), the <span class="emphasis" id="970891157#odlzut_17">hive</span> service account on the <span class="control" id="970891157#odlzut_18">RIGHT</span> cluster (usually <span class="inline-code" id="970891157#odlzut_19">hive</span>) needs access to the storage layer on the <span class="control" id="970891157#odlzut_20">LEFT</span> cluster to use this data to support sidecar testing, where we use the data of the LEFT cluster but <span class="emphasis" id="970891157#odlzut_21">mirror</span> the metadata.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="970891157#odlzut_22">Having Ranger on both clusters helps because you can create additional ACLs to provide the access required.</p></div>
</blockquote>
<p id="970891157#odlzut_7"><span class="control" id="970891157#odlzut_23">OR</span></p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="970891157#odlzut_24">Checked permissions of '&lt;submitting_user&gt;': Found that the '&lt;submitting_user&gt;' user was NOT the owner of the files in these directories. The user running the process needs to be in 'dfs.permissions.superusergroup' for the LEFT clusters 'hdfs' service. Ambari 2.6 has issues setting this property: https://jira.cloudera.com/browse/EAR-7805</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="970891157#odlzut_25">Follow workaround above or add user to the 'hdfs' group. I had to use '/var/lib/ambari-server/resources/scripts/configs.py' to set it manually for Ambari.</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="970891157#odlzut_26"><span class="inline-code" id="970891157#odlzut_27">sudo ./configs.py --host=k01.streever.local --port=8080 -u admin -p admin -n hdp50 -c hdfs-site -a set -k dfs.permissions.superusergroup -v hdfs_admin</span></p></div>
</blockquote>
</article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1958103884">Linking Cluster Storage Layers</h1><p id="1958103884#-acpkya_3">For the <span class="inline-code" id="1958103884#-acpkya_9">hms-mirror</span> process to work, it relies on the RIGHT clusters' ability to <span class="emphasis" id="1958103884#-acpkya_10">SEE</span> and <span class="emphasis" id="1958103884#-acpkya_11">ACCESS</span> data in the LEFT clusters HDFS namespace. This is the same access/configuration required to support DISTCP for an HA environment and accounts for failovers.</p><p id="1958103884#-acpkya_4">We suggest that <span class="inline-code" id="1958103884#-acpkya_12">distcp</span> operations be run from the RIGHT cluster, which usually has the greater 'hdfs' version in a migration scenario.</p><p id="1958103884#-acpkya_5">The RIGHT cluster HCFS namespace requires access to the LEFT clusters HCFS namespace. RIGHT clusters with a greater HDFS version support <span class="control" id="1958103884#-acpkya_13">LIMITED</span> functionality for data access in the LEFT cluster.</p><p id="1958103884#-acpkya_6">NOTE: This isn't designed to be a permanent solution and should only be used for testing and migration purposes.</p><section><h2 id="1958103884#goal" data-toc="goal#Linking-Cluster-Storage-Layers.md-goal">Goal</h2><p id="1958103884#-acpkya_14">What does it take to support HDFS visibility between these two clusters?</p><p id="1958103884#-acpkya_15">Can that integration be used to support the Higher Clusters' use of the Lower Clusters HDFS Layer for distcp AND Hive External Table support?</p></section><section><h2 id="1958103884#scenario-1" data-toc="scenario-1#Linking-Cluster-Storage-Layers.md-scenario-1">Scenario #1</h2><section><h3 id="1958103884#hdp-2-6-5-hadoop-2-7-x" data-toc="hdp-2-6-5-hadoop-2-7-x#Linking-Cluster-Storage-Layers.md-hdp-2-6-5-hadoop-2-7-x">HDP 2.6.5 (Hadoop 2.7.x)</h3><p id="1958103884#-acpkya_20">Kerberized - sharing same KDC as CDP Base Cluster</p><p id="1958103884#-acpkya_21"><span class="control" id="1958103884#-acpkya_31">Configuration Changes</span></p><p id="1958103884#-acpkya_22">The <span class="emphasis" id="1958103884#-acpkya_32">namenode</span> <span class="emphasis" id="1958103884#-acpkya_33">kerberos</span> principal MUST be changed from <span class="inline-code" id="1958103884#-acpkya_34">nn</span> to <span class="inline-code" id="1958103884#-acpkya_35">hdfs</span> to match the namenode principal of the CDP cluster.</p><p id="1958103884#-acpkya_23">Note: You may need to add/adjust the <span class="inline-code" id="1958103884#-acpkya_36">auth_to_local</span> settings to match this change.</p><p id="1958103884#-acpkya_24">If this isn't done, <span class="inline-code" id="1958103884#-acpkya_37">spark-shell</span> and <span class="inline-code" id="1958103884#-acpkya_38">spark-submit</span> will fail to initialize. When changing this in Ambari on HDP, you will need to <span class="emphasis" id="1958103884#-acpkya_39">reset</span> the HDFS zkfc <span class="inline-code" id="1958103884#-acpkya_40">ha</span> zNode in Zookeeper and reinitialize the hdfs <span class="inline-code" id="1958103884#-acpkya_41">zkfc</span>.</p><p id="1958103884#-acpkya_25">From a Zookeeper Client: <span class="inline-code" id="1958103884#-acpkya_42">/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server localhost</span></p><div class="detached code-block" id="1958103884#-acpkya_26"><pre><code class="language-none">rmr /hadoop-ha</code></pre></div><p id="1958103884#-acpkya_27">Initialize zkfc</p><div class="detached code-block" id="1958103884#-acpkya_28"><pre><code class="language-none">hdfs zkfc -formatZK</code></pre></div><p id="1958103884#-acpkya_29"><span class="emphasis" id="1958103884#-acpkya_43">core-site.xml</span></p><div class="detached code-block" id="1958103884#-acpkya_30"><pre><code class="language-none">hadoop.rpc.protection=true
dfs.encrypt.data.transfer=true
dfs.encrypt.data.transfer.algorithm=3des
dfs.encrypt.data.transfer.cipher.key.bitlength=256</code></pre></div></section><section><h3 id="1958103884#cdp-7-1-4-hadoop-3-1-x" data-toc="cdp-7-1-4-hadoop-3-1-x#Linking-Cluster-Storage-Layers.md-cdp-7-1-4-hadoop-3-1-x">CDP 7.1.4 (Hadoop 3.1.x)</h3><p id="1958103884#-acpkya_44">Kerberized, TLS Enabled</p><p id="1958103884#-acpkya_45"><span class="control" id="1958103884#-acpkya_51">Configuration Changes</span></p><p id="1958103884#-acpkya_46">Requirements that allow this (upper) cluster to negotiate and communicate with the lower environment.</p><p id="1958103884#-acpkya_47"><span class="emphasis" id="1958103884#-acpkya_52">Cluster Wide hdfs-site.xml Safety Value</span></p><div class="detached code-block" id="1958103884#-acpkya_48"><pre><code class="language-none">ipc.client.fallback-to-simple-auth-allowed=true</code></pre></div><p id="1958103884#-acpkya_49"><span class="emphasis" id="1958103884#-acpkya_53">HDFS Service Advanced Config hdfs-site.xml</span></p><div class="detached code-block" id="1958103884#-acpkya_50"><pre><code class="language-none"># For this Clusters Name Service
dfs.internal.nameservices=HOME90

# For the target (lower) environment HA NN Services
dfs.ha.namenodes.HDP50=nn1,nn2
dfs.namenode.rpc-address.HDP50.nn1=k01.streever.local:8020
dfs.namenode.rpc-address.HDP50.nn2=k02.streever.local:8020
dfs.namenode.http-address.HDP50.nn1=k01.streever.local:50070
dfs.namenode.http-address.HDP50.nn2=k02.streever.local:50070
dfs.namenode.https
 address.HDP50.nn1=k01.streever.local:50471
dfs.namenode.https-address.HDP50.nn2=k02.streever.local:50470
dfs.client.failover.proxy.provider.HDP50=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider

# For Available Name Services
dfs.nameservices=HOME90,HDP50</code></pre></div></section><section><h3 id="1958103884#running-distcp-from-the-right-cluster" data-toc="running-distcp-from-the-right-cluster#Linking-Cluster-Storage-Layers.md-running-distcp-from-the-right-cluster">Running distcp from the RIGHT Cluster</h3><p id="1958103884#-acpkya_55">NOTE: Running <span class="inline-code" id="1958103884#-acpkya_62">distcp</span> from the <span class="control" id="1958103884#-acpkya_63">LEFT</span> cluster isn't supported since the <span class="inline-code" id="1958103884#-acpkya_64">hcfs client</span> is not forward compatible.</p><p id="1958103884#-acpkya_56">Copy 'from' Lower Cluster</p><div class="detached code-block" id="1958103884#-acpkya_57"><pre><code class="language-none">hadoop distcp hdfs://HDP50/user/dstreev/sstats/queues/2020-10.txt /user/dstreev/temp</code></pre></div><p id="1958103884#-acpkya_58">Copy 'to' Lower Cluster</p><div class="detached code-block" id="1958103884#-acpkya_59"><pre><code class="language-none">hadoop distcp /warehouse/tablespace/external/hive/cvs_hathi_workload.db/queue/2020-10.txt hdfs://HDP50/user/dstreev/temp</code></pre></div></section><section><h3 id="1958103884#sourcing-data-from-lower-cluster-to-support-upper-cluster-external-tables" data-toc="sourcing-data-from-lower-cluster-to-support-upper-cluster-external-tables#Linking-Cluster-Storage-Layers.md-sourcing-data-from-lower-cluster-to-support-upper-cluster-external-tables">Sourcing Data from Lower Cluster to Support Upper Cluster External Tables</h3><section><h4 id="1958103884#proxy-permissions" data-toc="proxy-permissions#Linking-Cluster-Storage-Layers.md-proxy-permissions">Proxy Permissions</h4><p id="1958103884#-acpkya_66">The lower cluster must allow the upper clusters <span class="emphasis" id="1958103884#-acpkya_73">Hive Server 2</span> host as a 'hive' proxy. The setting in the lower clusters <span class="emphasis" id="1958103884#-acpkya_74">custom</span> <span class="inline-code" id="1958103884#-acpkya_75">core-site.xml</span> may limit this to that clusters (lower) HS2 hosts. Open it up to include the upper clusters HS2 host.</p><p id="1958103884#-acpkya_67"><span class="emphasis" id="1958103884#-acpkya_76">Custom core-site.xml in Lower Cluster</span></p><div class="detached code-block" id="1958103884#-acpkya_68"><pre><code class="language-none">hadoop.proxyuser.hive.hosts=*</code></pre></div><p id="1958103884#-acpkya_69">Credentials from the 'upper' cluster will be projected down to the 'lower' cluster. The <span class="inline-code" id="1958103884#-acpkya_77">hive</span> user in the upper cluster, when running with 'non-impersonation' will require access to the datasets in the lower cluster HDFS.</p><p id="1958103884#-acpkya_70">For table creation in the 'upper' clusters Metastore, a permissions check will be done on the lower environments directory for the submitting user. So, both the service user AND <span class="inline-code" id="1958103884#-acpkya_78">hive</span> will require access to the directory location specified in the lower cluster.</p><p id="1958103884#-acpkya_71">When the two clusters <span class="emphasis" id="1958103884#-acpkya_79">share</span> accounts, and the same accounts are used between environments for users and service accounts, then access should be simple.</p><p id="1958103884#-acpkya_72">When a different set of accounts are used, the 'principal' from the upper clusters service account for 'hive' and the 'user' principal will be used in the lower cluster. This means additional HDFS policies in the lower cluster may be required to support this cross-environment work.</p></section></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1489882229">Configuration</h1><section><h2 id="1489882229#left-and-right-clusters" data-toc="left-and-right-clusters#hms-mirror-cfg.md-left-and-right-clusters">LEFT and RIGHT Clusters</h2><p id="1489882229#-cji8ez_4"><span class="inline-code" id="1489882229#-cji8ez_10">hms-mirror</span> defines clusters as LEFT and RIGHT. The LEFT cluster is the source of the metadata and the RIGHT cluster is the target. The LEFT cluster is usually the older cluster version. Regardless, under specific scenario's, <span class="inline-code" id="1489882229#-cji8ez_11">hms-mirror</span> will use an HDFS client to check directories and move small amounts of data (AVRO schema files). <span class="inline-code" id="1489882229#-cji8ez_12">hms-mirror</span> will depend on the configuration of the node it's running on to locate the 'hcfs filesystem'. This means that the <span class="inline-code" id="1489882229#-cji8ez_13">/etc/hadoop/conf</span> directory should contain all the environments settings to successfully connect to <span class="inline-code" id="1489882229#-cji8ez_14">hcfs (Hadoop Compatible File System</span>.</p><p id="1489882229#-cji8ez_5">The configuration is done via a 'yaml' file, details below.</p><p id="1489882229#-cji8ez_6">There are two ways to get started:</p><ul class="list" id="1489882229#-cji8ez_7" start="1"><li class="list-item" id="1489882229#-cji8ez_15"><p>The first time you run <span class="inline-code" id="1489882229#-cji8ez_17">hms-mirror</span> and it can't find a configuration, it will walk you through building one and save it to <span class="inline-code" id="1489882229#-cji8ez_18">$HOME/.hms-mirror/cfg/default.yaml</span>. Here's what you'll need to complete the setup: </p><ul class="list" id="1489882229#-cji8ez_19" start="1"><li class="list-item" id="1489882229#-cji8ez_20"><p>URI's for each clusters HiveServer2</p></li><li class="list-item" id="1489882229#-cji8ez_21"><p>STANDALONE jar files for EACH Hive version. </p><ul class="list" id="1489882229#-cji8ez_24" start="1"><li class="list-item" id="1489882229#-cji8ez_25"><p>We support the Apache Hive based drivers for Hive 1 and Hive2/3.</p></li><li class="list-item" id="1489882229#-cji8ez_26"><p>Recently added support for the Cloudera JDBC driver for CDP.</p></li></ul></li><li class="list-item" id="1489882229#-cji8ez_22"><p>Username and Password for non-kerberized connections. </p><ul class="list" id="1489882229#-cji8ez_27" start="1"><li class="list-item" id="1489882229#-cji8ez_28"><p>Note: <span class="inline-code" id="1489882229#-cji8ez_29">hms-mirror</span> will only support one kerberos connection. For the other, use another AUTH method.</p></li></ul></li><li class="list-item" id="1489882229#-cji8ez_23"><p>The hcfs (Hadoop Compatible FileSystem) protocol and prefix used for the hive table locations in EACH cluster.</p></li></ul></li><li class="list-item" id="1489882229#-cji8ez_16"><p>Use the template yaml (<a href="#28311084">Default Configuration Template</a>) for reference and create a <span class="inline-code" id="1489882229#-cji8ez_31">default.yaml</span> in the running users <span class="inline-code" id="1489882229#-cji8ez_32">$HOME/.hms-mirror/cfg</span> directory.</p></li></ul><p id="1489882229#-cji8ez_8">You'll need JDBC driver jar files that are * <span class="emphasis" id="1489882229#-cji8ez_33">specific</span> to the clusters you'll integrate. If the <span class="control" id="1489882229#-cji8ez_34">LEFT</span> cluster isn't the same version as the <span class="control" id="1489882229#-cji8ez_35">RIGHT</span> cluster, don't use the same JDBC jar file, especially when integrating Hive 1 and Hive 3 services. The Hive 3 driver is NOT backwardly compatible with Hive 1.</p><p id="1489882229#-cji8ez_9">See the running (<a href="#-1624968880">Running</a>) section for examples on running <span class="inline-code" id="1489882229#-cji8ez_37">hms-mirror</span> for various environment types and connections.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1461399276">Hive JDBC Drivers and Configuration</h1><p id="1461399276#-tm3wmq_3"><span class="inline-code" id="1461399276#-tm3wmq_24">hms-mirror</span> requires JDBC drivers to connect to the various end-points needed to perform tasks. The <span class="inline-code" id="1461399276#-tm3wmq_25">LEFT</span> and <span class="inline-code" id="1461399276#-tm3wmq_26">RIGHT</span> cluster endpoints for HiveServer2 require the standalone JDBC drivers that are specific to that Hive version.</p><p id="1461399276#-tm3wmq_4"><span class="inline-code" id="1461399276#-tm3wmq_27">hms-mirror</span> supports the Apache Hive packaged <span class="control" id="1461399276#-tm3wmq_28">standalone</span> drivers that are found with your distribution. You can find a copy of this driver in:</p><div class="table-wrapper detached"><table id="1461399276#-tm3wmq_5"><tr class="header-row" id="1461399276#-tm3wmq_29"><th id="1461399276#-tm3wmq_33"><p>Platform</p></th><th id="1461399276#-tm3wmq_34"><p>Driver Location/Pattern</p></th></tr><tr class="" id="1461399276#-tm3wmq_30"><td id="1461399276#-tm3wmq_35"><p>HDP</p></td><td id="1461399276#-tm3wmq_36"><p><span class="inline-code" id="1461399276#-tm3wmq_37">/usr/hdp/current/hive-client/jdbc/hive-jdbc-&lt;hive-platform-version&gt;-standalone.jar</span></p></td></tr><tr class="" id="1461399276#-tm3wmq_31"><td id="1461399276#-tm3wmq_38"><p>CDP</p></td><td id="1461399276#-tm3wmq_39"><p><span class="inline-code" id="1461399276#-tm3wmq_40">/opt/cloudera/parcels/CDH/jars/hive-jdbc-&lt;hive-platform-version&gt;-standalone.jar</span></p></td></tr><tr class="" id="1461399276#-tm3wmq_32"><td id="1461399276#-tm3wmq_41"></td><td id="1461399276#-tm3wmq_42"></td></tr></table></div><p id="1461399276#-tm3wmq_6">For CDP, we also support to Cloudera JDBC driver found and maintained at on the Cloudera Hive JDBC Downloads Page (<a href="https://www.cloudera.com/downloads/connectors/hive/jdbc">https://www.cloudera.com/downloads/connectors/hive/jdbc</a>). Note that the URL configurations between the Apache and Cloudera JDBC drivers are different.</p><p id="1461399276#-tm3wmq_7">Hive JDBC Drivers need to be inline with the version of HS2 you're connecting to. If the cluster is an HDP cluster, get the appropriate <span class="control" id="1461399276#-tm3wmq_44">standalone</span> driver from that cluster. These drivers(jar files) should be stored locally on the machine running <span class="inline-code" id="1461399276#-tm3wmq_45">hms-mirror</span> and referenced in the configuration file.</p><blockquote class="prompt flex bordered-element-rounded warning detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Do NOT put these drivers in ${HOME}/.hms-mirror/aux_libs or any sub-directory of that location. `hms-mirror` connects to different versions of Hive and the drivers need to be specific to the version of Hive you're connecting to. To do this, we need to manage the classpath and the drivers in a more controlled manner. They should NOT be in the applications main classpath which includes jar files in `$HOME/.hms-mirror/aux_libs`, this will cause connectivity issues.</p></div>
</blockquote>
<div id="1461399276#-tm3wmq_9"><div class="detached" id="1461399276#-tm3wmq_46"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><p><b id="1461399276#-tm3wmq_48">Hive Server 2 Configuration</b></p><div class="container"><figure class="image-container"><img class="center image image-size" id="1461399276#-tm3wmq_49" alt="hs2_cfg.png" title="hs2_cfg.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/hs2_cfg.png" width="851" height="396"><figcaption class="center-text">hs2_cfg.png</figcaption></figure></div></div></div><div class="detached" id="1461399276#cli-hs2"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><div class="detached code-block" id="1461399276#-tm3wmq_50"><pre><code class="language-yaml">hiveServer2:
  uri: &quot;&lt;cloudera_jdbc_url&gt;&quot;
  driverClassName: &quot;com.cloudera.hive.jdbc.HS2Driver&quot;
  connectionProperties:
    user: &quot;xxx&quot;
    password: &quot;xxx&quot;</code></pre></div></div></div></div><p id="1461399276#-tm3wmq_10">Starting with the Apache Standalone driver shipped with <span class="control" id="1461399276#-tm3wmq_51">CDP 7.1.8 cummulative</span> hot fix parcels, you will need to include additional jars in the configuration <span class="inline-code" id="1461399276#-tm3wmq_52">jarFile</span> configuration, due to some packaging adjustments.</p><p id="1461399276#-tm3wmq_11">For example: <span class="inline-code" id="1461399276#-tm3wmq_53">jarFile: &quot;&lt;cdp_parcel_jars&gt;/hive-jdbc-3.1.3000.7.1.8.28-1-standalone.jar:&lt;cdp_parcel_jars&gt;/log4j-1.2-api-2.18.0.jar:&lt;cdp_parcel_jars&gt;/log4j-api-2.18.0.jar:&lt;cdp_parcel_jars&gt;/log4j-core-2.18.0.jar&quot;</span> NOTE: The jar file with the Hive Driver MUST be the first in the list of jar files.</p><p id="1461399276#-tm3wmq_12">The Cloudera JDBC driver shouldn't require additional jars.</p><section><h2 id="1461399276#kerberized-hs2-connections" data-toc="kerberized-hs2-connections#JDBC-Drivers-and-Configuration.md-kerberized-hs2-connections">Kerberized HS2 Connections</h2><p id="1461399276#-tm3wmq_16">We currently have validated <span class="control" id="1461399276#-tm3wmq_23">kerberos</span> HS2 connections to CDP clusters using the Hive JDBC driver you'll find in your target CDP distribution.</p><blockquote class="prompt flex bordered-element-rounded warning detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Connections to Kerberized HS2 endpoints on NON-CDP clusters is NOT currently supported. You will need to use KNOX in HDP to connect to a kerberized HS2 endpoint. For CDH, you can setup a non-kerberized HS2 endpoint to support the migration.</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded note detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M21 12a9 9 0 1 1-9-9 9 9 0 0 1 9 9zM10.5 7.5A1.5 1.5 0 1 0 12 6a1.5 1.5 0 0 0-1.5 1.5zm-.5 3.54v1h1V18h2v-6a.96.96 0 0 0-.96-.96z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>This process has CHANGED compared to v1.x of `hms-mirror`. Please adjust your configurations accordingly.</p></div>
</blockquote>
<p id="1461399276#-tm3wmq_19">We NO LONGER need to have the hive JDBC driver in the <span class="inline-code" id="1461399276#-tm3wmq_54">aux_libs</span> directory ($HOME/.hms-mirror/aux_libs). The driver should be stored locally on the machine running <span class="inline-code" id="1461399276#-tm3wmq_55">hms-mirror</span> and referenced in the configuration file via the `jarFile' attribute. Follow the same procedure as above for <span class="control" id="1461399276#-tm3wmq_56">Kerberized</span> connections as is done for non-kerberized connections.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="1461399276#-tm3wmq_20" alt="hs2_kerb_config.png" title="hs2_kerb_config.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/hs2_kerb_config.png" width="1041" height="507"><figcaption class="center-text">hs2_kerb_config.png</figcaption></figure></div><p id="1461399276#-tm3wmq_21">At this point, just like the previous version of <span class="inline-code" id="1461399276#-tm3wmq_57">hms-mirror</span>, you'll need to have a valid kerberos ticket on the machine running <span class="inline-code" id="1461399276#-tm3wmq_58">hms-mirror</span>. This is required to authenticate to the kerberized HS2 endpoint.</p><p id="1461399276#-tm3wmq_22">REMOVE all 'hive' related JDBC jar files from <span class="inline-code" id="1461399276#-tm3wmq_59">aux_libs</span>. Leaving them there WILL cause conflicts during the service startup.</p></section><section><h2 id="1461399276#validating-hs2-connectivity" data-toc="validating-hs2-connectivity#JDBC-Drivers-and-Configuration.md-validating-hs2-connectivity">Validating HS2 Connectivity</h2><p id="1461399276#-tm3wmq_60">Once you have everything configured, you can validate all connections required by <span class="inline-code" id="1461399276#-tm3wmq_61">hms-mirror</span> through the 'CONNECTIONS --&gt; Validate' left menu option in the UI. This will test the connectivity to the various endpoints required by <span class="inline-code" id="1461399276#-tm3wmq_62">hms-mirror</span>.</p></section><section><h2 id="1461399276#hdp-3-connections" data-toc="hdp-3-connections#JDBC-Drivers-and-Configuration.md-hdp-3-connections">HDP 3 Connections</h2><p id="1461399276#-tm3wmq_63">The JDBC driver for HDP Hive 3 has some embedded classes for <span class="inline-code" id="1461399276#-tm3wmq_64">log4j</span> that conflict with the <span class="inline-code" id="1461399276#-tm3wmq_65">log4j</span> classes in the <span class="inline-code" id="1461399276#-tm3wmq_66">hms-mirror</span> application. To resolve this, you can use the Cloudera Apache JDBC driver for HDP 3 Hive. This driver is compatible with HDP 3 and does not have the <span class="inline-code" id="1461399276#-tm3wmq_67">log4j</span> conflict.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="870733106">Example URL's</h1><section><h2 id="870733106#cdp-hive-via-knox-gateway" data-toc="cdp-hive-via-knox-gateway#Example-URL-s.md-cdp-hive-via-knox-gateway">CDP Hive via Knox Gateway</h2><p id="870733106#phge7y_8">Doesn't require Kerberos. Knox is SSL, so depending on whether you've self-signed your certs you may need to make adjustments.</p><ul class="list" id="870733106#phge7y_9" start="1"><li class="list-item" id="870733106#phge7y_14"><p>Apache Hive and CDP Packaged Apache Hive JDBC Driver</p></li></ul><div class="detached code-block" id="870733106#phge7y_10"><pre><code class="language-none">jdbc:hive2://s03.streever.local:8443/;ssl=true;transportMode=http;httpPath=gateway/cdp-proxy-api/hive;sslTrustStore=/Users/dstreev/bin/certs/gateway-client-trust.jks;trustStorePassword=changeit</code></pre></div><ul class="list" id="870733106#phge7y_11" start="1"><li class="list-item" id="870733106#phge7y_15"><p>Cloudera JDBC Driver</p></li></ul><div class="detached code-block" id="870733106#phge7y_12"><pre><code class="language-none">jdbc:hive2://s03.streever.local:8443;transportMode=http;AuthMech=3;httpPath=gateway/cdp-proxy-api/hive;SSL=1;AllowSelfSignedCerts=1</code></pre></div></section><section><h2 id="870733106#cdp-hive-direct-with-kerberos" data-toc="cdp-hive-direct-with-kerberos#Example-URL-s.md-cdp-hive-direct-with-kerberos">CDP Hive direct with Kerberos</h2><p id="870733106#phge7y_17">When connecting to via Kerberos, configure the jar files the same way as non-kerberized connections.</p><ul class="list" id="870733106#phge7y_18" start="1"><li class="list-item" id="870733106#phge7y_24"><p>Apache Hive and CDP Packaged Apache Hive JDBC Driver</p></li></ul><div class="detached code-block" id="870733106#phge7y_19"><pre><code class="language-none">jdbc:hive2://s04.streever.local:10001/;ssl=true;transportMode=http;httpPath=cliservice;sslTrustStore=/home/dstreev/bin/certs/gateway-client-trust.jks;trustStorePassword=changeit;principal=hive/_HOST@STREEVER.LOCAL</code></pre></div><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="870733106#phge7y_25">NOTE: This configuration includes a certificate reference for SSL. If you're using self-signed certs, you'll need to adjust the <span class="inline-code" id="870733106#phge7y_26">sslTrustStore</span> and <span class="inline-code" id="870733106#phge7y_27">trustStorePassword</span> values.</p></div>
</blockquote>
<ul class="list" id="870733106#phge7y_21" start="1"><li class="list-item" id="870733106#phge7y_28"><p>Cloudera JDBC Driver</p></li></ul><div class="detached code-block" id="870733106#phge7y_22"><pre><code class="language-none">jdbc:hive2://s04.streever.local:10001;transportMode=http;AuthMech=1;KrbRealm=STREEVER.LOCAL;KrbHostFQDN=s04.streever.local;KrbServiceName=hive;KrbAuthType=2;httpPath=cliservice;SSL=1;AllowSelfSignedCerts=1</code></pre></div></section><section><h2 id="870733106#hdp2-hs2-with-no-auth" data-toc="hdp2-hs2-with-no-auth#Example-URL-s.md-hdp2-hs2-with-no-auth">HDP2 HS2 with No Auth</h2><p id="870733106#phge7y_30">Since CDP is usually kerberized AND <span class="inline-code" id="870733106#phge7y_34">hms-mirror</span> doesn't support the simultanous connections to 2 different kerberos environments, I've setup an HS2 on HDP2 specifically for this effort. NOTE: You need to specify a <span class="inline-code" id="870733106#phge7y_35">username</span> when connecting to let Hive know what the user is. No password required.</p><ul class="list" id="870733106#phge7y_31" start="1"><li class="list-item" id="870733106#phge7y_36"><p>Apache Hive Standalone Driver shipped with HDP2.</p></li></ul><div class="detached code-block" id="870733106#phge7y_32"><pre><code class="language-none">jdbc:hive2://k02.streever.local:10000</code></pre></div></section><section><h2 id="870733106#direct-metastore-db-access" data-toc="direct-metastore-db-access#Example-URL-s.md-direct-metastore-db-access">Direct Metastore DB Access</h2><p id="870733106#phge7y_37">The <span class="inline-code" id="870733106#phge7y_38">LEFT</span> and <span class="inline-code" id="870733106#phge7y_39">RIGHT</span> configurations also suppport 'direct' metastore access to collect detailed partition information. The support this feature, get the JDBC driver that is appropriate for your metastore(s) backend dbs and place it in <span class="inline-code" id="870733106#phge7y_40">$HOME/.hms-mirror/aux_libs</span> directory.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="703441149">Metastore JDBC Drivers and Configuration</h1><p id="703441149#-uooelf_3">For some data points, we revert to direct access to the metastore RDBMS. To support this, you need to place the appropriate JDBC drivers for the metastore RDBMS in <span class="inline-code" id="703441149#-uooelf_7">$HOME/.hms-mirror/aux_libs</span> directory.</p><p id="703441149#-uooelf_4">You'll have to get the drivers from your RDBMS vendor.</p><div id="703441149#-uooelf_5"><div class="detached" id="703441149#web-msd"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><div class="container"><figure class="image-container"><img class="center image image-size" id="703441149#-uooelf_10" alt="metastore_direct_cfg.png" title="metastore_direct_cfg.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/metastore_direct_cfg.png" width="851" height="396"><figcaption class="center-text">metastore_direct_cfg.png</figcaption></figure></div></div></div><div class="detached" id="703441149#cli-msd"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"></div></div></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1700610744">Storage Systems (Distributed File Systems)</h1><p id="1700610744#dq90g2_3">By default, <span class="inline-code" id="1700610744#dq90g2_11">hms-mirror</span> is built with native support to communicate with 'hdfs', and 'ozone' distributed file systems. The <span class="inline-code" id="1700610744#dq90g2_12">hms-mirror</span> can be extended to support other distributed file systems by implementing an <span class="inline-code" id="1700610744#dq90g2_13">hcfs</span> (hadoop compatible filesystem) interface. These include Amazon S3, Google Cloud Storage, Azure Blob Storage, etc.</p><p id="1700610744#dq90g2_4">To support communication with any of these cloud platforms or other distributed files systems, you'll need to ensure the libraries needed to support that communication are available in the classpath for 'hms-mirror'.</p><p id="1700610744#dq90g2_5">Make sure these are available in the classpath for <span class="inline-code" id="1700610744#dq90g2_14">hms-mirror</span> <span class="control" id="1700610744#dq90g2_15">BEFORE</span> you start the application (Web and CLI).</p><p id="1700610744#dq90g2_6">We'll cover the libraries in the following sections. These libraries need to be copied to the <span class="inline-code" id="1700610744#dq90g2_16">$HOME/. hms-mirror/aux_libs</span> directory.</p><p id="1700610744#dq90g2_7">All these libraries are available in the Cloudera distribution. The below file listings have been sourced through our community testing and may not be exhaustive. If you find we've missed any, please let us know by logging an issue at hms-mirror Github Issues (<a href="https://github.com/cloudera-labs/hms-mirror/issues">https://github.com/cloudera-labs/hms-mirror/issues</a>)</p><section><h2 id="1700610744#amazon-s3" data-toc="amazon-s3#Storage-Systems-Distributed-File-Systems.md-amazon-s3">Amazon S3</h2><p id="1700610744#dq90g2_18">This obviously includes Amazon S3, but also includes other S3 compatible storage systems like Minio, etc. that are compatible with the S3 API.</p><div class="detached code-block" id="1700610744#dq90g2_19"><pre><code class="language-none">hadoop-aws-&lt;platform-version&gt;.jar
aws-java-sdk-bundle-&lt;platform-version&gt;.jar
ranger-raz-hook-s3-&lt;platform-version&gt;.jar</code></pre></div></section><section><h2 id="1700610744#microsoft-azure" data-toc="microsoft-azure#Storage-Systems-Distributed-File-Systems.md-microsoft-azure">Microsoft Azure</h2><p id="1700610744#dq90g2_20"><span class="control" id="1700610744#dq90g2_22">abfs</span></p><div class="detached code-block" id="1700610744#dq90g2_21"><pre><code class="language-none">hadoop-azure-&lt;platform-version&gt;9.jar
ranger-raz-hook-abfs-&lt;platform-version&gt;.jar</code></pre></div></section><section><h2 id="1700610744#google-cloud-storage-gfs" data-toc="google-cloud-storage-gfs#Storage-Systems-Distributed-File-Systems.md-google-cloud-storage-gfs">Google Cloud Storage (GFS)</h2><div class="detached code-block" id="1700610744#dq90g2_23"><pre><code class="language-none">google-cloud-storage-&lt;platform-version&gt;.jar</code></pre></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1116233166">Password Security</h1><section><h2 id="-1116233166#secure-passwords-in-configuration" data-toc="secure-passwords-in-configuration#Password-Security.md-secure-passwords-in-configuration">Secure Passwords in Configuration</h2><p id="-1116233166#-t5i35u_6">There are multiple passwords stored in the configuration files. By default, the passwords are in clear text in the configuration file. This usually isn't an issue since the file can be protected at the UNIX level from peering eyes. But if you need to protect those passwords, <span class="inline-code" id="-1116233166#-t5i35u_8">hms-mirror</span> supports storing an encrypted version of the password in the configuration.</p><p id="-1116233166#-t5i35u_7">When you're using this feature, you need to have a <span class="inline-code" id="-1116233166#-t5i35u_9">password-key</span>. This is a key used to encrypt and decrypt the password in the configuration. The <span class="control" id="-1116233166#-t5i35u_10">same</span> <span class="inline-code" id="-1116233166#-t5i35u_11">password-key</span> must be used for <span class="control" id="-1116233166#-t5i35u_12">ALL</span> passwords in the configuration file.</p></section><section><h2 id="-1116233166#web-ui" data-toc="web-ui#Password-Security.md-web-ui">WEB UI</h2><p id="-1116233166#-t5i35u_13">Passwords are saved in the configuration and can easily be encrypted and decrypted using the Web UI. If the password (s) are encrypted, the 'Passwords Encrypted' checkbox will be checked.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1116233166#-t5i35u_14" alt="pwd_mngd.png" title="pwd_mngd.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/pwd_mngd.png" width="930" height="246"><figcaption class="center-text">pwd_mngd.png</figcaption></figure></div><p id="-1116233166#-t5i35u_15">If the passwords are encrypted, you'll need to specify the 'Encrypt Key' before running or connection to any endpoints. Set the 'Encrypt Key' and click 'Save' to save the key for the session.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-1116233166#-t5i35u_18">The 'Encrypt Key' is only used for the current session and will NOT be saved if you 'persist' the session.</p></div>
</blockquote>
<p id="-1116233166#-t5i35u_17">Once encrypted, you'll need to specify the 'password key' with that session to decrypt them for use.</p></section><section><h2 id="-1116233166#cli" data-toc="cli#Password-Security.md-cli">CLI</h2><section><h3 id="-1116233166#generate-the-encrypted-password" data-toc="generate-the-encrypted-password#Password-Security.md-generate-the-encrypted-password">Generate the Encrypted Password</h3><p id="-1116233166#-t5i35u_22">Use the <span class="inline-code" id="-1116233166#-t5i35u_28">-pkey</span> and <span class="inline-code" id="-1116233166#-t5i35u_29">-p</span> options of <span class="inline-code" id="-1116233166#-t5i35u_30">hms-mirror</span> to generate and decrypt the password(s).</p><p id="-1116233166#-t5i35u_23"><span class="inline-code" id="-1116233166#-t5i35u_31">hms-mirror -pkey cloudera -p have-a-nice-day</span></p><p id="-1116233166#-t5i35u_24">Will generate:</p><div class="detached code-block" id="-1116233166#-t5i35u_25"><pre><code class="language-none">=== Errors ===
	38:Password en/de crypt

=== Warnings ===
	56:Encrypted password: HD1eNF8NMFahA2smLM9c4g==</code></pre></div><p id="-1116233166#-t5i35u_26">Ignore the error 38, it's just a warning that the password is being encrypted. The encrypted password is the value after the <span class="inline-code" id="-1116233166#-t5i35u_32">Encrypted password:</span> string.</p><p id="-1116233166#-t5i35u_27">Copy this encrypted password and place it in your configuration file for the JDBC connection. Repeat for the other passwords, if it's different, and paste it in the configuration as well.</p></section><section><h3 id="-1116233166#running-hms-mirror-with-encrypted-passwords" data-toc="running-hms-mirror-with-encrypted-passwords#Password-Security.md-running-hms-mirror-with-encrypted-passwords">Running hms-mirror with Encrypted Passwords</h3><p id="-1116233166#-t5i35u_34">Using the <span class="control" id="-1116233166#-t5i35u_38">same</span> <span class="inline-code" id="-1116233166#-t5i35u_39">-pkey</span> you used to generate the encrypted password, we'll run <span class="inline-code" id="-1116233166#-t5i35u_40">hms-mirror</span></p><p id="-1116233166#-t5i35u_35"><span class="inline-code" id="-1116233166#-t5i35u_41">hms-mirror -db &lt;db&gt; -pkey cloudera ...</span></p><p id="-1116233166#-t5i35u_36">When the <span class="inline-code" id="-1116233166#-t5i35u_42">-pkey</span> option is specified <span class="control" id="-1116233166#-t5i35u_43">WITHOUT</span> the <span class="inline-code" id="-1116233166#-t5i35u_44">-p</span> option (used previously), <span class="inline-code" id="-1116233166#-t5i35u_45">hms-mirror</span> will understand to * *decrypt ** the configuration passwords before connecting to jdbc. If you receive jdbc connection exceptions, recheck the <span class="inline-code" id="-1116233166#-t5i35u_46">-pkey</span> and encrypted password from before.</p></section><section><h3 id="-1116233166#testing-the-encrypted-password" data-toc="testing-the-encrypted-password#Password-Security.md-testing-the-encrypted-password">Testing the Encrypted Password</h3><p id="-1116233166#-t5i35u_47">If you're unsure if the password is being decrypted correctly, you can use the <span class="inline-code" id="-1116233166#-t5i35u_53">-dp</span> option to decrypt the hashed password and print it to the console.</p><p id="-1116233166#-t5i35u_48"><span class="inline-code" id="-1116233166#-t5i35u_54">hms-mirror -pkey cloudera -dp HD1eNF8NMFahA2smLM9c4g==</span></p><p id="-1116233166#-t5i35u_49">Will generate:</p><div class="detached code-block" id="-1116233166#-t5i35u_50"><pre><code class="language-none">=== Errors ===
	38:Password en/de crypt

=== Warnings ===
	57:Decrypted password: have-a-nice-day</code></pre></div><p id="-1116233166#-t5i35u_51">Again, ignore the error 38, it's just a warning that the password is being decrypted. The decrypted password is the value after the <span class="inline-code" id="-1116233166#-t5i35u_55">Decrypted password:</span> string.</p><p id="-1116233166#-t5i35u_52">This should match the password you used to generate the encrypted password.</p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="893562735">Memory Settings</h1><p id="893562735#z682tpd_3">The default memory footprint of <span class="inline-code" id="893562735#z682tpd_9">hms-mirror</span> is set in the startup script here (<a href="https://github.com/cloudera-labs/hms-mirror/blob/560166ca90c0d7ce243f8215ba9f29ca1a10cba2/bin/hms-mirror#L70">https://github.com/cloudera-labs/hms-mirror/blob/560166ca90c0d7ce243f8215ba9f29ca1a10cba2/bin/hms-mirror#L70</a>) which declares an 8GB max footprint.</p><p id="893562735#z682tpd_4">If you need to increase this, you can opt to export <span class="inline-code" id="893562735#z682tpd_11">APP_JAVA_OPTS</span> before running <span class="inline-code" id="893562735#z682tpd_12">hms-mirror</span>. You'll need to include:</p><ul class="list" id="893562735#z682tpd_5" start="1"><li class="list-item" id="893562735#z682tpd_13"><p>Xms</p></li><li class="list-item" id="893562735#z682tpd_14"><p>Xmx</p></li><li class="list-item" id="893562735#z682tpd_15"><p>Garbage Collection info.</p></li></ul><p id="893562735#z682tpd_6">For example:</p><p id="893562735#z682tpd_7"><span class="inline-code" id="893562735#z682tpd_16">export APP_JAVA_OPTS=&quot;-Xms8192m -Xmx16384m -XX:+UseG1GC</span></p><p id="893562735#z682tpd_8">This raises the memory limit to 16Gb.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="28311084">Default Configuration Template</h1><p id="28311084#idrg3o_3">Use this as a template for the <span class="inline-code" id="28311084#idrg3o_5">default.yaml</span> configuration file used by the <span class="inline-code" id="28311084#idrg3o_6">cli</span> interface. You can also build a configuration file in the 'web' interface and reference it in the <span class="inline-code" id="28311084#idrg3o_7">cli</span> interface.</p><div class="detached code-block" id="28311084#idrg3o_4"><pre><code class="language-yaml"># Copyright 2024 Cloudera, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

transfer:
  # Optional (default: 4)
  concurrency:         10
  # Optional (default: 'transfer_')
  transferPrefix:      &quot;hms_mirror_transfer_&quot;
  # This directory is appended to the 'clusters:...:hcfsNamespace' value to store the transfer package for hive export/import.
  # Optional (default: '/apps/hive/warehouse/export_')
  exportBaseDirPrefix: &quot;/apps/hive/warehouse/export_&quot;
clusters:
  LEFT:
    # Set for Hive 1/2 environments
    legacyHive:    true
    # Is the 'Hadoop COMPATIBLE File System' used to prefix data locations for this cluster.
    # It is mainly used as the transfer location for metadata (export)
    # If the primary storage for this cluster is 'hdfs' than use 'hdfs://...'
    # If the primary storage for this action is cloud storage, use the
    #    cloud storage prefix. IE: s3a://my_bucket
    hcfsNamespace: &quot;&lt;hdfs://namespace&gt;&quot;
    hiveServer2:
      # URI is the Hive JDBC URL in the form of:
      # jdbc:hive2://&lt;server&gt;:&lt;port&gt;
      # See docs for restrictions
      uri:     &quot;&lt;LEFT-cluster-jdbc-url&gt;&quot;
      connectionProperties:
        user:     &quot;*****&quot;
        password: &quot;*****&quot;
      # Standalone jar file used to connect via JDBC to the LEFT environment Hive Server 2
      # NOTE-1: Hive 3 jars will NOT work against Hive 1.  The protocol isn't compatible.
      # NOTE-2: You can specify a 'colon' separated list of jar files in the jarFile property. We've found this
      #       useful when the JDBC driver requires additional jar files to be present in the classpath to support it.
      #       For example, the CDH 5 Hive JDBC driver requires that the `hadoop-common` jar file to be present in the
      #       classpath. You can specify this as follows: jarFile: &quot;/path/to/hive-jdbc.jar:/path/to/hadoop-common.jar&quot;
      #    The order of the jar files is important. The first jar file in the list MUST have the JDBC driver class file.
      jarFile: &quot;&lt;environment-specific-jdbc-standalone-driver&gt;&quot;
    # Optional.  Required only for (-epl) with DUMP or SCHEMA_ONLY
    # This will require the user to install the jdbc driver for the metastoreDirect in $HOME/.hms-mirror/aux_libs
    metastore_direct:
      uri: &quot;&lt;jdbc_url_to_metastore_db_including_db&gt;&quot;
      type: MYSQL|POSTGRES|ORACLE
      connectionProperties:
        user: &quot;&lt;db_user&gt;&quot;
        password: &quot;&lt;db_password&gt;&quot;
      connectionPool:
        min: 3
        max: 5
  RIGHT:
    legacyHive:    false
    # Is the 'Hadoop COMPATIBLE File System' used to prefix data locations for this cluster.
    # It is mainly used to as a baseline for where &quot;DATA&quot; will be transfered in the
    # STORAGE stage.  The data location in the source location will be move to this
    # base location + the extended path where it existed in the source system.
    # The intent is to keep the data in the same relative location for this new cluster
    # as the old cluster.
    hcfsNamespace: &quot;&lt;hdfs://namespace&gt;&quot;
    hiveServer2:
      # URI is the Hive JDBC URL in the form of:
      # jdbc:hive2://&lt;server&gt;:&lt;port&gt;
      # See docs for restrictions
      uri:     &quot;&lt;RIGHT-cluster-jdbc-url&gt;&quot;
      connectionProperties:
        user:     &quot;*****&quot;
        password: &quot;*****&quot;
      # Standalone jar file used to connect via JDBC to the LEFT environment Hive Server 2
      # NOTE-1: Hive 3 jars will NOT work against Hive 1.  The protocol isn't compatible.
      # NOTE-2: You can specify a 'colon' separated list of jar files in the jarFile property. We've found this
      #       useful when the JDBC driver requires additional jar files to be present in the classpath to support it.
      #       For example, the CDH 5 Hive JDBC driver requires that the `hadoop-common` jar file to be present in the
      #       classpath. You can specify this as follows: jarFile: &quot;/path/to/hive-jdbc.jar:/path/to/hadoop-common.jar&quot;
      #    The order of the jar files is important. The first jar file in the list MUST have the JDBC driver class file.
      jarFile: &quot;&lt;environment-specific-jdbc-standalone-driver&gt;&quot;
    partitionDiscovery:
      # Addition HMS configuration needed for this &quot;discover.partitions&quot;=&quot;true&quot;
      auto:     true
      # When a table is created, run MSCK when there are partitions.
      initMSCK: true</code></pre></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1624968880">Running</h1><p id="-1624968880#-1bh93q_3">After running the <span class="inline-code" id="-1624968880#-1bh93q_9">setup.sh</span> script, <span class="inline-code" id="-1624968880#-1bh93q_10">hms-mirror</span> will be available in the <span class="inline-code" id="-1624968880#-1bh93q_11">$PATH</span> in a default configuration.</p><section><h2 id="-1624968880#assumptions" data-toc="assumptions#hms-mirror-running.md-assumptions">Assumptions</h2><ol class="list list-decimal" id="-1624968880#-1bh93q_12" type="1" start="1"><li class="list-item" id="-1624968880#-1bh93q_18"><p>This process will only 'migrate' EXTERNAL and MANAGED (non-ACID/Transactional) table METADATA (not data, except with SQL (<a href="#82350">SQL</a>) and EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>)).</p></li><li class="list-item" id="-1624968880#-1bh93q_19"><p>MANAGED tables replicated to the **RIGHT ** cluster will be converted to &quot;EXTERNAL&quot; tables for the 'metadata' stage. They will be tagged as 'legacy managed' in the **RIGHT ** cluster. They will be assigned the <span class="inline-code" id="-1624968880#-1bh93q_26">external.table.purge=true</span> flag, to continue the behaviors of the legacy managed tables.</p></li><li class="list-item" id="-1624968880#-1bh93q_20"><p>The <span class="control" id="-1624968880#-1bh93q_27">RIGHT</span> cluster has 'line of sight' to the <span class="control" id="-1624968880#-1bh93q_28">LEFT</span> cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_21"><p>The <span class="control" id="-1624968880#-1bh93q_29">RIGHT</span> cluster has been configured to access the **LEFT ** cluster storage. See link clusters (<a href="#1958103884">Linking Cluster Storage Layers</a>). This is the same configuration required to support <span class="inline-code" id="-1624968880#-1bh93q_31">distcp</span> from the <span class="control" id="-1624968880#-1bh93q_32">RIGHT</span> cluster to the <span class="control" id="-1624968880#-1bh93q_33">LEFT</span> cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_22"><p>The movement of metadata/data is from the <span class="control" id="-1624968880#-1bh93q_34">LEFT</span> cluster to the <span class="control" id="-1624968880#-1bh93q_35">RIGHT</span> cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_23"><p>With Kerberos, each cluster must share the same trust mechanism.</p></li></ol><ul class="list" id="-1624968880#-1bh93q_13" start="1"><li class="list-item" id="-1624968880#-1bh93q_36"><p>The <span class="control" id="-1624968880#-1bh93q_38">RIGHT</span> cluster must be Kerberized IF the <span class="control" id="-1624968880#-1bh93q_39">LEFT</span> cluster is.</p></li><li class="list-item" id="-1624968880#-1bh93q_37"><p>The <span class="control" id="-1624968880#-1bh93q_40">LEFT</span> cluster does NOT need to be kerberized if the <span class="control" id="-1624968880#-1bh93q_41">RIGHT</span> cluster is kerberized.</p></li></ul><ol class="list list-decimal" id="-1624968880#-1bh93q_14" type="1" start="7"><li class="list-item" id="-1624968880#-1bh93q_42"><p>The * <span class="emphasis" id="-1624968880#-1bh93q_44">LEFT</span> cluster does NOT have access to the <span class="control" id="-1624968880#-1bh93q_45">RIGHT</span> cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_43"><p>The credentials use by 'hive' (doas=false) in the **RIGHT ** cluster must have access to the required storage (hdfs) locations on the lower cluster.</p></li></ol><ul class="list" id="-1624968880#-1bh93q_15" start="1"><li class="list-item" id="-1624968880#-1bh93q_46"><p>If the **RIGHT ** cluster is running impersonation (doas=true), that user must have access to the required storage (hdfs) locations on the lower cluster.</p></li></ul><section><h3 id="-1624968880#transfer-data-beyond-the-metadata" data-toc="transfer-data-beyond-the-metadata#hms-mirror-running.md-transfer-data-beyond-the-metadata">Transfer DATA, beyond the METADATA</h3><p id="-1624968880#-1bh93q_47">HMS-Mirror does NOT migrate data between clusters unless you're using the SQL (<a href="#82350">SQL</a>) or EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>) data strategies. In some cases where data is co-located, you don't need to move it. IE: Cloud to Cloud. As long as the new cluster environment has access to the original location. This is the intended target for strategies COMMON (<a href="#1993481707">COMMON</a>) and to some extend LINKED (<a href="#-2049336807">LINKED</a>).</p><p id="-1624968880#-1bh93q_48">When you do need to move data, <span class="inline-code" id="-1624968880#-1bh93q_53">hms-mirror</span> creates a workbook of 'source' and 'target' locations in an output file called <span class="inline-code" id="-1624968880#-1bh93q_54">distcp_workbook.md</span>. Use this to help build a transfer job in <span class="inline-code" id="-1624968880#-1bh93q_55">distcp</span> using the <span class="inline-code" id="-1624968880#-1bh93q_56">-f</span> option to specify multiple sources.</p></section><section><h3 id="-1624968880#application-return-codes" data-toc="application-return-codes#hms-mirror-running.md-application-return-codes">Application Return Codes</h3><p id="-1624968880#-1bh93q_57">The <span class="inline-code" id="-1624968880#-1bh93q_62">hms-mirror</span> application returns <span class="inline-code" id="-1624968880#-1bh93q_63">0</span> when everything is ok. If there is a configuration validation issue, the return code will be a negative value who's absolute value represents the bitSets cumulative <span class="inline-code" id="-1624968880#-1bh93q_64">OR</span> value. See: MessageCodes (<a href="https://github.com/cloudera-labs/hms-mirror/blob/main/src/main/java/com/cloudera/utils/hadoop/hms/mirror/MessageCode.java">https://github.com/cloudera-labs/hms-mirror/blob/main/src/main/java/com/cloudera/utils/hadoop/hms/mirror/MessageCode.java</a>) for values and Messages.java for the calculation (<a href="https://github.com/cloudera-labs/hms-mirror/blob/df9df251803d8722ef67426a73cbcfb86f981d3e/src/main/java/com/cloudera/utils/hadoop/hms/mirror/Messages.java#L26">https://github.com/cloudera-labs/hms-mirror/blob/df9df251803d8722ef67426a73cbcfb86f981d3e/src/main/java/com/cloudera/utils/hadoop/hms/mirror/Messages.java#L26</a>).</p><p id="-1624968880#-1bh93q_58">When you receive an error code (negative value), you'll also get the items printed to the screen and the log that make up that error code.</p><p id="-1624968880#-1bh93q_59">For example, the following would yield a code of <span class="inline-code" id="-1624968880#-1bh93q_67">-2305843009214742528</span> (20 and 61).</p><div class="detached code-block" id="-1624968880#-1bh93q_60"><pre><code class="language-none">******* ERRORS *********
20:STORAGE_MIGRATION requires you to specify PATH location for 'managed' and 'external' tables (-wd, -ewd) to migrate storage.  These will be appended to the -smn (storage-migration-namespace) parameter and used to set the 'database' LOCATION and MANAGEDLOCATION properties
61:You're using the same namespace in STORAGE_MIGRATION, without `-rdl` you'll need to ensure you have `-glm` set to map locations.</code></pre></div><p id="-1624968880#-1bh93q_61"><span class="inline-code" id="-1624968880#-1bh93q_68">((2^20)+(2^61))*-1=-2305843009214742528</span></p></section></section><section><h2 id="-1624968880#running-against-a-legacy-non-cdp-kerberized-hiveserver2" data-toc="running-against-a-legacy-non-cdp-kerberized-hiveserver2#hms-mirror-running.md-running-against-a-legacy-non-cdp-kerberized-hiveserver2">Running Against a LEGACY (Non-CDP) Kerberized HiveServer2</h2><p id="-1624968880#-1bh93q_69"><span class="inline-code" id="-1624968880#-1bh93q_71">hms-mirror</span> is pre-built with CDP libraries and WILL NOT be compatible with LEGACY kerberos environments. A Kerberos connection can only be made to ONE cluster when the clusters are NOT running the same 'major' version of Hadoop.</p><p id="-1624968880#-1bh93q_70">To attach to a LEGACY HS2, run <span class="inline-code" id="-1624968880#-1bh93q_72">hms-mirror</span> with the <span class="inline-code" id="-1624968880#-1bh93q_73">--hadoop-classpath</span> command-line option. This will strip the CDP libraries from <span class="inline-code" id="-1624968880#-1bh93q_74">hms-mirror</span> and use the hosts Hadoop libraries by calling <span class="inline-code" id="-1624968880#-1bh93q_75">hadoop classpath</span> to locate the binaries needed to do this.</p></section><section><h2 id="-1624968880#on-prem-to-cloud-migrations" data-toc="on-prem-to-cloud-migrations#hms-mirror-running.md-on-prem-to-cloud-migrations">On-Prem to Cloud Migrations</h2><p id="-1624968880#-1bh93q_76">On-Prem to Cloud Migrations should run <span class="inline-code" id="-1624968880#-1bh93q_82">hms-mirror</span> from the LEFT cluster since visibility in this scenario is usually restricted to LEFT-&gt;RIGHT.</p><p id="-1624968880#-1bh93q_77">If the cluster is an older version of Hadoop (HDP 2, CDH 5), your connection to the LEFT HS2 should NOT be kerberized. Use LDAP or NO_AUTH.</p><p id="-1624968880#-1bh93q_78">The clusters LEFT hcfsNamespace (clusters:LEFT:hcfsNamespace) should be the LEFT clusters HDFS service endpoint. The RIGHT hcfsNamespace (clusters:RIGHT:hcfsNamespace) should be the <span class="emphasis" id="-1624968880#-1bh93q_83">target</span> root cloud storage location. The LEFT clusters configuration (/etc/hadoop/conf) should have all the necessary credentials to access this location. Ensure that the cloud storage connectors are available in the LEFT environment.</p><p id="-1624968880#-1bh93q_79">There are different strategies available for migrations between on-prem and cloud environments.</p><section><h3 id="-1624968880#schema-only" data-toc="schema-only#hms-mirror-running.md-schema-only">SCHEMA_ONLY</h3><p id="-1624968880#-1bh93q_84">This is a schema-only transfer, where the <span class="inline-code" id="-1624968880#-1bh93q_87">hcfsNamespace</span> in the metadata definitions is 'replaced' with the <span class="inline-code" id="-1624968880#-1bh93q_88">hcfsNamespace</span> value defined on the RIGHT. NOTE: The 'relative' directory location is maintained in the migration.</p><p id="-1624968880#-1bh93q_85">No data will be migrated in this case.</p><p id="-1624968880#-1bh93q_86">There will be a <span class="inline-code" id="-1624968880#-1bh93q_91">distcp</span> Planning Workbook (<a href="#45251276#distcp-planning-workbook-and-scripts">&quot;distcp Planning Workbook and Scripts&quot; in &quot;Features&quot;</a>) generated with a plan that can be used to build the data migration process with <span class="inline-code" id="-1624968880#-1bh93q_90">distcp</span>.</p></section><section><h3 id="-1624968880#intermediate" data-toc="intermediate#hms-mirror-running.md-intermediate">INTERMEDIATE</h3></section></section><section><h2 id="-1624968880#connections" data-toc="connections#hms-mirror-running.md-connections">Connections</h2><p id="-1624968880#-1bh93q_92"><span class="inline-code" id="-1624968880#-1bh93q_96">hms-mirror</span> connects to 3 endpoints. The hive jdbc endpoints for each cluster (2) and the <span class="inline-code" id="-1624968880#-1bh93q_97">hdfs</span> environment configured on the running host. This means you'll need:</p><ul class="list" id="-1624968880#-1bh93q_93" start="1"><li class="list-item" id="-1624968880#-1bh93q_98"><p>JDBC drivers to match the JDBC endpoints</p></li><li class="list-item" id="-1624968880#-1bh93q_99"><p>For <span class="control" id="-1624968880#-1bh93q_100">non</span> CDP 7.x environments and Kerberos connections, an edge node with the current Hadoop libraries.</p></li></ul><p id="-1624968880#-1bh93q_94">See the config (<a href="#1489882229">Configuration</a>) section to setup the config file for <span class="inline-code" id="-1624968880#-1bh93q_102">hms-mirror</span>.</p><section><h3 id="-1624968880#configuring-the-libraries" data-toc="configuring-the-libraries#hms-mirror-running.md-configuring-the-libraries">Configuring the Libraries</h3><section><h4 id="-1624968880#aux-libs-classpath-additions" data-toc="aux-libs-classpath-additions#hms-mirror-running.md-aux-libs-classpath-additions">AUX_LIBS - CLASSPATH Additions</h4><section><h5 id="-1624968880#s3" data-toc="s3#hms-mirror-running.md-s3">S3</h5><p id="-1624968880#-1bh93q_110">The directory $HOME/.hms-mirror/aux_libs will be scanned for 'jar' files. Each 'jar' will be added the java classpath of the application. Add any required libraries here.</p><p id="-1624968880#-1bh93q_111">The application contains all the necessary hdfs classes already. You will need to add to the aux_libs directory the following:</p><ul class="list" id="-1624968880#-1bh93q_112" start="1"><li class="list-item" id="-1624968880#-1bh93q_113"><p>JDBC driver for HS2 Connectivity (only when using Kerberos)</p></li><li class="list-item" id="-1624968880#-1bh93q_114"><p>AWS S3 Drivers, if s3 is used to store Hive tables. (appropriate versions) </p><ul class="list" id="-1624968880#-1bh93q_115" start="1"><li class="list-item" id="-1624968880#-1bh93q_116"><p>hadoop-aws.jar</p></li><li class="list-item" id="-1624968880#-1bh93q_117"><p>aws-java-sdk-bundle.jar</p></li></ul></li></ul></section></section><section><h4 id="-1624968880#jdbc-connection-strings-for-hs2" data-toc="jdbc-connection-strings-for-hs2#hms-mirror-running.md-jdbc-connection-strings-for-hs2">JDBC Connection Strings for HS2</h4><p id="-1624968880#-1bh93q_118">See the Apache docs (<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30758725#HiveServer2Clients-JDBC">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30758725#HiveServer2Clients-JDBC</a>) regarding these details if you are using the environment 'Standalone' JDBC drivers. Other drivers may have different connect string requirements.</p><p id="-1624968880#-1bh93q_119">The drivers for the various environments are located:</p><ul class="list" id="-1624968880#-1bh93q_120" start="1"><li class="list-item" id="-1624968880#-1bh93q_122"><p>HDP - <span class="inline-code" id="-1624968880#-1bh93q_124">/usr/hdp/current/hive-server2/jdbc/hive-jdbc-&lt;version&gt;-standalone.jar</span> (NOTE: Use the hive-1 standalone jar file for HDP 2.6.5, not the hive-2 jar)</p></li><li class="list-item" id="-1624968880#-1bh93q_123"><p>CDH/CDP - <span class="inline-code" id="-1624968880#-1bh93q_125">/opt/cloudera/parcels/CDH/jars/hive-jdbc-&lt;version&gt;-standalone.jar</span></p></li></ul></section><section><h4 id="-1624968880#non-kerberos-connections" data-toc="non-kerberos-connections#hms-mirror-running.md-non-kerberos-connections">Non-Kerberos Connections</h4><p id="-1624968880#-1bh93q_126">The most effortless connections are 'non-kerberos' JDBC connections either to HS2 with AUTH models that aren't * <span class="emphasis" id="-1624968880#-1bh93q_132">Kerberos</span>* or through a <span class="control" id="-1624968880#-1bh93q_133">Knox</span> proxy. Under these conditions, only the __standalone __ JDBC drivers are required. Each of the cluster configurations contains an element <span class="inline-code" id="-1624968880#-1bh93q_134">jarFile</span> to identify those standalone libraries.</p><div class="detached code-block" id="-1624968880#-1bh93q_127"><pre><code class="language-yaml">hiveServer2:
  uri: &quot;&lt;jdbc-url&gt;&quot;
  connectionProperties:
    user: &quot;*****&quot;
    password: &quot;*****&quot;
  jarFile: &quot;&lt;environment-specific-jdbc-standalone-driver&gt;&quot;</code></pre></div><p id="-1624968880#-1bh93q_128">When dealing with clusters supporting different Hive (Hive 1 vs. Hive 3) versions, the JDBC drivers aren't forward OR backward compatible between these versions. Hence, each JDBC jar file is loaded in a sandbox that allows us to use the same driver class, but isolates it between the two JDBC jars.</p><p id="-1624968880#-1bh93q_129">Place the two jdbc jar files in any directory **EXCEPT ** <span class="inline-code" id="-1624968880#-1bh93q_135">$HOME/.hms-mirror/aux_libs</span> and reference the full path in the <span class="inline-code" id="-1624968880#-1bh93q_136">jarFile</span> property for that <span class="inline-code" id="-1624968880#-1bh93q_137">hiveServer2</span> configuration.</p><p id="-1624968880#-1bh93q_130"><span class="emphasis" id="-1624968880#-1bh93q_138">SAMPLE Commandline</span></p><p id="-1624968880#-1bh93q_131"><span class="inline-code" id="-1624968880#-1bh93q_139">hms-mirror -db tpcds_bin_partitioned_orc_10</span></p></section><section><h4 id="-1624968880#kerberized-connections" data-toc="kerberized-connections#hms-mirror-running.md-kerberized-connections">Kerberized Connections</h4><p id="-1624968880#-1bh93q_140"><span class="inline-code" id="-1624968880#-1bh93q_147">hms-mirror</span> relies on the Hadoop libraries to connect via 'kerberos'. Suppose the clusters are running different versions of Hadoop/Hive. In that case, we can only support connecting to one of the clusters via Kerberos.</p><blockquote class="prompt flex bordered-element-rounded warning detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Packaging improvements in version 2.x include all the Kerberos libraries you need to connection to a kerberized Hive Server2 and HDFS. The `--hadoop-classpath` option is no longer required for Kerberos connections. The application will use the embedded Hadoop 3.1 libraries to connect to the kerberized endpoints. The `--hadoop-classpath` option is still available for connecting to older Hadoop 2.x environments.</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded note detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M21 12a9 9 0 1 1-9-9 9 9 0 0 1 9 9zM10.5 7.5A1.5 1.5 0 1 0 12 6a1.5 1.5 0 0 0-1.5 1.5zm-.5 3.54v1h1V18h2v-6a.96.96 0 0 0-.96-.96z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>The `jarFile` property is NOT used for Kerberos connections. The JDBC jar file for the kerberized cluster should be placed in the `$HOME/.hms-mirror/aux_libs` directory.</p></div>
</blockquote>
<p id="-1624968880#-1bh93q_143">There are three scenarios for kerberized connections.</p><div class="table-wrapper detached"><table id="-1624968880#-1bh93q_144"><tr class="header-row" id="-1624968880#-1bh93q_148"><th id="-1624968880#-1bh93q_153"><p>Scenario</p></th><th id="-1624968880#-1bh93q_154"><p>LEFT Kerberized/Version</p></th><th id="-1624968880#-1bh93q_155"><p>RIGHT Kerberized/Version</p></th><th id="-1624968880#-1bh93q_156"><p>Notes</p></th><th id="-1624968880#-1bh93q_157"><p>Sample Commandline</p></th></tr><tr class="" id="-1624968880#-1bh93q_149"><td id="-1624968880#-1bh93q_158"><p>1</p></td><td id="-1624968880#-1bh93q_159"><p>No </p><p> HDP2</p></td><td id="-1624968880#-1bh93q_160"><p>Yes </p><p> HDP 3 or CDP 7</p></td><td id="-1624968880#-1bh93q_161"><ol class="list list-decimal" id="-1624968880#-1bh93q_165" type="1" start="1"><li class="list-item" id="-1624968880#-1bh93q_166"><p>'hms-mirror' needs to be run from a node on the HDP3/CDP cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_167"><p>place the RIGHT cluster jdbc jar file in `$HOME/.hms-mirror/aux_libs` (yes this contradicts some earlier directions)</p></li><li class="list-item" id="-1624968880#-1bh93q_168"><p>comment out the `jarFile` property for the RIGHT cluster hiveServer2 setting.</p></li></ol></td><td id="-1624968880#-1bh93q_162"><p>`hms-mirror -db tpcds_bin_partitioned_orc_10 --hadoop-classpath`</p></td></tr><tr class="" id="-1624968880#-1bh93q_150"><td id="-1624968880#-1bh93q_169"><p>2</p></td><td id="-1624968880#-1bh93q_170"><p>YES </p><p> HDP 3 or CDP 7</p></td><td id="-1624968880#-1bh93q_171"><p>YES </p><p> HDP 3 or CDP 7</p></td><td id="-1624968880#-1bh93q_172"><ol class="list list-decimal" id="-1624968880#-1bh93q_176" type="1" start="1"><li class="list-item" id="-1624968880#-1bh93q_177"><p>'hms-mirror' needs to be run from a node on the HDP3/CDP cluster.</p></li><li class="list-item" id="-1624968880#-1bh93q_178"><p>place the RIGHT cluster jdbc jar file in $HOME/.hms-mirror/aux_libs (yes this contradicts some earlier directions)</p></li><li class="list-item" id="-1624968880#-1bh93q_179"><p>comment out the `jarFile` property for the LEFT AND RIGHT cluster hiveServer2 settings.</p></li></ol></td><td id="-1624968880#-1bh93q_173"><p>`hms-mirror -db tpcds_bin_partitioned_orc_10 --hadoop-classpath`</p></td></tr><tr class="" id="-1624968880#-1bh93q_151"><td id="-1624968880#-1bh93q_180"><p>3</p></td><td id="-1624968880#-1bh93q_181"><p>YES </p><p> HDP 2 or Hive 1</p></td><td id="-1624968880#-1bh93q_182"><p>NO </p><p> HDP 3 or CDP 7</p></td><td id="-1624968880#-1bh93q_183"><p>Limited testing, but you'll need to run `hms-mirror` ON the **LEFT** cluster and include the LEFT clusters hive standalone jdbc driver in `$HOME/.hms-mirror/cfg/aux_libs`.</p></td><td id="-1624968880#-1bh93q_184"><p>`hms-mirror -db tpcds_bin_partitioned_orc_10 --hadoop-classpath`</p></td></tr><tr class="" id="-1624968880#-1bh93q_152"><td id="-1624968880#-1bh93q_187"><p>4</p></td><td id="-1624968880#-1bh93q_188"><p>YES </p><p> HDP 2 or Hive 1</p></td><td id="-1624968880#-1bh93q_189"><p>YES </p><p> HDP 2 or Hive 1</p></td><td id="-1624968880#-1bh93q_190"><ol class="list list-decimal" id="-1624968880#-1bh93q_194" type="1" start="1"><li class="list-item" id="-1624968880#-1bh93q_195"><p>The Kerberos credentials must be TRUSTED to both clusters</p></li><li class="list-item" id="-1624968880#-1bh93q_196"><p>Add `--hadoop-classpath` as a commandline option to `hms-mirror`. This replaces the prebuilt Hadoop 3 libraries with the current environments Hadoop Libs.</p></li><li class="list-item" id="-1624968880#-1bh93q_197"><p>Add the jdbc standalone jar file to `$HOME/.hms-mirror/aux_libs`</p></li><li class="list-item" id="-1624968880#-1bh93q_198"><p>Comment out/remove the `jarFile` references for BOTH clusters in the configuration file.</p></li></ol></td><td id="-1624968880#-1bh93q_191"><p>`hms-mirror -db tpcds_bin_partitioned_orc_10 --hadoop-classpath`</p></td></tr></table></div><p id="-1624968880#-1bh93q_145">For Kerberos JDBC connections, ensure you are using an appropriate Kerberized Hive URL.</p><p id="-1624968880#-1bh93q_146"><span class="inline-code" id="-1624968880#-1bh93q_199">jdbc:hive2://s03.streever.local:10000/;principal=hive/_HOST@STREEVER.LOCAL</span></p></section><section><h4 id="-1624968880#zookeeper-discovery-connections" data-toc="zookeeper-discovery-connections#hms-mirror-running.md-zookeeper-discovery-connections">ZooKeeper Discovery Connections</h4><p id="-1624968880#-1bh93q_200">You may run into issues connecting to an older cluster using ZK Discovery. This mode brings in a LOT of the Hadoop ecosystem classes and may conflict across environments. We recommend using ZooKeeper discovery on only the RIGHT cluster. Adjust the LEFT cluster to access HS2 directly.</p></section><section><h4 id="-1624968880#tls-ssl-connections" data-toc="tls-ssl-connections#hms-mirror-running.md-tls-ssl-connections">TLS/SSL Connections</h4><p id="-1624968880#-1bh93q_201">If your HS2 connection requires TLS, you will need to include that detail in the jdbc 'uri' you provide. In addition, if the SSL certificate is 'self-signed' you will need to include details about the certificate to the java environment. You have 2 options:</p><ul class="list" id="-1624968880#-1bh93q_202" start="1"><li class="list-item" id="-1624968880#-1bh93q_203"><p>Set the JAVA_OPTS environment with the details about the certificate. </p><ul class="list" id="-1624968880#-1bh93q_205" start="1"><li class="list-item" id="-1624968880#-1bh93q_206"><p><span class="inline-code" id="-1624968880#-1bh93q_207">export JAVA_OPTS=-Djavax.net.ssl.trustStore=/home/dstreev/certs/gateway-client-trust.jks -Djavax.net.ssl.trustStorePassword=changeit</span></p></li></ul></li><li class="list-item" id="-1624968880#-1bh93q_204"><p>Add <span class="inline-code" id="-1624968880#-1bh93q_208">-D</span> options to the <span class="inline-code" id="-1624968880#-1bh93q_209">hms-mirror</span> commandline to inject those details. </p><ul class="list" id="-1624968880#-1bh93q_210" start="1"><li class="list-item" id="-1624968880#-1bh93q_211"><p><span class="inline-code" id="-1624968880#-1bh93q_212">hms-mirror -db test_db -Djavax.net.ssl.trustStore=/home/dstreev/certs/gateway-client-trust.jks -Djavax.net.ssl.trustStorePassword=changeit</span></p></li></ul></li></ul></section></section></section><section><h2 id="-1624968880#troubleshooting" data-toc="troubleshooting#hms-mirror-running.md-troubleshooting">Troubleshooting</h2><p id="-1624968880#-1bh93q_213">If each JDBC endpoint is Kerberized and the connection to the LEFT or RIGHT is successful, both NOT both, and the program seems to hang with no exception... it's most likely that the Kerberos ticket isn't TRUSTED across the two environments. You will only be able to support a Kerberos connection to the cluster where the ticket is trusted. The other cluster connection will need to be anything BUT Kerberos.</p><p id="-1624968880#-1bh93q_214">Add <span class="inline-code" id="-1624968880#-1bh93q_219">--show-cp</span> to the <span class="inline-code" id="-1624968880#-1bh93q_220">hms-mirror</span> command line to see the classpath used to run.</p><p id="-1624968880#-1bh93q_215">The argument <span class="inline-code" id="-1624968880#-1bh93q_221">--hadoop-classpath</span> allows us to replace the embedded Hadoop Libs (v3.1) with the libs of the current platform via a call to <span class="inline-code" id="-1624968880#-1bh93q_222">hadoop classpath</span>. This is necessary to connect to kerberized Hadoop v2/Hive v1 environments.</p><p id="-1624968880#-1bh93q_216">Check the location and references to the JDBC jar files. General rules for Kerberos Connections:</p><ul class="list" id="-1624968880#-1bh93q_217" start="1"><li class="list-item" id="-1624968880#-1bh93q_223"><p>The JDBC jar file should be in the <span class="inline-code" id="-1624968880#-1bh93q_225">$HOME/.hms-mirror/aux_libs</span>. For Kerberos connections, we've seen issues attempting to load this jar in a sandbox, so this makes it available to the global classpath/loader.</p></li><li class="list-item" id="-1624968880#-1bh93q_224"><p>Get a Kerberos ticket for the running user before launching <span class="inline-code" id="-1624968880#-1bh93q_226">hms-mirror</span>.</p></li></ul><section><h3 id="-1624968880#unrecognized-hadoop-major-version-number-3-1-1-7-1-0-257" data-toc="unrecognized-hadoop-major-version-number-3-1-1-7-1-0-257#hms-mirror-running.md-unrecognized-hadoop-major-version-number-3-1-1-7-1-0-257">&quot;Unrecognized Hadoop major version number: 3.1.1.7.1...0-257&quot;</h3><p id="-1624968880#-1bh93q_227">This happens when you're trying to connect to an HS2 instance.</p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1566128265">Optimizations</h1><p id="-1566128265#c2lroj_3">Moving metadata and data between two clusters is a pretty straightforward process but depends entirely on the proper configurations in each cluster. Listed here are a few tips on some crucial configurations.</p><p id="-1566128265#c2lroj_4">HMS-Mirror only moves data with the SQL (<a href="#82350">SQL</a>) and EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>) data strategies. All other strategies either use the data as-is (LINKED (<a href="#-2049336807">LINKED</a>) or COMMON (<a href="#1993481707">COMMON</a>)) or depend on the data being moved by something like <span class="inline-code" id="-1566128265#c2lroj_14">distcp</span>.</p><section><h2 id="-1566128265#controlling-the-yarn-queue-that-runs-the-sql-queries-from-hms-mirror" data-toc="controlling-the-yarn-queue-that-runs-the-sql-queries-from-hms-mirror#hms-mirror-optimizations.md-controlling-the-yarn-queue-that-runs-the-sql-queries-from-hms-mirror">Controlling the YARN Queue that runs the SQL queries from hms-mirror</h2><p id="-1566128265#c2lroj_16">Use the jdbc url defined in <span class="inline-code" id="-1566128265#c2lroj_20">default.yaml</span> to set a queue.</p><p id="-1566128265#c2lroj_17"><span class="inline-code" id="-1566128265#c2lroj_21">jdbc:hive2://host:10000/.....;...?tez.queue.name=batch</span></p><p id="-1566128265#c2lroj_18">The commandline properties <span class="inline-code" id="-1566128265#c2lroj_22">-po</span>, <span class="inline-code" id="-1566128265#c2lroj_23">-pol</span>, and <span class="inline-code" id="-1566128265#c2lroj_24">-por</span> can be used to override the queue name as well. For example: <span class="inline-code" id="-1566128265#c2lroj_25">-pol tez.queue.name=batch</span> will set the queue for the &quot;LEFT&quot; cluster while <span class="inline-code" id="-1566128265#c2lroj_26">-por tez.queue.name=migration</span> will set the queue for the &quot;RIGHT&quot; cluster.</p></section><section><h2 id="-1566128265#make-backups-before-running-hms-mirror" data-toc="make-backups-before-running-hms-mirror#hms-mirror-optimizations.md-make-backups-before-running-hms-mirror">Make Backups before running hms-mirror</h2><p id="-1566128265#c2lroj_28">Take snapshots of areas you'll touch:</p><ul class="list" id="-1566128265#c2lroj_29" start="1"><li class="list-item" id="-1566128265#c2lroj_31"><p id="-1566128265#c2lroj_33">The HMS database on the LEFT and RIGHT clusters</p></li><li class="list-item" id="-1566128265#c2lroj_32"><p id="-1566128265#c2lroj_34">A snapshot of the HDFS directories on BOTH the LEFT and RIGHT clusters will be used/touched.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-1566128265#c2lroj_36">NOTE: If you are testing and &quot;DROPPING&quot; dbs, Snapshots of those data directories could protect you from accidental deletions if you don't manage purge options correctly. Don't skip this... A snapshot of the db directory on HDFS will prevent <span class="inline-code" id="-1566128265#c2lroj_37">DROP DATABASE x CASCADE</span> from removing the DB directory (observed in CDP 7.1.4+ as tested, check your version) and all sub-directories even though tables were NOT configured with <span class="inline-code" id="-1566128265#c2lroj_38">purge</span> options.</p></div>
</blockquote>
</li></ul></section><section><h2 id="-1566128265#isolate-migration-activities" data-toc="isolate-migration-activities#hms-mirror-optimizations.md-isolate-migration-activities">Isolate Migration Activities</h2><p id="-1566128265#c2lroj_39">The migration of schemas can put a heavy load on HS2 and the HMS server it's using. That impact can manifest itself as 'pauses' for other clients trying to run queries. Extended schema/discovery operations have a 'blocking' tendency in HS2.</p><p id="-1566128265#c2lroj_40">To prevent average user operational impact, I suggest establishing an isolated HMS and HS2 environment for the migration process.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1566128265#c2lroj_41" alt="Isolate Migration Service Endpoints" title="Isolate Migration Service Endpoints" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/isolation.png" width="1048" height="658"><figcaption class="center-text">Isolate Migration Service Endpoints</figcaption></figure></div></section><section><h2 id="-1566128265#speed-up-create-alter-table-statements-with-existing-data" data-toc="speed-up-create-alter-table-statements-with-existing-data#hms-mirror-optimizations.md-speed-up-create-alter-table-statements-with-existing-data">Speed up CREATE/ALTER Table Statements - with existing data</h2><p id="-1566128265#c2lroj_42">Set <span class="inline-code" id="-1566128265#c2lroj_47">ranger.plugin.hive.urlauth.filesystem.schemes=file</span> in the Hive Server 2(hive_on_tez) Ranger Plugin Safety Value, via Cloudera Manager.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1566128265#c2lroj_43" alt="Safety Value" title="Safety Value" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/hs2_ranger_schemas.png" width="1544" height="552"><figcaption class="center-text">Safety Value</figcaption></figure></div><p id="-1566128265#c2lroj_44">Add this to the HS2 instance on the RIGHT cluster when Ranger is used for Auth. This skips the check done against every directory at the table location (for CREATE or ALTER LOCATION). It is allowing the process of CREATE/ALTER to run much faster.</p><p id="-1566128265#c2lroj_45">The default (true) behavior works well for the interactive use case. Still, bulk operations like this can take a long time if this validation needs to happen for every new partition during creation or discovery.</p><p id="-1566128265#c2lroj_46">I recommend turning this back after the migration is complete. This setting exposes permissions issues at the time of CREATE/ALTER. So by skipping this, future access issues may arise if the permissions aren't aligned, which isn't a Ranger/Hive issue, it's a permissions issue.</p></section><section><h2 id="-1566128265#turn-on-hms-partition-discovery" data-toc="turn-on-hms-partition-discovery#hms-mirror-optimizations.md-turn-on-hms-partition-discovery">Turn ON HMS partition discovery</h2><p id="-1566128265#c2lroj_48">In CDP 7.1.4 and below, the housekeeping threads in HMS used to discover partitions are NOT running. Add <span class="inline-code" id="-1566128265#c2lroj_51">metastore.housekeeping.threads.on=true</span> to the HMS Safety Value to activate the partition discovery thread. Once this has been set, the following parameters can be used to modify the default behavior.</p><div class="detached code-block" id="-1566128265#c2lroj_49"><pre><code class="language-none">hive.metastore.partition.management.task.frequency
hive.exec.input.listing.max.threads
hive.load.dynamic.partitions.thread
hive.metastore.fshandler.threads</code></pre></div><section><h3 id="-1566128265#source-reference" data-toc="source-reference#hms-mirror-optimizations.md-source-reference">Source Reference</h3><div class="detached code-block" id="-1566128265#c2lroj_52"><pre><code class="language-none">METASTORE_HOUSEKEEPING_LEADER_HOSTNAME(&quot;metastore.housekeeping.leader.hostname&quot;,
            &quot;hive.metastore.housekeeping.leader.hostname&quot;, &quot;&quot;,
&quot;If multiple Thrift metastore services are running, the hostname of Thrift metastore &quot; +
        &quot;service to run housekeeping tasks at. By default, this value is empty, which &quot; +
        &quot;means that the current metastore will run the housekeeping tasks. If configuration&quot; +
        &quot;metastore.thrift.bind.host is set on the intended leader metastore, this value should &quot; +
        &quot;match that configuration. Otherwise it should be same as the hostname returned by &quot; +
        &quot;InetAddress#getLocalHost#getHostName(). Given the uncertainty in the later &quot; +
        &quot;it is desirable to configure metastore.thrift.bind.host on the intended leader HMS.&quot;),
    METASTORE_HOUSEKEEPING_THREADS_ON(&quot;metastore.housekeeping.threads.on&quot;,
        &quot;hive.metastore.housekeeping.threads.on&quot;, false,
        &quot;Whether to run the tasks under metastore.task.threads.remote on this metastore instance or not.\n&quot; +
            &quot;Set this to true on one instance of the Thrift metastore service as part of turning\n&quot; +
            &quot;on Hive transactions. For a complete list of parameters required for turning on\n&quot; +
            &quot;transactions, see hive.txn.manager.&quot;),</code></pre></div><p id="-1566128265#c2lroj_53">The default batch size for partition discovery via <span class="inline-code" id="-1566128265#c2lroj_54">msck</span> is 3000. Adjustments to this can be made via the <span class="inline-code" id="-1566128265#c2lroj_55">hive.msck.repair.batch.size</span> property in HS2.</p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1057781433">Tips</h1><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Run in `screen` or `tmux` </p><p id="-1057781433#z4djh1l_6">This process can be a long-running process. It depends on how much you've asked it to do. Having the application terminated because the <span class="inline-code" id="-1057781433#z4djh1l_8">ssh</span> session to the edgenode timed out and your computer went to sleep will be very disruptive.</p><p id="-1057781433#z4djh1l_7">Using either of these session state tools (or another of your choice) while running it on an edgenode will allow you to sign-off without disrupting the process AND reattach to see the interactive progress at a later point.</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Use `dryrun` FIRST </p><p id="-1057781433#z4djh1l_9">Before you run a process that will make changes, try running <span class="inline-code" id="-1057781433#z4djh1l_10">hms-mirror</span> with the <span class="inline-code" id="-1057781433#z4djh1l_11">dry-run</span> option first. The report generated at the end of the job will provide insight into what issues (if any) you'll run across.</p></div>
</blockquote>
<blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Start Small </p><p id="-1057781433#z4djh1l_12">Use <span class="inline-code" id="-1057781433#z4djh1l_15">-db</span> (database) AND <span class="inline-code" id="-1057781433#z4djh1l_16">-tf</span> (table filter) options to limit the scope of what you're processing. Start with a test database that contains various table types you'd like to migrate.</p><p id="-1057781433#z4djh1l_13">Review the output report for details/problems you encountered in processing.</p><p id="-1057781433#z4djh1l_14">Since the process expects the user to have adequate permissions in both clusters, you may find you have some prework to do before continuing.</p></div>
</blockquote>
</article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-356146741">Location Alignment</h1><p id="-356146741#-a1512p_3">In the Hive Metastore, Database definitions, Table Schemas, and Partition details include the location of their datasets. These locations contain a full URI to the dataset. Migrating from one cluster to another requires us to make adjustments to these locations.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-356146741#-a1512p_4" alt="datamovement_strategy.png" title="datamovement_strategy.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/datamovement_strategy.png" width="1150" height="920"><figcaption class="center-text">datamovement_strategy.png</figcaption></figure></div><p id="-356146741#-a1512p_5">The most simple translation will change the namespace of the URI so everything is RELATIVE. This helps reduce the impact of other tools that might be using these datasets outside the Hive Metastore definitions.</p><p id="-356146741#-a1512p_6">Using the <span class="inline-code" id="-356146741#-a1512p_17">RELATIVE</span> 'Location Translation Strategy' is suggested for side-car cluster migrations where you want to keep everything the same as much as possible.</p><p id="-356146741#-a1512p_7">When you are reorganizing, consolidating, or changing storage environments then the <span class="inline-code" id="-356146741#-a1512p_18">ALIGNED</span> 'Location Translation Strategy' will aid in that process. We suggest building out Warehouse Plans (<a href="#1685164704">Warehouse Plans</a>) for each database for maximum control of that movement.</p><p id="-356146741#-a1512p_8">Attributes of location transformations:</p><ul class="list" id="-356146741#-a1512p_9" start="1"><li class="list-item" id="-356146741#-a1512p_20"><p>Global Warehouse Directories</p></li><li class="list-item" id="-356146741#-a1512p_21"><p>Environment Warehouse Directories</p></li></ul><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-356146741#-a1512p_22">These are pulling automatically from the Hive Environment when available.</p></div>
</blockquote>
<ul class="list" id="-356146741#-a1512p_11" start="1"><li class="list-item" id="-356146741#-a1512p_23"><p>Target Namespace</p></li></ul><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-356146741#-a1512p_24">This is defined in the configuration through the <span class="inline-code" id="-356146741#-a1512p_25">transfer.targetNamespace</span> configuration attribute or the <span class="inline-code" id="-356146741#-a1512p_26">target-namespace</span> configuration setting. This is used for migrations between two clusters and for STORAGE_MIGRATION's within the cluster.</p></div>
</blockquote>
<ul class="list" id="-356146741#-a1512p_13" start="1"><li class="list-item" id="-356146741#-a1512p_27"><p>Warehouse Plans</p></li></ul><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="-356146741#-a1512p_28">Defined for each 'database'. And when defined we should assume that we expect locations of the database, tables, and partitions will be ALIGNED with that location.</p></div>
</blockquote>
<section><h2 id="-356146741#order-of-evaluation" data-toc="order-of-evaluation#Location-Alignment.md-order-of-evaluation">Order of Evaluation</h2><p id="-356146741#-a1512p_29">Order of Evaluation means that we will evaluate the attribute in the describe order and once a valid mapping is found, we will stop the evaluation. Evaluation order depends on the translation type as well.</p><div id="-356146741#-a1512p_30"><div class="detached" id="-356146741#aligned-tab"><div class="tab-title"><div>ALIGNED</div></div><div class="bordered-element tab-content"><ul class="list" id="-356146741#-a1512p_33" start="1"><li class="list-item" id="-356146741#-a1512p_34"><p>Target Namespace</p></li><li class="list-item" id="-356146741#-a1512p_35"><p>Warehouse Plans (when defined)</p></li><li class="list-item" id="-356146741#-a1512p_36"><p>Global Warehouse Directories</p></li><li class="list-item" id="-356146741#-a1512p_37"><p>Environment Warehouse Directories (under certain conditions)</p></li></ul></div></div><div class="detached" id="-356146741#relative-tab"><div class="tab-title"><div>RELATIVE</div></div><div class="bordered-element tab-content"><ul class="list" id="-356146741#-a1512p_38" start="1"><li class="list-item" id="-356146741#-a1512p_39"><p>Target Namespace</p></li></ul></div></div></div></section><section><h2 id="-356146741#translation-types" data-toc="translation-types#Location-Alignment.md-translation-types">Translation Types</h2><p id="-356146741#-a1512p_40">Translation types are used to determine how the location should be transformed. The following are the translation types are <span class="inline-code" id="-356146741#-a1512p_45">ALIGNED</span> and <span class="inline-code" id="-356146741#-a1512p_46">RELATIVE</span>.</p><p id="-356146741#-a1512p_41"><span class="control" id="-356146741#-a1512p_47">Legend</span></p><div class="table-wrapper detached"><table id="-356146741#-a1512p_42"><tr class="header-row" id="-356146741#-a1512p_48"><th id="-356146741#-a1512p_60"><p>Icon</p></th><th id="-356146741#-a1512p_61"><p>Description</p></th></tr><tr class="" id="-356146741#-a1512p_49"><td id="-356146741#-a1512p_62"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_64" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_63"><p>Valid</p></td></tr><tr class="" id="-356146741#-a1512p_50"><td id="-356146741#-a1512p_65"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_67" alt="Close round" title="Close round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/closeRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_66"><p>Invalid</p></td></tr><tr class="" id="-356146741#-a1512p_51"><td id="-356146741#-a1512p_68"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_70" alt="Ignored" title="Ignored" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/ignored.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_69"><p>Ignored</p></td></tr><tr class="" id="-356146741#-a1512p_52"><td id="-356146741#-a1512p_71"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_73" alt="Sql icon" title="Sql icon" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/sql-icon.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_72"><p>SQL</p></td></tr><tr class="" id="-356146741#-a1512p_53"><td id="-356146741#-a1512p_74"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_76" alt="Files o copy" title="Files o copy" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/files-o-copy.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_75"><p>Distcp</p></td></tr><tr class="" id="-356146741#-a1512p_54"><td id="-356146741#-a1512p_77"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_79" alt="Person" title="Person" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/person.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_78"><p>Manual</p></td></tr><tr class="" id="-356146741#-a1512p_55"><td id="-356146741#-a1512p_80"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_82" alt="Typcn arrow sync" title="Typcn arrow sync" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-arrow-sync.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_81"><p>Automatic</p></td></tr><tr class="" id="-356146741#-a1512p_56"><td id="-356146741#-a1512p_83"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_85" alt="Optional" title="Optional" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/optional.jpg" width="20" height="20"></p></td><td id="-356146741#-a1512p_84"><p>Optional</p></td></tr><tr class="" id="-356146741#-a1512p_57"><td id="-356146741#-a1512p_86"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_88" alt="Linecons database" title="Linecons database" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linecons-database.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_87"><p>Metastore Direct</p></td></tr><tr class="" id="-356146741#-a1512p_58"><td id="-356146741#-a1512p_89"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_91" alt="Typcn world" title="Typcn world" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-world.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_90"><p>Global Warehouse</p></td></tr><tr class="" id="-356146741#-a1512p_59"><td id="-356146741#-a1512p_92"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_94" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_93"><p>Warehouse Plan</p></td></tr></table></div><p id="-356146741#-a1512p_43"><span class="control" id="-356146741#-a1512p_95">Translation Scenarios</span></p><div class="table-wrapper detached"><table id="-356146741#-a1512p_44"><tr class="header-row" id="-356146741#-a1512p_96"><th id="-356146741#-a1512p_113"></th><th id="-356146741#-a1512p_114" colspan="2"><p>Translation Type</p></th><th id="-356146741#-a1512p_115"><p>Data</p><p> Movement</p></th><th id="-356146741#-a1512p_116"><p>Required</p></th><th id="-356146741#-a1512p_117"><p>Notes</p></th></tr><tr class="" id="-356146741#-a1512p_97"><th id="-356146741#-a1512p_119"><p>DUMP</p></th><td id="-356146741#-a1512p_120"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_125" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_121"><p id="-356146741#-a1512p_126">RELATIVE</p></td><td id="-356146741#-a1512p_122"></td><td id="-356146741#-a1512p_123"></td><td id="-356146741#-a1512p_124"></td></tr><tr class="" id="-356146741#-a1512p_98"><th id="-356146741#-a1512p_127"></th><td id="-356146741#-a1512p_128"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_133" alt="Close round" title="Close round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/closeRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_129"><p id="-356146741#-a1512p_134">ALIGNED</p></td><td id="-356146741#-a1512p_130"></td><td id="-356146741#-a1512p_131"></td><td id="-356146741#-a1512p_132"></td></tr><tr class="" id="-356146741#-a1512p_99"><th id="-356146741#-a1512p_135"><p>SCHEMA_ONLY</p></th><td id="-356146741#-a1512p_136"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_141" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_137"><p id="-356146741#-a1512p_142">RELATIVE</p></td><td id="-356146741#-a1512p_138"><p id="-356146741#-a1512p_143"><img class="inline-icon image image-container" id="-356146741#-a1512p_144" alt="Files o copy" title="Files o copy" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/files-o-copy.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_145" alt="Person" title="Person" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/person.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_139"></td><td id="-356146741#-a1512p_140"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_146" alt="Typcn warning" title="Typcn warning" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-warning.png" width="30" height="30"> Using distcp <img class="inline-icon image image-container" id="-356146741#-a1512p_147" alt="Files o copy" title="Files o copy" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/files-o-copy.png" width="20" height="20"> here when either table or partition locations aren't standard, will result in data loss because we're not inspecting all the locations through the Metastore Direct connection. It's recommended to use `ALIGNED` with `distcp` to build an accurate `distcp` plan.</p></td></tr><tr class="" id="-356146741#-a1512p_100"><th id="-356146741#-a1512p_148"></th><td id="-356146741#-a1512p_149"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_154" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_150"><p id="-356146741#-a1512p_155">ALIGNED</p></td><td id="-356146741#-a1512p_151"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_156" alt="Files o copy" title="Files o copy" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/files-o-copy.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_152"><p id="-356146741#-a1512p_157"><img class="inline-icon image image-container" id="-356146741#-a1512p_158" alt="Linecons database" title="Linecons database" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linecons-database.png" width="20" height="20"> when Warehouse Plan(s) used <img class="inline-icon image image-container" id="-356146741#-a1512p_160" alt="Typcn world" title="Typcn world" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-world.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_162" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"> (Optional)</p></td><td id="-356146741#-a1512p_153"></td></tr><tr class="" id="-356146741#-a1512p_101"><th id="-356146741#-a1512p_163"><p>SQL</p></th><td id="-356146741#-a1512p_164"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_169" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_165"><p id="-356146741#-a1512p_170">RELATIVE</p></td><td id="-356146741#-a1512p_166"></td><td id="-356146741#-a1512p_167"></td><td id="-356146741#-a1512p_168"></td></tr><tr class="" id="-356146741#-a1512p_102"><th id="-356146741#-a1512p_171"></th><td id="-356146741#-a1512p_172"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_177" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_173"><p id="-356146741#-a1512p_178">ALIGNED</p></td><td id="-356146741#-a1512p_174"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_179" alt="Sql icon" title="Sql icon" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/sql-icon.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_175"><p id="-356146741#-a1512p_180"><img class="inline-icon image image-container" id="-356146741#-a1512p_181" alt="Typcn world" title="Typcn world" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-world.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_183" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"> (Optional)</p></td><td id="-356146741#-a1512p_176"></td></tr><tr class="" id="-356146741#-a1512p_103"><th id="-356146741#-a1512p_184"><p>EXPORT_IMPORT</p></th><td id="-356146741#-a1512p_185"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_190" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_186"><p id="-356146741#-a1512p_191">RELATIVE</p></td><td id="-356146741#-a1512p_187"></td><td id="-356146741#-a1512p_188"></td><td id="-356146741#-a1512p_189"></td></tr><tr class="" id="-356146741#-a1512p_104"><th id="-356146741#-a1512p_192"></th><td id="-356146741#-a1512p_193"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_198" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_194"><p id="-356146741#-a1512p_199">ALIGNED</p></td><td id="-356146741#-a1512p_195"></td><td id="-356146741#-a1512p_196"><p id="-356146741#-a1512p_200"><img class="inline-icon image image-container" id="-356146741#-a1512p_201" alt="Typcn world" title="Typcn world" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-world.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_203" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"> (Optional)</p></td><td id="-356146741#-a1512p_197"></td></tr><tr class="" id="-356146741#-a1512p_105"><th id="-356146741#-a1512p_204"><p>HYBRID</p></th><td id="-356146741#-a1512p_205"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_210" alt="Close round" title="Close round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/closeRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_206"><p id="-356146741#-a1512p_211">RELATIVE</p></td><td id="-356146741#-a1512p_207"></td><td id="-356146741#-a1512p_208"></td><td id="-356146741#-a1512p_209"></td></tr><tr class="" id="-356146741#-a1512p_106"><th id="-356146741#-a1512p_212"></th><td id="-356146741#-a1512p_213"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_218" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_214"><p id="-356146741#-a1512p_219">ALIGNED</p></td><td id="-356146741#-a1512p_215"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_220" alt="Sql icon" title="Sql icon" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/sql-icon.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_216"><p id="-356146741#-a1512p_221"><img class="inline-icon image image-container" id="-356146741#-a1512p_222" alt="Typcn world" title="Typcn world" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/typcn-world.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_224" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"> (Optional)</p></td><td id="-356146741#-a1512p_217"></td></tr><tr class="" id="-356146741#-a1512p_107"><th id="-356146741#-a1512p_225"><p>STORAGE_MIGRATION</p></th><td id="-356146741#-a1512p_226"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_231" alt="Close round" title="Close round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/closeRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_227"><p id="-356146741#-a1512p_232">RELATIVE</p></td><td id="-356146741#-a1512p_228"></td><td id="-356146741#-a1512p_229"></td><td id="-356146741#-a1512p_230"></td></tr><tr class="" id="-356146741#-a1512p_108"><th id="-356146741#-a1512p_233"></th><td id="-356146741#-a1512p_234"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_240" alt="Checkmark round" title="Checkmark round" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/checkmarkRound.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_235"><p id="-356146741#-a1512p_241">ALIGNED</p></td><td id="-356146741#-a1512p_236"><p id="-356146741#-a1512p_242"><img class="inline-icon image image-container" id="-356146741#-a1512p_243" alt="Sql icon" title="Sql icon" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/sql-icon.png" width="20" height="20"> <img class="inline-icon image image-container" id="-356146741#-a1512p_244" alt="Files o copy" title="Files o copy" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/files-o-copy.png" width="20" height="20"></p></td><td id="-356146741#-a1512p_237"><p id="-356146741#-a1512p_245"><img class="inline-icon image image-container" id="-356146741#-a1512p_246" alt="Linecons database" title="Linecons database" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linecons-database.png" width="20" height="20"> when Warehouse Plan(s) used <img class="inline-icon image image-container" id="-356146741#-a1512p_248" alt="Linea basic elaboration document next" title="Linea basic elaboration document next" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/linea--basic-elaboration-document-next.png" width="20" height="20"> (Optional)</p></td><td id="-356146741#-a1512p_238"></td><td id="-356146741#-a1512p_239"></td></tr><tr class="" id="-356146741#-a1512p_109"><th id="-356146741#-a1512p_249"><p>LINKED</p></th><td id="-356146741#-a1512p_250"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_255" alt="Ignored" title="Ignored" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/ignored.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_251"><p id="-356146741#-a1512p_256">RELATIVE</p></td><td id="-356146741#-a1512p_252"></td><td id="-356146741#-a1512p_253"></td><td id="-356146741#-a1512p_254"></td></tr><tr class="" id="-356146741#-a1512p_110"><th id="-356146741#-a1512p_257"></th><td id="-356146741#-a1512p_258"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_263" alt="Ignored" title="Ignored" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/ignored.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_259"><p id="-356146741#-a1512p_264">ALIGNED</p></td><td id="-356146741#-a1512p_260"></td><td id="-356146741#-a1512p_261"></td><td id="-356146741#-a1512p_262"></td></tr><tr class="" id="-356146741#-a1512p_111"><th id="-356146741#-a1512p_265"><p>COMMON</p></th><td id="-356146741#-a1512p_266"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_271" alt="Ignored" title="Ignored" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/ignored.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_267"><p id="-356146741#-a1512p_272">RELATIVE</p></td><td id="-356146741#-a1512p_268"></td><td id="-356146741#-a1512p_269"></td><td id="-356146741#-a1512p_270"></td></tr><tr class="" id="-356146741#-a1512p_112"><th id="-356146741#-a1512p_273"></th><td id="-356146741#-a1512p_274"><p><img class="inline-icon image image-container" id="-356146741#-a1512p_279" alt="Ignored" title="Ignored" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/ignored.png" width="30" height="30"></p></td><td id="-356146741#-a1512p_275"><p id="-356146741#-a1512p_280">ALIGNED</p></td><td id="-356146741#-a1512p_276"></td><td id="-356146741#-a1512p_277"></td><td id="-356146741#-a1512p_278"></td></tr></table></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1642806840">Databases</h1><p id="1642806840#-7nxyko_3">The whole goal of <span class="inline-code" id="1642806840#-7nxyko_9">hms-mirror</span> is to move metadata from one cluster to another. Picking which databases to move is a primary function of the application.</p><p id="1642806840#-7nxyko_4">There are several ways to select databases for migration. Each with its own benefits.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p id="1642806840#-7nxyko_10">Once a method is selected to add database(s), the other add options will be limited until the option is cleared.</p></div>
</blockquote>
<section><h2 id="1642806840#add-database-names-to-the-runtime-configuration" data-toc="add-database-names-to-the-runtime-configuration#Databases.md-add-database-names-to-the-runtime-configuration">Add 'database' names to the runtime configuration.</h2><p id="1642806840#-7nxyko_11">This is the simplest way to select databases. Additional filtered can be applied to tables through the table <span class="control" id="1642806840#-7nxyko_13">RegEx</span> filters.</p><div id="1642806840#-7nxyko_12"><div class="detached" id="1642806840#-7nxyko_14"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><p>Ensure you've selected 'Edit' from the Left Navigation Menu. Enter a comma separated list of databases and press 'Add'. </p><div class="container"><figure class="image-container"><img class="center image image-size" id="1642806840#-7nxyko_16" alt="dbs_by_comma.png" title="dbs_by_comma.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/dbs_by_comma.png" width="1612" height="1054"><figcaption class="center-text">dbs_by_comma.png</figcaption></figure></div></div></div><div class="detached" id="1642806840#-7nxyko_15"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p>The `-db|--database` option allows you to list the databases you want to process.</p></div></div></div></section><section><h2 id="1642806840#use-the-database-regex-filter-to-include-matching-databases-found-in-the-source-cluster" data-toc="use-the-database-regex-filter-to-include-matching-databases-found-in-the-source-cluster#Databases.md-use-the-database-regex-filter-to-include-matching-databases-found-in-the-source-cluster">Use the Database RegEx filter to include matching databases found in the source cluster.</h2><div id="1642806840#-7nxyko_18"><div class="detached" id="1642806840#-7nxyko_20"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><p>Ensure you've selected 'Edit' from the Left Navigation Menu. Enter a RegEx pattern to match the databases you want to include and press 'Add'.</p></div></div><div class="detached" id="1642806840#-7nxyko_21"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p>The `-dbRegEx|--database-regex` option allows you to filter matching databases.</p></div></div></div></section><section><h2 id="1642806840#create-warehouse-plans-for-the-databases-you-want-to-include" data-toc="create-warehouse-plans-for-the-databases-you-want-to-include#Databases.md-create-warehouse-plans-for-the-databases-you-want-to-include">Create Warehouse Plans for the databases you want to include.</h2><div id="1642806840#-7nxyko_22"><div class="detached" id="1642806840#-7nxyko_23"><div class="tab-title"><div>Web UI</div></div><div class="bordered-element tab-content"><p>Ensure you've selected 'Edit' from the Left Navigation Menu. Select the 'Warehouse Plans' tab. Create a new Warehouse Plan and add the databases you want to include.</p></div></div><div class="detached" id="1642806840#-7nxyko_24"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p>This feature isn't available through CLI commandline Options. It is possible to use the WebUI to create the configuration and then use the 'persisted' version of that configuration as the configuration for the CLI via `-cfg`.</p></div></div></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1685164704">Warehouse Plans</h1><p id="1685164704#-b5r9io_3">Warehouse Plans are the way you can control how 'each' Database is translated between clusters or storage environments. Warehouse plans allow you to control where the data will be translated to.</p><p id="1685164704#-b5r9io_4">There are three types of 'Warehouse Plans'.</p><ul class="list" id="1685164704#-b5r9io_5" start="1"><li class="list-item" id="1685164704#-b5r9io_10"><p>Global</p></li><li class="list-item" id="1685164704#-b5r9io_11"><p>Per Database</p></li><li class="list-item" id="1685164704#-b5r9io_12"><p>Environment</p></li></ul><p id="1685164704#-b5r9io_6">When you choose to 'ALIGN' your datasets during the migration (See Location Alignment (<a href="#-356146741">Location Alignment</a>) for settings that are applicable for each strategy), we'll evaluate the warehouse plans to determine where each dataset should land. If you're using 'SQL' to move data, we'll build the schema's and sql that will make the adjustments required to reorganize the data based on the warehouse plan that is found.</p><p id="1685164704#-b5r9io_7">It's recommended to define a warehouse plan for each <span class="control" id="1685164704#-b5r9io_14">database</span> you want to move when you're using the 'ALIGNED' data movement strategy. When this is defined for the database, we'll inspect the all the current locations of tables and partitions in that dataset and make the necessary adjustments to locations.</p><p id="1685164704#-b5r9io_8">The 'Warehouse Plans' get converted into 'Global Location Maps' that are inspected during processing to make that conversion.</p><section><h2 id="1685164704#standard-locations" data-toc="standard-locations#Warehouse-Plans.md-standard-locations">Standard Locations</h2><p id="1685164704#-b5r9io_15">When you choose to <span class="control" id="1685164704#-b5r9io_17">ALIGN</span> the datasets, you are choosing to collect everything in the dataset/database under the same location as you've defined in the <span class="control" id="1685164704#-b5r9io_18">warehouse plan</span>. When you choose <span class="inline-code" id="1685164704#-b5r9io_19">DISTCP</span> as the movement plan for <span class="inline-code" id="1685164704#-b5r9io_20">ALIGNED</span>, we'll build a distcp plan that will make those translations. <span class="control" id="1685164704#-b5r9io_21">BUT</span>, there are some restrictions to this.</p><p id="1685164704#-b5r9io_16">When you choose to use non-standard locations for 'partition specs', we can't build a proper <span class="inline-code" id="1685164704#-b5r9io_22">distcp</span> plan. In this case we will throw an 'error' for the offending table and describe to imbalance. You can either fix/adjust the dataset OR choose to use the <span class="inline-code" id="1685164704#-b5r9io_23">SQL</span> data movement strategy.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1477024918">Global Warehouse Plans</h1><div class="container"><figure class="image-container"><img class="center image image-size" id="1477024918#z18i3ac_3" alt="global_warehouse.png" title="global_warehouse.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/global_warehouse.png" width="1279" height="386"><figcaption class="center-text">global_warehouse.png</figcaption></figure></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1930923410">Database Warehouse Plans</h1><p id="-1930923410#-a648ac_3"><div class="container"><figure class="image-container"><img class="center image image-size" id="-1930923410#-a648ac_4" alt="filter_wp-add.png" title="filter_wp-add.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/filter_wp-add.png" width="1033" height="380"><figcaption class="center-text">filter_wp-add.png</figcaption></figure></div><div class="container"><figure class="image-container"><img class="center image image-size" id="-1930923410#-a648ac_5" alt="wp-add-detail.png" title="wp-add-detail.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/wp-add-detail.png" width="634" height="276"><figcaption class="center-text">wp-add-detail.png</figcaption></figure></div></p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1414560951">Environment Warehouse</h1><p id="-1414560951#-ezydfd_3">When a job is started, we'll gather details about the environment from Hive via <span class="inline-code" id="-1414560951#-ezydfd_5">set;</span>. Hive defines default global warehouse locations for 'external' and 'managed' tables for databases. If a database doesn't define a <span class="inline-code" id="-1414560951#-ezydfd_6">LOCATION</span> and/or <span class="inline-code" id="-1414560951#-ezydfd_7">MANAGEDLOCATION</span>, these entries will be used to define to locations of a table/partition when it's created.</p><p id="-1414560951#-ezydfd_4">In a multi-tenant environment where different storage locations and/or namespaces are used, these global default settings are inadequate. In these environments, you should be declaring the locations at the database level to ensure all new datasets end up in the desired locations and you can maintain a true multi-tenant environment.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-17525952">Hive Conversions</h1><p id="-17525952#-jbb8cm_3">Hive has gone through a lot of changes as it's evolved over the last several years. Especially between Hive 1/2 and Hive 3. The default syntax used to 'create' tables hasn't changed, but the resulting table structure may have.</p><p id="-17525952#-jbb8cm_4">Understand those changes and what hive flags are available to help influence that structure aren't very clear to even the most seasoned Hive user.</p><p id="-17525952#-jbb8cm_5"><span class="inline-code" id="-17525952#-jbb8cm_11">hms-mirror</span> uses settings in the 'cluster' configuration to influence <span class="control" id="-17525952#-jbb8cm_12">how</span> tables are translated during the migration.</p><div id="-17525952#-jbb8cm_6"><div class="detached" id="-17525952#webui"><div class="tab-title"><div>Web Interface</div></div><div class="bordered-element tab-content"><p>Set the properties in the appropriate cluster configuration(s) for your strategy. </p><div class="container"><figure class="image-container"><img class="center image image-size" id="-17525952#-jbb8cm_15" alt="cluster_tabs.png" title="cluster_tabs.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/cluster_tabs.png" width="2068" height="254"><figcaption class="center-text">cluster_tabs.png</figcaption></figure></div><p id="-17525952#-jbb8cm_16">Set the flags that match you cluster's Hive version.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-17525952#-jbb8cm_17" alt="hive_version_flags.png" title="hive_version_flags.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/hive_version_flags.png" width="686" height="746"><figcaption class="center-text">hive_version_flags.png</figcaption></figure></div></div></div><div class="detached" id="-17525952#cli"><div class="tab-title"><div>CLI</div></div><div class="bordered-element tab-content"><p>Modify the `hms-mirror` configuration to include the following settings: </p><div class="detached code-block" id="-17525952#-jbb8cm_18"><pre><code class="language-yaml">clusters:
  LEFT|RIGHT:
    platformType: HDP2|HDP3|CHD5|CDH6|CDP7.1|CDP7_2|..</code></pre></div></div></div></div><p id="-17525952#-jbb8cm_7">If you were upgrading in-place a Hive 1/2 cluster to Hive 3, the Hive upgrade process would convert 'legacy' managed tables to 'EXTERNAL' tables and add a 'PURGE' flag in the table properties so that tables behavior remains consistent with the original Hive 1/2 behavior while also being a compatible Hive 3 table.</p><p id="-17525952#-jbb8cm_8">Managed non-ACID tables are NOT a valid state in Hive 3.</p><p id="-17525952#-jbb8cm_9">With the properties set in the configuration for the cluster (above), <span class="inline-code" id="-17525952#-jbb8cm_19">hms-mirror</span> will make these conversions for you, in the same way that the Hive upgrade process would.</p><section><h2 id="-17525952#hive-1-2-table-types-history" data-toc="hive-1-2-table-types-history#Hive-Conversions.md-hive-1-2-table-types-history">Hive 1/2 Table Types History</h2><p id="-17525952#-jbb8cm_20">Tables in Hive 1/2 are typically 'external' tables. This means that the data is managed independently of the Hive Metastore. The table definition in the Hive Metastore points to the location of the data but does NOT manage the data. In this case, if you drop the table (or a partition), the data remains on the filesystem. These are created with the <span class="inline-code" id="-17525952#-jbb8cm_27">CREATE EXTERNAL TABLE</span> command.</p><p id="-17525952#-jbb8cm_21">Another table type in Hive 1/2 is the 'managed' table. This is where Hive manages the data. If you drop the table (or partition), Hive will clean up the data on the filesystem. These are created with the <span class="inline-code" id="-17525952#-jbb8cm_28">CREATE TABLE</span> command.</p><p id="-17525952#-jbb8cm_22">There is also a variant of the 'managed' table that is ACID compliant. These are created with the <span class="inline-code" id="-17525952#-jbb8cm_29">CREATE TABLE</span> command with table properties that enable ACID characteristics <span class="inline-code" id="-17525952#-jbb8cm_30">transactional=true</span>. These tables have special characteristics and are not friendly to the 'schema on read' paradigm, since these tables are 'schema on write' and embed special columns in the data files.</p><p id="-17525952#-jbb8cm_23">Starting with Hive 3, the default behavior of the <span class="inline-code" id="-17525952#-jbb8cm_31">CREATE TABLE</span> command is to create a 'managed' ACID table. The additional table properties for a 'transactional' table are no longer required to create an ACID table. This is a significant change from Hive 1/2.</p><p id="-17525952#-jbb8cm_24">There is a web application that allows you to experiment with various commands and settings, showing the resulting table structures.</p><p id="-17525952#-jbb8cm_25">Regardless, use the following web application to help you quickly through the trial and error phase of understanding these new structural and session settings.</p><p id="-17525952#-jbb8cm_26">Hive Create Path (<a href="https://dstreev.github.io/hive/create_path.html">https://dstreev.github.io/hive/create_path.html</a>)</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="45251276">Features</h1><p id="45251276#-t757a4_3"><span class="inline-code" id="45251276#-t757a4_33">hms-mirror</span> is designed to migrate schema definitions from one cluster to another or simply provide an extract of the schemas via <span class="inline-code" id="45251276#-t757a4_34">-d DUMP</span>.</p><p id="45251276#-t757a4_4">Under certain conditions, <span class="inline-code" id="45251276#-t757a4_35">hms-mirror</span> will 'move' data too. Using the data strategies <span class="inline-code" id="45251276#-t757a4_36">-d SQL|EXPORT_IMPORT|HYBRID</span> well use a combination of SQL temporary tables and Linking Clusters Storage Layers (<a href="#1958103884">Linking Cluster Storage Layers</a>) to facilitate this.</p><section><h2 id="45251276#iceberg-table-migration-via-hive" data-toc="iceberg-table-migration-via-hive#hms-mirror-features.md-iceberg-table-migration-via-hive">Iceberg Table Migration via Hive</h2><p id="45251276#-t757a4_38">See Iceberg Migration (<a href="#1035902883">ICEBERG_MIGRATION</a>) for details.</p></section><section><h2 id="45251276#file-system-stats" data-toc="file-system-stats#hms-mirror-features.md-file-system-stats">File System Stats</h2><p id="45251276#-t757a4_40">SQL based operations, <span class="inline-code" id="45251276#-t757a4_43">hms-mirror</span> will attempt to gather file system stats for the tables being migrated. This is done by running <span class="inline-code" id="45251276#-t757a4_44">hdfs dfs -count</span> on the table location. This is done to help determine the best strategy for moving data and allows us to set certain hive session values and distribution strategies in SQL to optimize the data movement.</p><p id="45251276#-t757a4_41">But some FileSystems may not be very efficient at gathering stats. For example, S3. In these cases, you can disable the stats gathering by adding <span class="inline-code" id="45251276#-t757a4_45">-ssc|--skip-stats-collection</span> to your command line.</p><p id="45251276#-t757a4_42">When you have a LOT of tables, collecting stats can have a significant impact on the time it takes to run <span class="inline-code" id="45251276#-t757a4_46">hms-mirror</span> and the general pressure on the FileSystem to gather this information. In this case, you have to option to disable stats collection through <span class="inline-code" id="45251276#-t757a4_47">-scc</span>.</p></section><section><h2 id="45251276#create-external-table-if-not-exists-option" data-toc="create-external-table-if-not-exists-option#hms-mirror-features.md-create-external-table-if-not-exists-option">CREATE [EXTERNAL] TABLE IF NOT EXISTS Option</h2><p id="45251276#-t757a4_49">Default behavior for <span class="inline-code" id="45251276#-t757a4_53">hms-mirror</span> is to NOT include the <span class="inline-code" id="45251276#-t757a4_54">IF NOT EXISTS</span> clause in the <span class="inline-code" id="45251276#-t757a4_55">CREATE TABLE</span> statements. This is because we want to ensure that the table is created and that the schema is correct. If the table already exists, we want to fail.</p><p id="45251276#-t757a4_50">But there are some scenarios where the table is present and we don't want the process to fail on the CREATE statement to ensure the remaining SQL statements are executed. In this case, you can modify add the commandline option <span class="inline-code" id="45251276#-t757a4_56">-cine</span> or add to the configuration:</p><div class="detached code-block" id="45251276#-t757a4_51"><pre><code class="language-yaml">clusters:
  RIGHT|LEFT:
    createIfNotExists: &quot;true&quot;</code></pre></div><p id="45251276#-t757a4_52">Using this option has the potential to create a table with a different schema than the source. This is not recommended. This option is applied when using the <span class="inline-code" id="45251276#-t757a4_57">SCHEMA_ONLY</span> data strategy.</p></section><section><h2 id="45251276#auto-gathering-stats-disabled-by-default" data-toc="auto-gathering-stats-disabled-by-default#hms-mirror-features.md-auto-gathering-stats-disabled-by-default">Auto Gathering Stats (disabled by default)</h2><p id="45251276#-t757a4_58">CDP Default settings have enabled <span class="inline-code" id="45251276#-t757a4_62">hive.stats.autogather</span> and <span class="inline-code" id="45251276#-t757a4_63">hive.stats.column.autogather</span>. This impacts the speed of INSERT statements (used by <span class="inline-code" id="45251276#-t757a4_64">hms-mirror</span> to migrate data) and for large/numerous tables, the impact can be significant.</p><p id="45251276#-t757a4_59">The default configuration for <span class="inline-code" id="45251276#-t757a4_65">hms-mirror</span> is to disable these settings. You can re-enable this in <span class="inline-code" id="45251276#-t757a4_66">hms-mirror</span> for each side (LEFT|RIGHT) separately. Add the following to your configuration.</p><div class="detached code-block" id="45251276#-t757a4_60"><pre><code class="language-yaml">clusters:
  LEFT|RIGHT:
    enableAutoTableStats: true
    enableAutoColumnStats: true</code></pre></div><p id="45251276#-t757a4_61">To disable, remove the <span class="inline-code" id="45251276#-t757a4_67">enableAutoTableStats</span> and <span class="inline-code" id="45251276#-t757a4_68">enableAutoColumnStats</span> entries or set them to <span class="inline-code" id="45251276#-t757a4_69">false</span>.</p></section><section><h2 id="45251276#non-standard-partition-locations" data-toc="non-standard-partition-locations#hms-mirror-features.md-non-standard-partition-locations">Non-Standard Partition Locations</h2><p id="45251276#-t757a4_70">Partitions created by 'hive' with default locations follow a file system naming convention that allows other partitions of 'hive' to discovery/manage those location and partition associations.</p><p id="45251276#-t757a4_71">The standard is for partitions to exist as sub-directories of the table location. For example: Table Location is <span class="inline-code" id="45251276#-t757a4_78">hdfs://my-cluster/warehouse/tablespace/external/hive/my_test.db/my_table</span> and the partition location is <span class="inline-code" id="45251276#-t757a4_79">hdfs://my-cluster/warehouse/tablespace/external/hive/my_test.db/my_table/dt=2020-01-01</span>, assuming the partition column name is <span class="inline-code" id="45251276#-t757a4_80">dt</span>.</p><p id="45251276#-t757a4_72">When this convention is not followed, additional steps are required to build the partition metadata. You can't use <span class="inline-code" id="45251276#-t757a4_81">MSCK REPAIR</span> because it will not find the partitions. You can use <span class="inline-code" id="45251276#-t757a4_82">ALTER TABLE ADD PARTITION</span> but you'll need to provide the location of the partition. <span class="inline-code" id="45251276#-t757a4_83">hms-mirror</span> will do this for you when using the data strategies <span class="inline-code" id="45251276#-t757a4_84">-d DUMP|SCHEMA_ONLY</span> and the commandline flag <span class="inline-code" id="45251276#-t757a4_85">-epl|--evaluate-partition-location</span>.</p><p id="45251276#-t757a4_73">In order to make this evaluation efficient, we do NOT use standard HiveSQL to discover the partition details. It is possible to use HiveSQL for this, it's just not meant for operations at scale or tables with a lot of partitions.</p><p id="45251276#-t757a4_74">Hence, we tap directly into the hive metastore database. In order to use this feature, you will need to add the following configuration definition to your hms-mirror configuration file (default.yaml).</p><div class="detached code-block" id="45251276#-t757a4_75"><pre><code class="language-yaml">clusters:
  LEFT|RIGHT:
    ...
    metastore_direct:
      uri: &quot;&lt;db_url&gt;&quot;
      type: MYSQL|POSTGRES|ORACLE
      connectionProperties:
        user: &quot;&lt;db_user&gt;&quot;
        password: &quot;&lt;db_password&gt;&quot;
      connectionPool:
        min: 3
        max: 5</code></pre></div><p id="45251276#-t757a4_76">You will also need to place a copy of the RDBMS JDBC driver in <span class="inline-code" id="45251276#-t757a4_86">$HOME/.hms-mirror/aux_libs</span>. The driver must match the <span class="inline-code" id="45251276#-t757a4_87">type</span> defined in the configuration file.</p><p id="45251276#-t757a4_77"><span class="control" id="45251276#-t757a4_88">Note: Non-Standard Partition Location will affect other strategies like <span class="inline-code" id="45251276#-t757a4_89">SQL</span> where the LEFT clusters storage is accessible to the RIGHT and is used by the RIGHT to source data. The 'mirror' table used for the transfer will NOT discover the partitions and will NOT transfer data. See: Issue #63 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/63">https://github.com/cloudera-labs/hms-mirror/issues/63</a>) for updates on addressing this scenario. If this is affecting you, I highly recommend you comment on the issue to help us set priorities.</span></p></section><section><h2 id="45251276#optimizations" data-toc="optimizations#hms-mirror-features.md-optimizations">Optimizations</h2><p id="45251276#-t757a4_91">The following configuration settings control the various optimizations taken by <span class="inline-code" id="45251276#-t757a4_96">hms-mirror</span>. These settings are mutually exclusive.</p><ul class="list" id="45251276#-t757a4_92" start="1"><li class="list-item" id="45251276#-t757a4_97"><p><span class="inline-code" id="45251276#-t757a4_100">-at|--auto-tune</span></p></li><li class="list-item" id="45251276#-t757a4_98"><p><span class="inline-code" id="45251276#-t757a4_101">-so|--skip-optimizations</span></p></li><li class="list-item" id="45251276#-t757a4_99"><p><span class="inline-code" id="45251276#-t757a4_102">-sdpi|--sort-dynamic-partition-inserts</span></p></li></ul><section><h3 id="45251276#auto-tune" data-toc="auto-tune#hms-mirror-features.md-auto-tune">Auto-Tune</h3><p id="45251276#-t757a4_103"><span class="inline-code" id="45251276#-t757a4_108">-at|--auto-tune</span></p><p id="45251276#-t757a4_104">Auto-tuning will use some basic file level statistics about tables/partitions to provide overrides for the following settings:</p><ul class="list" id="45251276#-t757a4_105" start="1"><li class="list-item" id="45251276#-t757a4_109"><p><span class="inline-code" id="45251276#-t757a4_112">tez.grouping.max-size</span></p></li><li class="list-item" id="45251276#-t757a4_110"><p><span class="inline-code" id="45251276#-t757a4_113">hive.exec.max.dynamic.partitions</span></p></li><li class="list-item" id="45251276#-t757a4_111"><p><span class="inline-code" id="45251276#-t757a4_114">hive.exec.reducers.max</span></p></li></ul><p id="45251276#-t757a4_106">in addition to these session level setting, we'll use those basic file statistics to construct migration scripts that address things like 'small-files' and 'large' partition datasets.</p><p id="45251276#-t757a4_107">We'll set <span class="inline-code" id="45251276#-t757a4_115">hive.optimize.sort.dynamic.partition.threshold=-1</span> and append <span class="inline-code" id="45251276#-t757a4_116">DISTRIBUTE BY</span> to the SQL migration sql statement, just like we do with <span class="inline-code" id="45251276#-t757a4_117">-sdpi</span>. But we'll go one step further and review the average partition size and add an additional 'grouping' element to the SQL to ensure we get efficient writers to a partition. The means that tables with large partition datasets will have more than the standard single writer per partition, preventing the LONG running hanging task that is trying to write a very large partition.</p></section><section><h3 id="45251276#sort-dynamic-partition-inserts" data-toc="sort-dynamic-partition-inserts#hms-mirror-features.md-sort-dynamic-partition-inserts">Sort Dynamic Partition Inserts</h3><p id="45251276#-t757a4_118"><span class="inline-code" id="45251276#-t757a4_121">-sdpi|--sort-dynamic-partition-inserts</span></p><p id="45251276#-t757a4_119">This will set the session property <span class="inline-code" id="45251276#-t757a4_122">hive.optimize.sort.dynamic.partition.threshold=0</span>, which will enable plans to distribute multi partition inserts by the partition key, therefore reducing partitions writes to a single 'writer/reducer'.</p><p id="45251276#-t757a4_120">When this isn't set, we set <span class="inline-code" id="45251276#-t757a4_123">hive.optimize.sort.dynamic.partition.threshold=-1</span>, and append <span class="inline-code" id="45251276#-t757a4_124">DISTRIBUTE BY</span> to the SQL migration sql statement to ensure the same behavior of grouping reducers by partition values.</p></section><section><h3 id="45251276#skip-optimizations" data-toc="skip-optimizations#hms-mirror-features.md-skip-optimizations">Skip Optimizations</h3><p id="45251276#-t757a4_125"><span class="inline-code" id="45251276#-t757a4_129">-so</span></p><p id="45251276#-t757a4_126">Feature Request #23 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/23">https://github.com/cloudera-labs/hms-mirror/issues/23</a>) was introduced in v1.5.4.2 and give an option to <span class="control" id="45251276#-t757a4_131">Skip Optimizations</span>.</p><p id="45251276#-t757a4_127">When migrating data via SQL with partitioned tables (OR downgrading an ACID table), there are optimizations that we apply to help hive distribute data more efficiently. One method is to use <span class="inline-code" id="45251276#-t757a4_132">hive.optimize.sort.dynamic.partition=true</span> which will &quot;DISTRIBUTE&quot; data along the partitions via a Reduction task. Another is to declare this in SQL with a <span class="inline-code" id="45251276#-t757a4_133">DISTRIBUTE BY</span> clause.</p><p id="45251276#-t757a4_128">But there is a corner case where these optimizations can get in the way and cause long-running tasks. If the source table has already been organized into large files (which would be within the partitions already), adding the optimizations above force a single reducer per partition. If the partitions are large and already have good file sizes, we want to skip these optimizations and let hive run the process with only a map task.</p></section></section><section><h2 id="45251276#hdp3-managedlocation-database-property" data-toc="hdp3-managedlocation-database-property#hms-mirror-features.md-hdp3-managedlocation-database-property">HDP3 MANAGEDLOCATION Database Property</h2><p id="45251276#-t757a4_134">HDP3 doesn't support MAANGEDLOCATION (<a href="https://github.com/cloudera-labs/hms-mirror/issues/52">https://github.com/cloudera-labs/hms-mirror/issues/52</a>) so we've added a property to the cluster configuration to allow the system to <span class="emphasis" id="45251276#-t757a4_137">SKIP</span> setting the <span class="inline-code" id="45251276#-t757a4_138">MANAGEDLOCATION</span> database property in HDP 3 / Hive 3 environments.</p><div class="detached code-block" id="45251276#-t757a4_135"><pre><code class="language-yaml">clusters:
  LEFT:
    platformType: 'HDP3'</code></pre></div></section><section><h2 id="45251276#compress-text-output" data-toc="compress-text-output#hms-mirror-features.md-compress-text-output">Compress Text Output</h2><p id="45251276#-t757a4_139"><span class="inline-code" id="45251276#-t757a4_140">-cto</span> will control the session level setting for `hive.exec.compress.output'.</p></section><section><h2 id="45251276#views" data-toc="views#hms-mirror-features.md-views">VIEWS</h2><p id="45251276#-t757a4_141"><span class="inline-code" id="45251276#-t757a4_146">hms-mirror</span> now supports the migration of VIEWs between two environments. Use the <span class="inline-code" id="45251276#-t757a4_147">-v|--views-only</span> option to execute this path. VIEW creation requires dependent tables to exist.</p><p id="45251276#-t757a4_142">Run <span class="inline-code" id="45251276#-t757a4_148">hms-mirror</span> to create all the target tables before running it with the <span class="inline-code" id="45251276#-t757a4_149">-v</span> option.</p><p id="45251276#-t757a4_143">This flag is an <span class="inline-code" id="45251276#-t757a4_150">OR</span> for processing VIEW's <span class="inline-code" id="45251276#-t757a4_151">OR</span> TABLE's. They are NOT processed together.</p><p id="45251276#-t757a4_144"><span class="control" id="45251276#-t757a4_152">Requirements</span></p><ul class="list" id="45251276#-t757a4_145" start="1"><li class="list-item" id="45251276#-t757a4_153"><p>The dependent tables must exist in the RIGHT cluster</p></li><li class="list-item" id="45251276#-t757a4_154"><p>When using <span class="inline-code" id="45251276#-t757a4_155">-dbp|--db-prefix</span> option, VIEW definitions are NOT modified and will most likely cause VIEW creation to fail.</p></li></ul></section><section><h2 id="45251276#acid-tables" data-toc="acid-tables#hms-mirror-features.md-acid-tables">ACID Tables</h2><p id="45251276#-t757a4_156"><span class="inline-code" id="45251276#-t757a4_162">hms-mirror</span> supports the migration of ACID tables using the <span class="inline-code" id="45251276#-t757a4_163">-d HYBRID|SQL|EXPORT_IMPORT</span> data strategy in combination with the <span class="inline-code" id="45251276#-t757a4_164">-ma|--migrate-acid</span> or <span class="inline-code" id="45251276#-t757a4_165">-mao|--migrate-acid-only</span> flag. You can also simply 'replay' the schema definition (without data) using <span class="inline-code" id="45251276#-t757a4_166">-d SCHEMA_ONLY -ma|-mao</span>. The <span class="inline-code" id="45251276#-t757a4_167">-ma|-mao</span> flag takes an <span class="emphasis" id="45251276#-t757a4_168">optional</span> integer value that sets an 'Artificial Bucket Threshold'. When no parameter is specified, the default is <span class="inline-code" id="45251276#-t757a4_169">2</span>.</p><p id="45251276#-t757a4_157">Use this value to set a bucket limit where we'll <span class="emphasis" id="45251276#-t757a4_170">remove</span> the bucket definition during the translation. This is helpful for legacy ACID tables which <span class="emphasis" id="45251276#-t757a4_171">required</span> a bucket definition but weren't a part of the intended design. The migration provides an opportunity to correct this artificial design element.</p><p id="45251276#-t757a4_158">With the default value <span class="inline-code" id="45251276#-t757a4_172">2</span>, we will <span class="emphasis" id="45251276#-t757a4_173">remove</span> CLUSTERING from any ACID table definitions with <span class="inline-code" id="45251276#-t757a4_174">2</span> or fewer buckets defined. If you wish to keep ALL CLUSTERED definitions, regardless of size, set this value to <span class="inline-code" id="45251276#-t757a4_175">0</span>.</p><p id="45251276#-t757a4_159">There is now an option to 'downgrade' ACID tables to EXTERNAL/PURGE during migration using the <span class="inline-code" id="45251276#-t757a4_176">-da</span> option.</p><section><h3 id="45251276#the-acid-migration-process" data-toc="the-acid-migration-process#hms-mirror-features.md-the-acid-migration-process">The ACID Migration Process</h3><p id="45251276#-t757a4_177">The ACID migration builds a 'transfer' table on the LEFT cluster, a 'legacy' managed table (when the LEFT is a legacy cluster), or an 'EXTERNAL/PURGE' table. Data is copied to this transfer table from the original ACID table via SQL.</p><p id="45251276#-t757a4_178">Since the clusters are linked (<a href="#1958103884">Linking Cluster Storage Layers</a>), we build a 'shadow' table that is 'EXTERNAL' on the 'RIGHT' cluster that uses the data in the 'LEFT' cluster. Similar to the LINKED data strategy. If the data is partitioned, we run <span class="inline-code" id="45251276#-t757a4_183">MSCK</span> on this 'shadow' table in the 'RIGHT' cluster to discover all the partitions.</p><p id="45251276#-t757a4_179">The final ACID table is created in the 'RIGHT' cluster, and SQL is used to copy data from the 'LEFT' cluster via the 'shadow' table.</p><p id="45251276#-t757a4_180"><span class="control" id="45251276#-t757a4_184">Requirements</span></p><ul class="list" id="45251276#-t757a4_181" start="1"><li class="list-item" id="45251276#-t757a4_185"><p>Data Strategy: <span class="inline-code" id="45251276#-t757a4_193">HYBRID</span>, <span class="inline-code" id="45251276#-t757a4_194">SQL</span>, or <span class="inline-code" id="45251276#-t757a4_195">EXPORT_IMPORT</span></p></li><li class="list-item" id="45251276#-t757a4_186"><p>Activate Migrate ACID: <span class="inline-code" id="45251276#-t757a4_196">-ma|-mao</span></p></li><li class="list-item" id="45251276#-t757a4_187"><p>Link Clusters (<a href="#1958103884">Linking Cluster Storage Layers</a>), unless using the <span class="inline-code" id="45251276#-t757a4_198">-is|--intermediate-storage</span> option.</p></li><li class="list-item" id="45251276#-t757a4_188"><p>This is a 'ONE' time transfer. It is not an incremental update process.</p></li><li class="list-item" id="45251276#-t757a4_189"><p>Adequate Storage on LEFT to make an 'EXTERNAL' copy of the ACID table.</p></li><li class="list-item" id="45251276#-t757a4_190"><p>Permissions: </p><ul class="list" id="45251276#-t757a4_199" start="1"><li class="list-item" id="45251276#-t757a4_200"><p>From the RIGHT cluster, the submitting user WILL need access to the LEFT cluster's storage layer (HDFS) to create the shadow table (with location) that points across clusters.</p></li><li class="list-item" id="45251276#-t757a4_201"><p>doas will have a lot to do with the permissions requirements.</p></li><li class="list-item" id="45251276#-t757a4_202"><p>The 'hive' service account on the RIGHT cluster will need elevated privileges to the LEFT storage LAYER (HDFS). For example: If the hive service accounts on each cluster DO NOT share the same identity, like <span class="inline-code" id="45251276#-t757a4_203">hive</span>, then the RIGHT hive identity MUST also have privileged access to the LEFT clusters HDFS layer.</p></li></ul></li><li class="list-item" id="45251276#-t757a4_191"><p>Partitioned tables must have data that is 'discoverable' via <span class="inline-code" id="45251276#-t757a4_204">MSCK</span>. NOTE: The METADATA activity and REDUCER restrictions to the number of BUCKETs can dramatically affect this.- The number of partitions in the source ACID tables must be below the <span class="inline-code" id="45251276#-t757a4_205">partitionLimit</span> (default 500). This strategy may not be successful when the partition count is above this, and we won't even attempt the conversion. Check YARN for the progress of jobs with a large number of partitions/buckets. Progress many appear stalled from 'hms-mirror'.</p></li><li class="list-item" id="45251276#-t757a4_192"><p>ACID table migration to Hive 1/2 is NOT supported due to the lack of support for &quot;INSERT OVERWRITE&quot; on transactional tables. Hive 1/2 to Hive 3 IS support and the target of this implementation. Hive 3 to Hive 3 is also supported.</p></li></ul></section><section><h3 id="45251276#replace-acid-r-or-replace" data-toc="replace-acid-r-or-replace#hms-mirror-features.md-replace-acid-r-or-replace">Replace ACID -r or --replace</h3><p id="45251276#-t757a4_207">When downgrading ACID tables during migration, the <span class="inline-code" id="45251276#-t757a4_210">-r</span> option will give you the option to 'replace' the original ACID table with the a table that is no longer ACID. This option is only available along with the <span class="inline-code" id="45251276#-t757a4_211">-da</span> and <span class="inline-code" id="45251276#-t757a4_212">SQL</span> data strategy options.</p></section></section><section><h2 id="45251276#intermediate-common-storage-options" data-toc="intermediate-common-storage-options#hms-mirror-features.md-intermediate-common-storage-options">Intermediate/Common Storage Options</h2><p id="45251276#-t757a4_213">When bridging the gap between two clusters, you may find they can't share/link storage. In this case, using one of these options will help you with the transfer.</p><p id="45251276#-t757a4_214">The <span class="inline-code" id="45251276#-t757a4_216">-is</span> or <span class="inline-code" id="45251276#-t757a4_217">--intermediate-storage</span> option is consider a transient location that both cluster can share, see, and have access to. The strategies for transferring data (EXPORT_IMPORT, SQL, HYBRID) will use this location to facilitate the transfer. This is a common strategy when migrating from on-prem environments to the cloud.</p><p id="45251276#-t757a4_215">The <span class="inline-code" id="45251276#-t757a4_218">-cs</span> or <span class="inline-code" id="45251276#-t757a4_219">--common-storage</span> option is similar to <span class="inline-code" id="45251276#-t757a4_220">-is</span> but this option ends up being the final resting place for the data, not just the transfer location. And with this option, we can streamline the jumps required to migrate data. Again, this location needs to be accessible to both clusters.</p></section><section><h2 id="45251276#non-native-hive-tables-hbase-kafka-jdbc-druid-etc" data-toc="non-native-hive-tables-hbase-kafka-jdbc-druid-etc#hms-mirror-features.md-non-native-hive-tables-hbase-kafka-jdbc-druid-etc">Non-Native Hive Tables (Hbase, KAFKA, JDBC, Druid, etc..)</h2><p id="45251276#-t757a4_221">Any table definition without a <span class="inline-code" id="45251276#-t757a4_225">LOCATION</span> element is typically a reference to an external system like: HBase, Kafka, Druid, and/or (but not limited to) JDBC.</p><p id="45251276#-t757a4_222"><span class="control" id="45251276#-t757a4_226">Requirements</span></p><p id="45251276#-t757a4_223">These references require the environment to be:</p><ul class="list" id="45251276#-t757a4_224" start="1"><li class="list-item" id="45251276#-t757a4_227"><p>Correctly configured to use these resources</p></li><li class="list-item" id="45251276#-t757a4_228"><p>Include the required libraries in the default hive environment.</p></li><li class="list-item" id="45251276#-t757a4_229"><p>The referenced resource must exist already BEFORE the 'hive' DDL will successfully run.</p></li></ul></section><section><h2 id="45251276#avro-tables" data-toc="avro-tables#hms-mirror-features.md-avro-tables">AVRO Tables</h2><p id="45251276#-t757a4_230">AVRO tables can be designed with a 'reference' to a schema file in <span class="inline-code" id="45251276#-t757a4_236">TBLPROPERTIES</span> with <span class="inline-code" id="45251276#-t757a4_237">avro.schema.url</span>. The referenced file needs to be 'copied' to the <span class="emphasis" id="45251276#-t757a4_238">RIGHT</span> cluster BEFORE the <span class="inline-code" id="45251276#-t757a4_239">CREATE</span> statement for the AVRO table will succeed.</p><p id="45251276#-t757a4_231">Add the <span class="inline-code" id="45251276#-t757a4_240">-asm|--avro-schema-move</span> option at the command line to <span class="emphasis" id="45251276#-t757a4_241">copy</span> the file from the LEFT cluster to the RIGHT cluster.</p><p id="45251276#-t757a4_232">As long as the clusters are linked (<a href="#1958103884">Linking Cluster Storage Layers</a>) and the cluster <span class="inline-code" id="45251276#-t757a4_243">hcfsNamespace</span> values are accurate, the user's credentials running <span class="inline-code" id="45251276#-t757a4_244">hms-mirror</span> will attempt to copy the schema file to the <span class="emphasis" id="45251276#-t757a4_245">RIGHT</span> cluster BEFORE executing the <span class="inline-code" id="45251276#-t757a4_246">CREATE</span> statement.</p><p id="45251276#-t757a4_233"><span class="control" id="45251276#-t757a4_247">Requirements</span></p><ul class="list" id="45251276#-t757a4_234" start="1"><li class="list-item" id="45251276#-t757a4_248"><p>Link Clusters (<a href="#1958103884">Linking Cluster Storage Layers</a>) for Data Strategies: <span class="inline-code" id="45251276#-t757a4_253">SCHEMA_ONLY</span>, <span class="inline-code" id="45251276#-t757a4_254">SQL</span>, <span class="inline-code" id="45251276#-t757a4_255">EXPORT_IMPORT</span>, and <span class="inline-code" id="45251276#-t757a4_256">HYBRID</span></p></li><li class="list-item" id="45251276#-t757a4_249"><p>Running user must have 'namespace' access to the directories identified in the <span class="inline-code" id="45251276#-t757a4_257">TBLPROPERTIES</span> key <span class="inline-code" id="45251276#-t757a4_258">avro.schema.url</span>.</p></li><li class="list-item" id="45251276#-t757a4_250"><p>The user running <span class="inline-code" id="45251276#-t757a4_259">hms-mirror</span> will need enough storage level permissions to copy the file.</p></li><li class="list-item" id="45251276#-t757a4_251"><p>When hive is running with <span class="inline-code" id="45251276#-t757a4_260">doas=false</span>, <span class="inline-code" id="45251276#-t757a4_261">hive</span> will need access to this file.</p></li></ul><section><h3 id="45251276#warnings" data-toc="warnings#hms-mirror-features.md-warnings">Warnings</h3><ul class="list" id="45251276#-t757a4_262" start="1"><li class="list-item" id="45251276#-t757a4_263"><p>With the <span class="inline-code" id="45251276#-t757a4_264">EXPORT_IMPORT</span> strategy, the <span class="inline-code" id="45251276#-t757a4_265">avro.schema.url</span> location will NOT be converted. It may lead to an issue reading the table if the location includes a prefix of the cluster's namespace OR the file doesn't exist in the new cluster.</p></li></ul></section></section><section><h2 id="45251276#table-translations" data-toc="table-translations#hms-mirror-features.md-table-translations">Table Translations</h2><section><h3 id="45251276#legacy-managed-tables" data-toc="legacy-managed-tables#hms-mirror-features.md-legacy-managed-tables">Legacy Managed Tables</h3><p id="45251276#-t757a4_267"><span class="inline-code" id="45251276#-t757a4_268">hms-mirror</span> will convert 'legacy' managed tables in Hive 1 or 2 to EXTERNAL tables in Hive 3. It relies on the <span class="inline-code" id="45251276#-t757a4_269">legacyHive</span> setting in the cluster configurations to accurately make this conversion. So make sure you've set this correctly.</p></section></section><section><h2 id="45251276#distcp-planning-workbook-and-scripts" data-toc="distcp-planning-workbook-and-scripts#hms-mirror-features.md-distcp-planning-workbook-and-scripts">distcp Planning Workbook and Scripts</h2><p id="45251276#-t757a4_271"><span class="inline-code" id="45251276#-t757a4_278">hms-mirror</span> will create source files and a shell script that can be used as the basis for the 'distcp' job(s) used to support the databases and tables requested in <span class="inline-code" id="45251276#-t757a4_279">-db</span>. <span class="inline-code" id="45251276#-t757a4_280">hms-mirror</span> will NOT run these jobs. It will provide the basic job constructs that match what it did for the schemas. Use these constructs to build your execution plan and run these separately.</p><p id="45251276#-t757a4_272">The constructs created are intended as a <span class="emphasis" id="45251276#-t757a4_281">one-time</span> transfer. If you are using <span class="emphasis" id="45251276#-t757a4_282">SNAPSHOTS</span> or <span class="inline-code" id="45251276#-t757a4_283">--update</span> flags in <span class="inline-code" id="45251276#-t757a4_284">distcp</span> to support incremental updates, you will have to make additional modifications to the scripts/process. Note: For these scenarios, <span class="inline-code" id="45251276#-t757a4_285">hms-mirror</span> supports options like <span class="inline-code" id="45251276#-t757a4_286">-ro|--read-only</span> and <span class="inline-code" id="45251276#-t757a4_287">-sync</span>.</p><p id="45251276#-t757a4_273">Each time <span class="inline-code" id="45251276#-t757a4_288">hms-mirror</span> is run, <span class="emphasis" id="45251276#-t757a4_289">source</span> files for each database are created. These source files need to be copied to the distributed filesystem and reference with an <span class="inline-code" id="45251276#-t757a4_290">-f</span> option in <span class="inline-code" id="45251276#-t757a4_291">distcp</span>. We also create a <span class="emphasis" id="45251276#-t757a4_292">basic</span> shell script that can be used as a template to run the actual <span class="inline-code" id="45251276#-t757a4_293">distcp</span> jobs.</p><p id="45251276#-t757a4_274">Depending on the job size and operational expectations, you may want to use <span class="emphasis" id="45251276#-t757a4_294">SNAPSHOTS</span> to ensure an immutable source or use a <span class="inline-code" id="45251276#-t757a4_295">diff</span> strategy for more complex migrations. Regardless, you'll need to make modifications to these scripts to suit your purposes.</p><p id="45251276#-t757a4_275">If your process requires the data to exist BEFORE you migrate the schemas, run <span class="inline-code" id="45251276#-t757a4_296">hms-mirror</span> in the <span class="inline-code" id="45251276#-t757a4_297">dry-run</span> mode (default) and use the distcp planning workbook and scripts to transfer the datasets. Then run <span class="inline-code" id="45251276#-t757a4_298">hms-mirror</span> with the <span class="inline-code" id="45251276#-t757a4_299">-e|--execute</span> option to migrate the schemas.</p><p id="45251276#-t757a4_276">These workbooks will NOT include elements for ACID/Transactional tables. Simply copying the dataset for transactional tables will NOT work. Use the <span class="inline-code" id="45251276#-t757a4_300">HYBRID</span> data strategy migration transactional table schemas and datasets.</p></section><section><h2 id="45251276#acid-table-downgrades" data-toc="acid-table-downgrades#hms-mirror-features.md-acid-table-downgrades">ACID Table Downgrades</h2><p id="45251276#-t757a4_301">The default table creation scenario for Hive 3 (CDP and HDP 3) installations is to create ACID transactional tables. If you're moving from a legacy platform like HDP 2 or CDH, this may have caught you off guard and resulted in a lot of ACID tables you did NOT intend on.</p><p id="45251276#-t757a4_302">The <span class="inline-code" id="45251276#-t757a4_304">-da|--downgrade-acid</span> option can be used to convert these ACID tables to <span class="emphasis" id="45251276#-t757a4_305">EXTERNAL/PURGE</span> tables.</p><p id="45251276#-t757a4_303">If you have ACID tables on the current platform and would like to <span class="emphasis" id="45251276#-t757a4_306">downgrade</span> them, but you're not doing a migration, try the <span class="inline-code" id="45251276#-t757a4_307">-ip|--in-place</span> option. This will archive to existing ACID table and build a new table (with the original table name) that is <span class="emphasis" id="45251276#-t757a4_308">EXTERNAL/PURGE</span>.</p></section><section><h2 id="45251276#reset-to-default-locations" data-toc="reset-to-default-locations#hms-mirror-features.md-reset-to-default-locations">Reset to Default Locations</h2><p id="45251276#-t757a4_309">Migrations present an opportunity to clean up a lot of history. While <span class="inline-code" id="45251276#-t757a4_312">hms-mirror</span> was originally designed to migration data and maintain the <span class="control" id="45251276#-t757a4_313">relative</span> locations of the data, some may want to reorganize the data during the migration.</p><p id="45251276#-t757a4_310">The option <span class="inline-code" id="45251276#-t757a4_314">-rdl|--reset-default-location</span> will overwrite the locations originally used and place the datasets in the 'default' locations as defined by <span class="inline-code" id="45251276#-t757a4_315">-wd|--warehouse-directory</span> and <span class="inline-code" id="45251276#-t757a4_316">-ewd|--external-warehouse-directory</span>.</p><p id="45251276#-t757a4_311">The <span class="inline-code" id="45251276#-t757a4_317">-rdl</span> option requires <span class="inline-code" id="45251276#-t757a4_318">-wd</span> and <span class="inline-code" id="45251276#-t757a4_319">-ewd</span> to be specified. These locations will be used to <span class="inline-code" id="45251276#-t757a4_320">ALTER</span> the databases <span class="inline-code" id="45251276#-t757a4_321">LOCATION</span> and <span class="inline-code" id="45251276#-t757a4_322">MANAGEDLOCATION</span> values. After which, all new <span class="inline-code" id="45251276#-t757a4_323">CREATE \[EXTERNAL\] TABLE</span> definitions don't specify a <span class="inline-code" id="45251276#-t757a4_324">LOCATION</span>, which means table locations will use the default.</p></section><section><h2 id="45251276#legacy-row-serde-translations" data-toc="legacy-row-serde-translations#hms-mirror-features.md-legacy-row-serde-translations">Legacy Row Serde Translations</h2><p id="45251276#-t757a4_325">By default, tables using old row <span class="emphasis" id="45251276#-t757a4_327">serde</span> classes will be converted to the newer serde as the definition is processed by <span class="inline-code" id="45251276#-t757a4_328">hms-mirror</span>. See here (<a href="https://github.com/cloudera-labs/hms-mirror/blob/6f6c309f24fbb8133e5bd52e5b18274094ff5be8/src/main/java/com/cloudera/utils/hadoop/hms/mirror/feature/LegacyTranslations.java#L28">https://github.com/cloudera-labs/hms-mirror/blob/6f6c309f24fbb8133e5bd52e5b18274094ff5be8/src/main/java/com/cloudera/utils/hadoop/hms/mirror/feature/LegacyTranslations.java#L28</a>) for a list of serdes we look for.</p><p id="45251276#-t757a4_326">If you do NOT want to apply this translation, add the option <span class="inline-code" id="45251276#-t757a4_330">-slt|--skip-legacy-translation</span> to the commandline.</p></section><section><h2 id="45251276#filtering-tables-to-process" data-toc="filtering-tables-to-process#hms-mirror-features.md-filtering-tables-to-process">Filtering Tables to Process</h2><p id="45251276#-t757a4_331">There are options to filter tables included in <span class="inline-code" id="45251276#-t757a4_335">hms-mirror</span> process. You can select <span class="inline-code" id="45251276#-t757a4_336">-tf|--table-filter</span> to &quot;include&quot; only tables that match this 'regular-expression'. Inversely, use <span class="inline-code" id="45251276#-t757a4_337">-etf|--exclude-table-filter</span> to omit tables from the list. These options are mutually exclusive.</p><p id="45251276#-t757a4_332">The filters for <span class="inline-code" id="45251276#-t757a4_338">-tf</span> and <span class="inline-code" id="45251276#-t757a4_339">-tef</span> are expressed as a 'regular-expression'. Complex expressions should be enclosed in quotes to ensure the commandline interpreter doesn't split them.</p><p id="45251276#-t757a4_333">Additional table filters (<span class="inline-code" id="45251276#-t757a4_340">-tfs|--table-filter-size-limit</span> and <span class="inline-code" id="45251276#-t757a4_341">-tfp|--table-filter-partition-count-limit</span>) that check a tables data size and partition count limits can also be applied to narrow the range of tables you'll process.</p><p id="45251276#-t757a4_334">The filter does NOT override the requirement for options like <span class="inline-code" id="45251276#-t757a4_342">-ma|-mao</span>. It is used as an additional filter.</p></section><section><h2 id="45251276#migrations-between-clusters-without-line-of-site" data-toc="migrations-between-clusters-without-line-of-site#hms-mirror-features.md-migrations-between-clusters-without-line-of-site">Migrations between Clusters WITHOUT line of Site</h2><p id="45251276#-t757a4_343">There will be cases where clusters can't be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). And without this line of sight, data movement needs to happen through some other means.</p><section><h3 id="45251276#on-prem-to-cloud" data-toc="on-prem-to-cloud#hms-mirror-features.md-on-prem-to-cloud">On-Prem to Cloud</h3><p id="45251276#-t757a4_346">This scenario is most common with &quot;on-prem&quot; to &quot;cloud&quot; migrations. Typically, <span class="inline-code" id="45251276#-t757a4_348">hms-mirror</span> is run from the <span class="control" id="45251276#-t757a4_349">RIGHT</span> cluster, but in this case the <span class="control" id="45251276#-t757a4_350">RIGHT</span> cloud cluster doesn't have line of site to the <span class="control" id="45251276#-t757a4_351">LEFT</span> on-prem cluster. But the on-prem cluster will have limited line of site to the cloud environment. In this case, the only option is to run <span class="inline-code" id="45251276#-t757a4_352">hms-mirror</span> from the on-prem cluster. The on-prem cluster will need access to either an <span class="inline-code" id="45251276#-t757a4_353">-is|--intermediate-storage</span> location that both clusters can access or a <span class="inline-code" id="45251276#-t757a4_354">-cs|--common-storage</span> location that each cluster can see, but can be considered the final resting place for the cloud environment. The <span class="inline-code" id="45251276#-t757a4_355">-cs</span> option usually means there are fewer data hops required to complete the migration.</p><p id="45251276#-t757a4_347">You'll run <span class="inline-code" id="45251276#-t757a4_356">hms-mirror</span> from a <span class="control" id="45251276#-t757a4_357">LEFT</span> cluster edgenode. This node will require line of site to the Hive Server 2 endpoint in the <span class="control" id="45251276#-t757a4_358">RIGHT</span> cloud environment. The <span class="control" id="45251276#-t757a4_359">LEFT</span> cluster will also need to be configured with the appropriate libraries and configurations to write to the location defined in <span class="inline-code" id="45251276#-t757a4_360">-is|-cs</span>. For example: For S3, the <span class="control" id="45251276#-t757a4_361">LEFT</span> cluster will need the AWS S3 libraries configured and the appropriate keys for S3 access setup to complete the transfer.</p></section></section><section><h2 id="45251276#shared-storage-models-isilon-spectrum-scale-etc" data-toc="shared-storage-models-isilon-spectrum-scale-etc#hms-mirror-features.md-shared-storage-models-isilon-spectrum-scale-etc">Shared Storage Models (Isilon, Spectrum-Scale, etc.)</h2><p id="45251276#-t757a4_362">There are cases where 'HDFS' isn't the primary data source. So the only thing the cluster share is storage in these 'common' storage units. You want to transfer the schema, but the data doesn't need to move (at least for 'EXTERNAL' (non-transactional) tables). In this case, try the <span class="inline-code" id="45251276#-t757a4_363">-d|--data-strategy</span> COMMON. The schema's will go through all the needed conversions while the data remains in the same location.</p></section><section><h2 id="45251276#disconnected-mode" data-toc="disconnected-mode#hms-mirror-features.md-disconnected-mode">Disconnected Mode</h2><p id="45251276#-t757a4_364">Use the <span class="inline-code" id="45251276#-t757a4_370">-rid|--right-is-disconnected</span> mode when you need to build (and/or) transfer schema/datasets from one cluster to another, but you can't connect to both at the same time. See the issues log for details regarding the cases here issue #17 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/17">https://github.com/cloudera-labs/hms-mirror/issues/17</a>).</p><p id="45251276#-t757a4_365">Use cases:</p><ul class="list" id="45251276#-t757a4_366" start="1"><li class="list-item" id="45251276#-t757a4_372"><p>Schema Only Transfers</p></li><li class="list-item" id="45251276#-t757a4_373"><p>SQL, EXPORT_IMPORT, and HYBRID only when -is or -cs is used. This might be the case when the clusters are secure (kerberized), but don't share a common kerberos domain/user auth. So an intermediate or common storage location will be used to migrate the data.</p></li><li class="list-item" id="45251276#-t757a4_374"><p>Both clusters (and HS2 endpoints) are Kerberized, but the clusters are NOT the same major hadoop version. In this case, hms-mirror doesn't support connecting to both of these endpoints at the same time. Running in the disconnected mode will help push through with the conversion.</p></li></ul><p id="45251276#-t757a4_367">hms-mirror will run as normal, with the exception of examining and running scripts against the right cluster. It will be assumed that the RIGHT cluster elements do NOT exist.</p><p id="45251276#-t757a4_368">The RIGHT_ 'execution' scripts and distcp commands will need to be run MANUALLY via Beeline on the RIGHT cluster.</p><p id="45251276#-t757a4_369">Note: This will be know as the &quot;right-is-disconnected&quot; option. Which means the process should be run from a node that has access to the &quot;left&quot; cluster. This is 'counter' to our general recommendation that the process should be run from the 'right' cluster.</p></section><section><h2 id="45251276#no-purge-option" data-toc="no-purge-option#hms-mirror-features.md-no-purge-option">No-Purge Option</h2><p id="45251276#-t757a4_375"><span class="inline-code" id="45251276#-t757a4_377">-np</span></p><p id="45251276#-t757a4_376">Feature Request #25 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/25">https://github.com/cloudera-labs/hms-mirror/issues/25</a>) was introduced in v1.5.4.2 and gives the user to option to remove the <span class="inline-code" id="45251276#-t757a4_379">external.table.purge</span> option that is added when converting legacy managed tables to external table (Hive 1/2 to 3). This does affect the behavior of the table from the older platforms.</p></section><section><h2 id="45251276#property-overrides" data-toc="property-overrides#hms-mirror-features.md-property-overrides">Property Overrides</h2><p id="45251276#-t757a4_380"><span class="inline-code" id="45251276#-t757a4_387">-po[l|r] &lt;key=value&gt;[,&lt;key=value&gt;]...</span></p><p id="45251276#-t757a4_381">Feature Request #27 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/27">https://github.com/cloudera-labs/hms-mirror/issues/27</a>) introduced in v1.5.4.2 provides the ability to set a hive properties at the beginning of each migration part. This is a comma separated list of key=value pairs with no space. If spaces are needed, quote the parameter on the commandline.</p><p id="45251276#-t757a4_382">You can use <span class="inline-code" id="45251276#-t757a4_389">-po</span> to set the properties for BOTH clusters or <span class="inline-code" id="45251276#-t757a4_390">-pol</span>|<span class="inline-code" id="45251276#-t757a4_391">-por</span> to set them specifically for the 'left' and/or 'right' cluster.</p><p id="45251276#-t757a4_383">For example: <span class="inline-code" id="45251276#-t757a4_392">-po hive.exec.orc.split.strategy=BI,hive.compute.query.using.stats=false</span></p><p id="45251276#-t757a4_384">To provide a consistent list of settings for ALL jobs, add/modify the following section in the configuration file ie: <span class="inline-code" id="45251276#-t757a4_393">default.yaml</span> used for processing. In this case you do NOT need to use the commandline option. Although, you can set basic values in the configuration file and add other via the commandline.</p><p id="45251276#-t757a4_385">Notice that there are setting for the LEFT and RIGHT clusters.</p><div class="detached code-block" id="45251276#-t757a4_386"><pre><code class="language-yaml">optimization:
  overrides:
    left:
      tez.queue.name: &quot;compaction&quot;
    right:
      tez.queue.name: &quot;migration&quot;</code></pre></div></section><section><h2 id="45251276#global-location-map" data-toc="global-location-map#hms-mirror-features.md-global-location-map">Global Location Map</h2><p id="45251276#-t757a4_394"><span class="inline-code" id="45251276#-t757a4_401">-glm|--global-location-map &lt;from=to&gt;[,...]</span></p><p id="45251276#-t757a4_395">This is an opportunity to make some specific directory mappings during the migration. You can supply a comma separated list of directory pairs to be use for evaluation.</p><p id="45251276#-t757a4_396"><span class="inline-code" id="45251276#-t757a4_402">-glm /data/my_original_location=/corp/finance_new_loc,/user/hive=/warehouse/hive</span></p><p id="45251276#-t757a4_397">These directory mappings ONLY apply to 'EXTERNAL' and 'DOWNGRADED ACID' tables. You can supply 'n' number of mappings to review through the commandline interface as describe above. To provide a consistent set of mappings to ALL jobs, add/modify the following section in the configuration file ie: <span class="inline-code" id="45251276#-t757a4_403">default.yaml</span> used for processing.</p><div class="detached code-block" id="45251276#-t757a4_398"><pre><code class="language-yaml">translator:
  globalLocationMap:
    /data/my_original_location: &quot;/corp/finance_new_loc&quot;
    /user/hive: &quot;/warehouse/hive&quot;</code></pre></div><p id="45251276#-t757a4_399">The list will be sorted by the length of the string, then alpha-numerically. This will ensure the deepest nested paths are evaluated FIRST. When a path is matched during evaluation, the process will NOT look and any more paths in the list. Therefore, avoiding possible double evaluations that may result when there are nested paths in the list.</p><p id="45251276#-t757a4_400">Paths are evaluated with 'startsWith' on the original path (minus the original namespace). When a match is found, the path 'part' will be replaced with the value specified. The remaining path will remain intact and regardless of the <span class="inline-code" id="45251276#-t757a4_404">-rdl</span> setting, the LOCATION element will be included in the tables new CREATE statement.</p></section><section><h2 id="45251276#force-external-locations" data-toc="force-external-locations#hms-mirror-features.md-force-external-locations">Force External Locations</h2><p id="45251276#-t757a4_405"><span class="inline-code" id="45251276#-t757a4_409">-fel|--force-external-location</span></p><p id="45251276#-t757a4_406">Under some conditions, the default warehouse directory hierarchy is not honored. We've seen this in HDP 3. The <span class="inline-code" id="45251276#-t757a4_410">-rdl</span> option collects the external tables in the default warehouse directory by omitting the LOCATION element in the CREATE statement, relying on the default location. The default location is set at the DATABASE level by <span class="inline-code" id="45251276#-t757a4_411">hms-mirror</span>.</p><p id="45251276#-t757a4_407">In HDP3, the CREATE statement doesn't honor the 'database' LOCATION element and reverts to the system wide warehouse directory configurations. The <span class="inline-code" id="45251276#-t757a4_412">-fel</span> flag will simply include the 'properly' adjusted LOCATION element in the CREATE statement to ensure the tables are created in the desired location. This setting overrides the effects intended by the <span class="inline-code" id="45251276#-t757a4_413">-rdl</span> option which intend to place the tables under the stated warehouse locations by omitting the location from the tables definition and relying on the default location specified in the database.</p><p id="45251276#-t757a4_408"><span class="inline-code" id="45251276#-t757a4_414">-fel</span> will use the original location as a starting point. If <span class="inline-code" id="45251276#-t757a4_415">-wd|-ewd</span> are specified, they aren't not used in the translation, but warnings may be issued if the final location doesn't align with the warehouse directory. The effect change in the location when using <span class="inline-code" id="45251276#-t757a4_416">-fel</span>, add mappings via <span class="inline-code" id="45251276#-t757a4_417">-glm</span>.</p></section><section><h2 id="45251276#hdp-3-hive" data-toc="hdp-3-hive#hms-mirror-features.md-hdp-3-hive">HDP 3 Hive</h2><p id="45251276#-t757a4_418">HDP 3 (Hive 3) was an incomplete implementation with regards to complex table 'location' management. When you are working in this environment, add to the cluster configuration (LEFT or RIGHT) the setting: <span class="inline-code" id="45251276#-t757a4_425">hdpHive3: true</span>. There is NOT a commandline switch for this. JUst add it to the configuration file you're using to run the application.</p><div class="detached code-block" id="45251276#-t757a4_419"><pre><code class="language-yaml">clusters:
  LEFT:
    environment: &quot;LEFT&quot;
    legacyHive: false
    hdpHive3: true</code></pre></div><p id="45251276#-t757a4_420">HDP3 Hive did NOT have a MANAGEDLOCATION attribute for Databases.</p><p id="45251276#-t757a4_421">The LOCATION element tracked the Manage ACID tables and will control where they go.</p><p id="45251276#-t757a4_422">This LOCATION will need to be transferred to MANAGEDLOCATION 'after' upgrading to CDP to ensure ACID tables maintain the same behavior. EXTERNAL tables will explicity set there LOCATION element to match the setting in <span class="inline-code" id="45251276#-t757a4_426">-ewd</span>.</p><p id="45251276#-t757a4_423">Future external tables, when no location is specified, will be created in the <span class="inline-code" id="45251276#-t757a4_427">hive.metastore.warehouse.external.dir</span>. This value is global in HDP Hive3 and can NOT be set for individual databases.</p><p id="45251276#-t757a4_424">Post upgrade to CDP, you should add a specific directory value at the database level for better control.</p></section><section><h2 id="45251276#schema-fix-features" data-toc="schema-fix-features#hms-mirror-features.md-schema-fix-features">Schema Fix Features</h2><p id="45251276#-t757a4_428">Schema Fix Features are a way to inject special considerations into the replay of a schema between clusters. Each schema is automatically check is a particular 'feature' applies.</p><p id="45251276#-t757a4_429">If you find that this features check is causing issues, add the flag <span class="inline-code" id="45251276#-t757a4_433">-sf</span> to the application parameters and the feature checks will be skipped.</p><section><h3 id="45251276#bad-orc-def" data-toc="bad-orc-def#hms-mirror-features.md-bad-orc-def">BAD_ORC_DEF</h3><p id="45251276#-t757a4_434"><span class="inline-code" id="45251276#-t757a4_442">BAD_ORC_DEF</span> is a feature that corrects poorly executed schema definitions in legacy Hive 1/2 that don't translate into a functioning table in Hive 3. In this case, the legacy definition was defined with:</p><div class="detached code-block" id="45251276#-t757a4_435"><pre><code class="language-none">ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'</code></pre></div><p id="45251276#-t757a4_436">when it should have been created with:</p><div class="detached code-block" id="45251276#-t757a4_437"><pre><code class="language-none">STORED AS ORC</code></pre></div><p id="45251276#-t757a4_438">The result, when not modified and replayed in Hive 3 is a table that isn't functional. The <span class="inline-code" id="45251276#-t757a4_443">BAD_ORC_DEF</span> feature will replace:</p><div class="detached code-block" id="45251276#-t757a4_439"><pre><code class="language-none">ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n'</code></pre></div><p id="45251276#-t757a4_440">with:</p><div class="detached code-block" id="45251276#-t757a4_441"><pre><code class="language-none">ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'</code></pre></div></section><section><h3 id="45251276#bad-rc-def" data-toc="bad-rc-def#hms-mirror-features.md-bad-rc-def">BAD_RC_DEF</h3><p id="45251276#-t757a4_444"><span class="inline-code" id="45251276#-t757a4_452">BAD_RC_DEF</span> is a feature that corrects poorly executed schema definitions in legacy Hive 1/2 that doesn't translate into a functioning table in Hive 3. In this case, the legacy definition was defined with:</p><div class="detached code-block" id="45251276#-t757a4_445"><pre><code class="language-none">ROW FORMAT DELIMITED,
    FIELDS TERMINATED BY '|'
 STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
 OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'</code></pre></div><p id="45251276#-t757a4_446">when it should have been created with:</p><div class="detached code-block" id="45251276#-t757a4_447"><pre><code class="language-none">STORED AS RCFILE</code></pre></div><p id="45251276#-t757a4_448">The result, when not modified and replayed in Hive 3 is a table that isn't functional. The <span class="inline-code" id="45251276#-t757a4_453">BAD_RC_DEF</span> feature will replace:</p><div class="detached code-block" id="45251276#-t757a4_449"><pre><code class="language-none">ROW FORMAT DELIMITED,                              
    FIELDS TERMINATED BY '|'                       
 STORED AS INPUTFORMAT</code></pre></div><p id="45251276#-t757a4_450">with:</p><div class="detached code-block" id="45251276#-t757a4_451"><pre><code class="language-none">STORED AS RCFILE</code></pre></div></section><section><h3 id="45251276#bad-textfile-def" data-toc="bad-textfile-def#hms-mirror-features.md-bad-textfile-def">BAD_TEXTFILE_DEF</h3><p id="45251276#-t757a4_454">Older Textfile schemas somehow are corrupted through subsequent ALTER statements that get the table into a state where you can NOT re-run the contents of <span class="inline-code" id="45251276#-t757a4_458">SHOW CREATE TABLE</span>. In this case, the issue is that there is a declaration for <span class="inline-code" id="45251276#-t757a4_459">WITH SERDEPROPERTIES</span> along with a <span class="inline-code" id="45251276#-t757a4_460">ROW FORMAT DELIMITED</span> clause. These two can NOT exist together. Here is an example of this:</p><div class="detached code-block" id="45251276#-t757a4_455"><pre><code class="language-none">ROW FORMAT DELIMITED
     FIELDS TERMINATED BY '|'
     LINES TERMINATED BY '\n'
WITH SERDEPROPERTIES (
     'escape.delim'='\\')
STORED AS INPUTFORMAT
     'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</code></pre></div><p id="45251276#-t757a4_456">In this case, we need to convert the <span class="inline-code" id="45251276#-t757a4_461">ROW FORMAT DELIMITED * TERMINATED BY *</span> values into the <span class="inline-code" id="45251276#-t757a4_462">SERDEPROPERTIES</span> and replace it with</p><div class="detached code-block" id="45251276#-t757a4_457"><pre><code class="language-none">ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'</code></pre></div></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="2080784241">Strategies</h1><p id="2080784241#-q8yoev_3">A 'strategy' in <span class="inline-code" id="2080784241#-q8yoev_6">hms-mirror</span> is the method used to migrate metadata and possibly data from one metastore to another.</p><p id="2080784241#-q8yoev_4">Some strategies are simply responsible for migrating metadata, while others are responsible for migrating metadata and data.</p><section><h2 id="2080784241#data-movement" data-toc="data-movement#Strategies.md-data-movement">Data Movement</h2><p id="2080784241#-q8yoev_7">When the strategy is responsible for moving data as well, you have two options: <span class="inline-code" id="2080784241#-q8yoev_12">SQL</span> and <span class="inline-code" id="2080784241#-q8yoev_13">distcp</span>. You can set this option here:</p><div class="container"><figure class="image-container"><img class="center image image-size" id="2080784241#-q8yoev_8" alt="dm_options.png" title="dm_options.png" src="/Users/dstreev/projects/david/hms-mirror/Writerside/images/dm_options.png" width="948" height="343"><figcaption class="center-text">dm_options.png</figcaption></figure></div><p id="2080784241#-q8yoev_9">SQL will rely on the engine and the clusters ability to see across data storage environments to move the data.</p><p id="2080784241#-q8yoev_10">For distcp (Distributed Copy) (<a href="https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html">https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html</a>), we'll build a plan that matches what you've asked to migrate.</p><p id="2080784241#-q8yoev_11">For a matrix of when this might be available, see Location Alignment (<a href="#-356146741">Location Alignment</a>).</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1171882890">SCHEMA_ONLY</h1><p id="1171882890#zdpiee_3">This strategy is used to migrate &quot;only&quot; the schema (mostly). When migrating from a <span class="inline-code" id="1171882890#zdpiee_6">legacy</span> (Hive 1/2) hive environment to a <span class="inline-code" id="1171882890#zdpiee_7">non-legacy</span> (Hive 3+) <span class="inline-code" id="1171882890#zdpiee_8">hms-mirror</span> will convert tables that are &quot;managed non-transactional&quot; to &quot;external/purge&quot;. This translation is a part of the HSMM (Hive Strict Managed Migration) process that run for 'in-place' upgrades.</p><p id="1171882890#zdpiee_4"><span class="inline-code" id="1171882890#zdpiee_9">hms-mirror</span> is mostly designed for &quot;side-car&quot; migrations which involves two separate clusters.</p><section><h2 id="1171882890#options" data-toc="options#SCHEMA_ONLY.md-options">Options</h2><section><h3 id="1171882890#f-flip" data-toc="f-flip#SCHEMA_ONLY.md-f-flip">-f|--flip</h3><p id="1171882890#zdpiee_16">The configuration yaml has two clusters defined: LEFT and RIGHT. By default, all work moves from LEFT to RIGHT. If you need to perform actions from RIGHT to LEFT, use the <span class="inline-code" id="1171882890#zdpiee_18">-f</span> option to switch these clusters. The final report will show that these clusters have been reversed.</p></section><section><h3 id="1171882890#ma-migrate-acid-or-mao-migrate-acid-only-optional-artificial-bucket-limit-default-is-2" data-toc="ma-migrate-acid-or-mao-migrate-acid-only-optional-artificial-bucket-limit-default-is-2#SCHEMA_ONLY.md-ma-migrate-acid-or-mao-migrate-acid-only-optional-artificial-bucket-limit-default-is-2">-ma|--migrate-acid or -mao|--migrate-acid-only  \[optional artificial bucket limit\] \(default is 2\)</h3><p id="1171882890#zdpiee_20">The default behaviour is NOT to handle ACID tables. ACID tables require some additional handling. When you need to address ACID tables, use these flags to either include them OR migrate ONLY ACID tables.</p><p id="1171882890#zdpiee_21">Migrations from legacy hive environments may have had to create tables with an unnecessary <span class="inline-code" id="1171882890#zdpiee_24">CLUSTERED BY</span> clause to support <span class="inline-code" id="1171882890#zdpiee_25">bucketing</span> for these ACID tables. The <span class="inline-code" id="1171882890#zdpiee_26">artificial bucket limit</span> is an opportunity to remove those definitions for the migrated tables. For example: If you've defined a table with 2 buckets, the default <span class="inline-code" id="1171882890#zdpiee_27">artificial bucket limit</span> is 2, so the table <span class="inline-code" id="1171882890#zdpiee_28">CLUSTERED BY ... INTO 2 BUCKETS</span> will be removed from the definition.</p></section><section><h3 id="1171882890#mnno-migrate-non-native-only" data-toc="mnno-migrate-non-native-only#SCHEMA_ONLY.md-mnno-migrate-non-native-only">-mnno|--migrate-non-native-only</h3><p id="1171882890#zdpiee_30">Use this to migrate <span class="emphasis" id="1171882890#zdpiee_33">non-native</span> hive tables like: Kafka, HBase, JDBC Federated, etc.. This option excludes <span class="emphasis" id="1171882890#zdpiee_34">native</span> tables from processing, so you should run it separately.</p><blockquote class="prompt flex bordered-element-rounded warning detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>Many of these tables **REQUIRE** that the supporting references **EXIST** first, or their creation will **FAIL**. For instance, migrating a table with the HBase Storage handler requires that the HBase table is *present* AND *visible* from Hive.</p></div>
</blockquote>
</section><section><h3 id="1171882890#v-views-only" data-toc="v-views-only#SCHEMA_ONLY.md-v-views-only">-v|--views-only</h3><p id="1171882890#zdpiee_36">Like <span class="inline-code" id="1171882890#zdpiee_38">-mnno</span> the <span class="inline-code" id="1171882890#zdpiee_39">-v</span> option excludes other tables from processing, so it's designed to run along. And like the <span class="inline-code" id="1171882890#zdpiee_40">-mnno</span> option, it too requires the supporting tables to exist, otherwise the view create statements will <span class="emphasis" id="1171882890#zdpiee_41">fail</span>.</p></section><section><h3 id="1171882890#asm-avro-schema-migration" data-toc="asm-avro-schema-migration#SCHEMA_ONLY.md-asm-avro-schema-migration">-asm|--avro-schema-migration</h3><p id="1171882890#zdpiee_43">AVRO tables have a <span class="inline-code" id="1171882890#zdpiee_52">TBLPROPERTIES</span> key <span class="inline-code" id="1171882890#zdpiee_53">avro.schema.url</span> that identifies the supporting avro schemas location on the current cluster. When migrating AVRO tables, use this option to check for this setting. As a result, <span class="inline-code" id="1171882890#zdpiee_54">hms-mirror</span> will parse out the location and replace it with the same relative location.</p><p id="1171882890#zdpiee_44"><span class="inline-code" id="1171882890#zdpiee_55">hms-mirror</span> will use the configured environments <span class="inline-code" id="1171882890#zdpiee_56">hdfs|core-site.xml</span> files to launch an <span class="inline-code" id="1171882890#zdpiee_57">hdfs</span> client that will copy the supporting avro schema between clusters. For this to work, the clusters MUST be visible to each other (<a href="#1958103884">Linking Cluster Storage Layers</a>).</p><p id="1171882890#zdpiee_45">The DUMP strategy is like SCHEMA_ONLY; it just doesn't require the RIGHT cluster to be connected. Although, it does need the following settings for the RIGHT cluster to make the proper adjustments:</p><div class="detached code-block" id="1171882890#zdpiee_46"><pre><code class="language-none">legacyHive
hcfsNamespace
hiveServer2 -&gt; partitionDiscovery</code></pre></div><p id="1171882890#zdpiee_47">When the option <span class="inline-code" id="1171882890#zdpiee_59">-ma</span> (migrate acid) is specified, the ACID schema's will be migrated/dumped. It is essential to know that the data for ACID tables can NOT simply be copied from one clusters hive instance to another. The data needs to be extracted to a none ACID table, then use an external table definition to read and INSERT the data into the new ACID table on the RIGHT cluster. For those that insist on trying to simply copy the data.... you've been warned ;).</p><p id="1171882890#zdpiee_48">With the DUMP strategy, you'll have a 'translated' (for legacy hive) table DDL that can be run on the new cluster independently.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="1171882890#zdpiee_49" alt="schema_only" title="schema_only" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/schema_only.png" width="680" height="523"><figcaption class="center-text">schema_only</figcaption></figure></div><div class="container"><figure class="image-container"><img class="center image image-size" id="1171882890#zdpiee_50" alt="schema_only_cloud" title="schema_only_cloud" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/schema_only_cloud.png" width="680" height="533"><figcaption class="center-text">schema_only_cloud</figcaption></figure></div></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="2109940">DUMP</h1><p id="2109940#-nw11h6_3">This strategy is used to migrate &quot;only&quot; the schema (mostly). When migrating from a <span class="inline-code" id="2109940#-nw11h6_5">legacy</span> (Hive 1/2) hive environment to a <span class="inline-code" id="2109940#-nw11h6_6">non-legacy</span> (Hive 3+) <span class="inline-code" id="2109940#-nw11h6_7">hms-mirror</span> will convert tables that are &quot;managed non-transactional&quot; to &quot;external/purge&quot;. This translation is a part of the HSMM (Hive Strict Managed Migration) process that run for 'in-place' upgrades.</p><p id="2109940#-nw11h6_4"><span class="inline-code" id="2109940#-nw11h6_8">hms-mirror</span> is mostly designed for &quot;side-car&quot; migrations which involves two separate clusters.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-415535163">Options</h1><section><h2 id="-415535163#f-flip" data-toc="f-flip#dump-options.md-f-flip">-f|--flip</h2><p id="-415535163#hnaf2t_9">The configuration yaml has two clusters defined: LEFT and RIGHT. By default, all work moves from LEFT to RIGHT. If you need to perform actions from RIGHT to LEFT, use the <span class="inline-code" id="-415535163#hnaf2t_11">-f</span> option to switch these clusters. The final report will show that these clusters have been reversed.</p></section><section><h2 id="-415535163#ma-migrate-acid-or-mao-migrate-acid-only" data-toc="ma-migrate-acid-or-mao-migrate-acid-only#dump-options.md-ma-migrate-acid-or-mao-migrate-acid-only">-ma|--migrate-acid or -mao|--migrate-acid-only</h2><p id="-415535163#hnaf2t_13"><span class="inline-code" id="-415535163#hnaf2t_18">[optional artificial bucket limit]</span> (default is 2)</p><p id="-415535163#hnaf2t_14">The default behaviour is NOT to handle ACID tables. ACID tables require some additional handling. When you need to address ACID tables, use these flags to either include them OR migrate ONLY ACID tables.</p><p id="-415535163#hnaf2t_15">Migrations from legacy hive environments may have had to create tables with an unnecessary <span class="inline-code" id="-415535163#hnaf2t_19">CLUSTERED BY</span> clause to support <span class="inline-code" id="-415535163#hnaf2t_20">bucketing</span> for these ACID tables. The <span class="inline-code" id="-415535163#hnaf2t_21">artificial bucket limit</span> is an opportunity to remove those definitions for the migrated tables. For example: If you've defined a table with 2 buckets, the default <span class="inline-code" id="-415535163#hnaf2t_22">artificial bucket limit</span> is 2, so the table <span class="inline-code" id="-415535163#hnaf2t_23">CLUSTERED BY ... INTO 2 BUCKETS</span> will be removed from the definition.</p></section><section><h2 id="-415535163#mnno-migrate-non-native-only" data-toc="mnno-migrate-non-native-only#dump-options.md-mnno-migrate-non-native-only">[-mnno|--migrate-non-native-only]</h2><p id="-415535163#hnaf2t_25">See Non-Native Tables (<a href="#45251276#non-native-hive-tables-hbase-kafka-jdbc-druid-etc">&quot;Non-Native Hive Tables (Hbase, KAFKA, JDBC, Druid, etc..)&quot; in &quot;Features&quot;</a>) for more information.</p><p id="-415535163#hnaf2t_26">Use this to migrate <span class="emphasis" id="-415535163#hnaf2t_30">non-native</span> hive tables like: Kafka, HBase, JDBC Federated, etc.. This option excludes <span class="emphasis" id="-415535163#hnaf2t_31">native</span> tables from processing, so you should run it separately.</p><p id="-415535163#hnaf2t_27">Note that many of these tables <span class="control" id="-415535163#hnaf2t_32">REQUIRE</span> that the supporting references <span class="control" id="-415535163#hnaf2t_33">EXIST</span> first, or their creation will <span class="control" id="-415535163#hnaf2t_34">FAIL</span>. For instance, migrating a table with the HBase Storage handler requires that the HBase table is <span class="emphasis" id="-415535163#hnaf2t_35">present</span> AND <span class="emphasis" id="-415535163#hnaf2t_36">visible</span> from Hive.</p></section><section><h2 id="-415535163#v-views-only" data-toc="v-views-only#dump-options.md-v-views-only">[-v|--views-only]</h2><p id="-415535163#hnaf2t_38">See Views (<a href="#45251276#views">&quot;VIEWS&quot; in &quot;Features&quot;</a>) for more information.</p><p id="-415535163#hnaf2t_39">Like <span class="inline-code" id="-415535163#hnaf2t_42">-mnno</span> the <span class="inline-code" id="-415535163#hnaf2t_43">-v</span> option excludes other tables from processing, so it's designed to run along. And like the <span class="inline-code" id="-415535163#hnaf2t_44">-mnno</span> option, it too requires the supporting tables to exist, otherwise the view create statements will <span class="emphasis" id="-415535163#hnaf2t_45">fail</span>.</p></section><section><h2 id="-415535163#asm-avro-schema-migration" data-toc="asm-avro-schema-migration#dump-options.md-asm-avro-schema-migration">-asm|--avro-schema-migration</h2><p id="-415535163#hnaf2t_47">AVRO tables have a <span class="inline-code" id="-415535163#hnaf2t_56">TBLPROPERTIES</span> key <span class="inline-code" id="-415535163#hnaf2t_57">avro.schema.url</span> that identifies the supporting avro schemas location on the current cluster. When migrating AVRO tables, use this option to check for this setting. As a result, <span class="inline-code" id="-415535163#hnaf2t_58">hms-mirror</span> will parse out the location and replace it with the same relative location.</p><p id="-415535163#hnaf2t_48"><span class="inline-code" id="-415535163#hnaf2t_59">hms-mirror</span> will use the configured environments <span class="inline-code" id="-415535163#hnaf2t_60">hdfs|core-site.xml</span> files to launch an <span class="inline-code" id="-415535163#hnaf2t_61">hdfs</span> client that will copy the supporting avro schema between clusters. For this to work, the clusters MUST be visible to each other (<a href="#1958103884">Linking Cluster Storage Layers</a>).</p><p id="-415535163#hnaf2t_49">The DUMP strategy is like SCHEMA_ONLY; it just doesn't require the RIGHT cluster to be connected. Although, it does need the following settings for the RIGHT cluster to make the proper adjustments:</p><div class="detached code-block" id="-415535163#hnaf2t_50"><pre><code class="language-none">legacyHive
hcfsNamespace
hiveServer2 -&gt; partitionDiscovery</code></pre></div><p id="-415535163#hnaf2t_51">When the option <span class="inline-code" id="-415535163#hnaf2t_63">-ma</span> (migrate acid) is specified, the ACID schema's will be migrated/dumped. It is essential to know that the data for ACID tables can NOT simply be copied from one clusters hive instance to another. The data needs to be extracted to a none ACID table, then use an external table definition to read and INSERT the data into the new ACID table on the RIGHT cluster. For those that insist on trying to simply copy the data.... you've been warned ;).</p><p id="-415535163#hnaf2t_52">With the DUMP strategy, you'll have a 'translated' (for legacy hive) table DDL that can be run on the new cluster independently.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-415535163#hnaf2t_53" alt="schema_only" title="schema_only" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/schema_only.png" width="680" height="523"><figcaption class="center-text">schema_only</figcaption></figure></div><div class="container"><figure class="image-container"><img class="center image image-size" id="-415535163#hnaf2t_54" alt="schema_only_cloud" title="schema_only_cloud" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/schema_only_cloud.png" width="680" height="533"><figcaption class="center-text">schema_only_cloud</figcaption></figure></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-2049336807">LINKED</h1><p id="-2049336807#-j4twe7_3">Assumes the clusters are linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). We'll transfer the schema and leave the location as is on the new cluster.</p><p id="-2049336807#-j4twe7_4">This provides a means to test hive on the RIGHT cluster using the LEFT cluster's storage.</p><p id="-2049336807#-j4twe7_5">The <span class="inline-code" id="-2049336807#-j4twe7_8">-ma</span> (migrate acid) tables option is NOT valid in this scenario and will result in an error if specified.</p><p id="-2049336807#-j4twe7_6">WARNING: If the LOCATION element is specified in the database definition AND you use <span class="inline-code" id="-2049336807#-j4twe7_9">DROP DATABASE ... CASCADE</span> from the RIGHT cluster, YOU WILL DROP THE DATA ON THE LEFT CLUSTER even though the tables are NOT purgeable. This is the DEFAULT behavior of hive 'DROP DATABASE'. So BE CAREFUL!!!!</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1487760827">CONVERT_LINKED</h1><p id="-1487760827#z8sob2t_3">If you migrated schemas with the Linked (<a href="#-2049336807">LINKED</a>) strategy and don't want to drop the database and run SCHEMA_ONLY to adjust the locations from the LEFT to the RIGHT cluster, use this strategy to make the adjustment.</p><p id="-1487760827#z8sob2t_4">This will work only on tables that were migrated already. It will reset the location to the same relative location on the RIGHT clusters <span class="inline-code" id="-1487760827#z8sob2t_9">hcfsNamespace</span>. This will also check to see if the table was a 'legacy' managed table and set the <span class="inline-code" id="-1487760827#z8sob2t_10">external.table.purge</span> flag appropriately. Tables that are 'partitioned' will be handled differently, since each partition has a reference to the 'linked' location. Those tables will first be validated that they NOT <span class="inline-code" id="-1487760827#z8sob2t_11">external.table.purge</span>. If they are, that property will 'UNSET'. Then the table will be dropped, which will remove all the partition information. Then they'll be created again and <span class="inline-code" id="-1487760827#z8sob2t_12">MSCK</span> will be used to discover the partitions again on the RIGHT clusters namespace.</p><p id="-1487760827#z8sob2t_5">This process does NOT move data. It expects that the data was migrated in the background, usually by <span class="inline-code" id="-1487760827#z8sob2t_13">distcp</span> and has been placed in the same relative locations on the RIGHT clusters <span class="inline-code" id="-1487760827#z8sob2t_14">hcfsNameSpace</span>.</p><p id="-1487760827#z8sob2t_6">This process does NOT work for ACID tables.</p><p id="-1487760827#z8sob2t_7">AVRO table locations and SCHEMA location and definitions will be changed and copied.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="82350">SQL</h1><p id="82350#-gn6dz2_3">The <span class="control" id="82350#-gn6dz2_5">SQL</span> data strategy will use Hive SQL to move data between clusters. When the cluster don't have direct line of sight to each other and can NOT be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>), you can use options like <span class="inline-code" id="82350#-gn6dz2_7">-cs</span> or <span class="inline-code" id="82350#-gn6dz2_8">-is</span> to bridge the gap.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="82350#-gn6dz2_4" alt="sql" title="sql" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/sql_exp-imp.png" width="680" height="539"><figcaption class="center-text">sql</figcaption></figure></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="846013616">Options</h1><p id="846013616#be5lmy_3">Use <span class="inline-code" id="846013616#be5lmy_11">-ep|--export-partition-count &lt;limit&gt;</span> to set the limit for the number of partitions to use EXPORT_IMPORT. The default is 100. When a table has a partition count that exceeds this value, the SQL (<a href="#82350">SQL</a>) strategy will be used.</p><section><h2 id="846013616#sp-sql-partition-count-limit" data-toc="sp-sql-partition-count-limit#hms-mirror-sql-options.md-sp-sql-partition-count-limit">-sp,--sql-partition-count &lt;limit&gt;</h2><p id="846013616#be5lmy_14">Sets the limit for the number of partitions that the SQL (<a href="#82350">SQL</a>) strategy will process. If the value is exceeded, the process will NOT migrate the table. The default is 500 .</p><p id="846013616#be5lmy_15">To persist a higher value without specifying the <span class="inline-code" id="846013616#be5lmy_19">-sp</span> option, add the following to the config (<a href="#28311084">Default Configuration Template</a>).</p><div class="detached code-block" id="846013616#be5lmy_16"><pre><code class="language-yaml">hybrid:
  sqlPartitionLimit: &lt;limit&gt;</code></pre></div></section><section><h2 id="846013616#ma-migrate-acid-or-mao-migrate-acid-only" data-toc="ma-migrate-acid-or-mao-migrate-acid-only#hms-mirror-sql-options.md-ma-migrate-acid-or-mao-migrate-acid-only">-ma|--migrate-acid or -mao|--migrate-acid-only</h2><p id="846013616#be5lmy_22">Include ACID tables in processing.</p></section><section><h2 id="846013616#da-downgrade-acid" data-toc="da-downgrade-acid#hms-mirror-sql-options.md-da-downgrade-acid">-da|--downgrade-acid</h2><p id="846013616#be5lmy_26">Applicable only when <span class="inline-code" id="846013616#be5lmy_28">-ma|o</span> is used. This will take the ACID tables and downgrade them to <span class="emphasis" id="846013616#be5lmy_29">EXTERNAL/PURGE</span> tables. Buckets adjustments (<a href="#45251276#acid-tables">&quot;ACID Tables&quot; in &quot;Features&quot;</a>) are applicable.</p></section><section><h2 id="846013616#dc-distcp" data-toc="dc-distcp#hms-mirror-sql-options.md-dc-distcp">-dc|--distcp</h2><p id="846013616#be5lmy_32">For EXTERNAL tables, this will build a <span class="inline-code" id="846013616#be5lmy_37">distcp</span> plan that can be used to transfer the database tables.</p><p id="846013616#be5lmy_33">When the option <span class="inline-code" id="846013616#be5lmy_38">-ma|o</span> is used, ACID tables will be migrated. <span class="emphasis" id="846013616#be5lmy_39">ACID</span> tables will be converted to EXTERNAL transfer tables for the <span class="inline-code" id="846013616#be5lmy_40">distcp</span> operations. On the other cluster, the tables will be created initially as <span class="emphasis" id="846013616#be5lmy_41">EXTERNAL</span>, then transferred back over to <span class="emphasis" id="846013616#be5lmy_42">ACID</span> tables, unless <span class="inline-code" id="846013616#be5lmy_43">-da</span> is used.</p><p id="846013616#be5lmy_34">If the <span class="inline-code" id="846013616#be5lmy_44">-da</span> is used the tables will remain <span class="emphasis" id="846013616#be5lmy_45">EXTERNAL</span> on the new cluster.</p><p id="846013616#be5lmy_35">When <span class="inline-code" id="846013616#be5lmy_46">-is</span> is used, there will be <span class="inline-code" id="846013616#be5lmy_47">distcp</span> operations required on both clusters to handle data movement to/from the <span class="inline-code" id="846013616#be5lmy_48">intermediate-storage</span> area.</p></section><section><h2 id="846013616#is-intermediate-storage-or-cs-common-storage" data-toc="is-intermediate-storage-or-cs-common-storage#hms-mirror-sql-options.md-is-intermediate-storage-or-cs-common-storage">-is|--intermediate-storage or -cs|--common-storage</h2><p id="846013616#be5lmy_50">When the clusters aren't linked (<a href="#1958103884">Linking Cluster Storage Layers</a>) these two options provide a way to transfer data through an intermediate/common storage location. Each cluster must have access to these locations. These values are mutually exclusive.</p><p id="846013616#be5lmy_51"><span class="inline-code" id="846013616#be5lmy_57">--intermediate-storage</span> requires an addition transfer of data. The LEFT transfers data to the <span class="inline-code" id="846013616#be5lmy_58">-is</span> location and the RIGHT cluster uses that data and initiates a transfer from the location to the final hcfsNamespace value set for the cluster. This is a two copy migration.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="846013616#be5lmy_52" alt="intermediate" title="intermediate" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/intermediate.png" width="680" height="546"><figcaption class="center-text">intermediate</figcaption></figure></div><p id="846013616#be5lmy_53"><span class="inline-code" id="846013616#be5lmy_59">--common-storage</span> assumes the location is also the final resting place for the data. Some optimizations are possible (ACID downgrades) that reduce that times data need to move. This is a single copy migration.</p></section><section><h2 id="846013616#wd-warehouse-directory-and-ewd-external-warehouse-directory" data-toc="wd-warehouse-directory-and-ewd-external-warehouse-directory#hms-mirror-sql-options.md-wd-warehouse-directory-and-ewd-external-warehouse-directory">-wd|--warehouse-directory and -ewd|--external-warehouse-directory</h2><p id="846013616#be5lmy_61">Are used to set the <span class="emphasis" id="846013616#be5lmy_64">databases</span> default locations for managed and external tables. This overrides the systems <span class="emphasis" id="846013616#be5lmy_65">hive metastore</span> properties.</p></section><section><h2 id="846013616#rdl-reset-to-default-location" data-toc="rdl-reset-to-default-location#hms-mirror-sql-options.md-rdl-reset-to-default-location">-rdl|--reset-to-default-location</h2><p id="846013616#be5lmy_67">Regardless of where the source data <span class="emphasis" id="846013616#be5lmy_70">relative</span> location was on the filesystem, this will reset it to the default location on the new cluster.</p><p id="846013616#be5lmy_68">If <span class="inline-code" id="846013616#be5lmy_71">-dc|--distcp</span> is used, then the <span class="inline-code" id="846013616#be5lmy_72">warehouse</span> options are required in order for <span class="inline-code" id="846013616#be5lmy_73">hms-mirror</span> to build the <span class="inline-code" id="846013616#be5lmy_74">distcp</span> workplan.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1900672368">EXPORT_IMPORT</h1><p id="-1900672368#cilg4g_3">We'll use EXPORT_IMPORT to get the data to the new cluster. The default behavior requires the clusters to be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>).</p><p id="-1900672368#cilg4g_4">EXPORT to a location on the LEFT cluster where the RIGHT cluster can pick it up with IMPORT.</p><p id="-1900672368#cilg4g_5">When <span class="inline-code" id="-1900672368#cilg4g_10">-ma</span> (migrate acid) tables are specified, and the LEFT and RIGHT cluster DON'T share the same 'legacy' setting, we will NOT be able to use the EXPORT_IMPORT process due to incompatibilities between the Hive versions. We will still attempt to migrate the table definition and data by copying the data to an EXTERNAL table on the lower cluster and expose this as the source for an INSERT INTO the ACID table on the RIGHT cluster.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1900672368#cilg4g_6" alt="export_import" title="export_import" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/sql_exp-imp.png" width="680" height="539"><figcaption class="center-text">export_import</figcaption></figure></div><p id="-1900672368#cilg4g_7">If the <span class="inline-code" id="-1900672368#cilg4g_11">-is &lt;intermediate-storage-path&gt;</span> is used with this option, we will migrate data to this location and use it as a transfer point between the two clusters. Each cluster will require access (some configuration adjustment may be required) to the location. In this scenario, the clusters do NOT need to be linked.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1900672368#cilg4g_8" alt="intermediate" title="intermediate" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/intermediate.png" width="680" height="546"><figcaption class="center-text">intermediate</figcaption></figure></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-308799310">Options</h1><section><h2 id="-308799310#ep-export-partition-count-limit" data-toc="ep-export-partition-count-limit#hms-mirror-EI_Options.md-ep-export-partition-count-limit">-ep|--export-partition-count &lt;limit&gt;</h2><p id="-308799310#-pteagy_5">Sets the limit for the number of partitions to use EXPORT_IMPORT. The default is 100. When a table has a partition count that exceeds this value, the SQL (<a href="#82350">SQL</a>) strategy will be used.</p><p id="-308799310#-pteagy_6">To persist a higher value without specifying the <span class="inline-code" id="-308799310#-pteagy_10">-ep</span> option, add the following to the config (<a href="#28311084">Default Configuration Template</a>).</p><div class="detached code-block" id="-308799310#-pteagy_7"><pre><code class="language-yaml">hybrid:
  exportImportPartitionLimit: &lt;limit&gt;</code></pre></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="2145539580">HYBRID</h1><p id="2145539580#xosmbi_3">This data strategy will use a combination of EXPORT_IMPORT and SQL to move data between clusters.</p><p id="2145539580#xosmbi_4">This strategy will select either SQL (<a href="#82350">SQL</a>) or EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>) based on the table type, and table partition counts.</p><p id="2145539580#xosmbi_5">The initial check will attempt to use EXPORT_IMPORT (<a href="#-1900672368">EXPORT_IMPORT</a>). If the table is ACID, or the partition count exceeds the limit (<span class="inline-code" id="2145539580#xosmbi_10">-ep</span>), the SQL (<a href="#82350">SQL</a>) strategy will be used.</p><section><h2 id="2145539580#interesting-options" data-toc="interesting-options#HYBRID.md-interesting-options">Interesting Options</h2><p id="2145539580#xosmbi_12">When the cluster don't have direct line of sight to each other and can NOT be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>), you can use options like <span class="inline-code" id="2145539580#xosmbi_14">-cs</span> or <span class="inline-code" id="2145539580#xosmbi_15">-is</span> to bridge the gap.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1993481707">COMMON</h1><p id="1993481707#-gxdwep_3">The data storage is shared between the two clusters, and no data migration is required.</p><p id="1993481707#-gxdwep_4">Schemas are transferred using the same location.</p><div class="container"><figure class="image-container"><img class="center image image-size" id="1993481707#-gxdwep_5" alt="common" title="common" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/common.png" width="680" height="523"><figcaption class="center-text">common</figcaption></figure></div></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="745346762">STORAGE_MIGRATION</h1></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1035902883">ICEBERG_MIGRATION</h1><p id="1035902883#o8tr93_3">This process will look at Hive tables, evaluate if the table is a candidate for migration to Iceberg, and then migrate the table to Iceberg.</p><p id="1035902883#o8tr93_4">Specify <span class="inline-code" id="1035902883#o8tr93_10">-d ICEBERG_CONVERSION</span> as the DataStrategy to run the Iceberg Migration.</p><p id="1035902883#o8tr93_5">This process uses Hive SQL to build(and run) the conversion scripts. These are &quot;inplace&quot; migrations.</p><p id="1035902883#o8tr93_6">The following options apply to the Iceberg Migration:</p><ul class="list" id="1035902883#o8tr93_7" start="1"><li class="list-item" id="1035902883#o8tr93_11"><p><span class="inline-code" id="1035902883#o8tr93_13">-iv|--iceberg-version</span> - The Iceberg version to use. Default is 1. Can be 1 or 2.</p></li><li class="list-item" id="1035902883#o8tr93_12"><p><span class="inline-code" id="1035902883#o8tr93_14">-itpo|--iceberg-table-property-overrides</span> - A comma separated list of table properties to override. See the Iceberg Table Properties available here (<a href="https://iceberg.apache.org/docs/latest/configuration/#configuration">https://iceberg.apache.org/docs/latest/configuration/#configuration</a>) and CDP Properties (<a href="https://docs.cloudera.com/cdw-runtime/cloud/iceberg-how-to/topics/iceberg-table-properties.html">https://docs.cloudera.com/cdw-runtime/cloud/iceberg-how-to/topics/iceberg-table-properties.html</a>)</p></li></ul><section><h2 id="1035902883#requirements" data-toc="requirements#hms-mirror-iceberg_migration.md-requirements">Requirements</h2><ul class="list" id="1035902883#o8tr93_17" start="1"><li class="list-item" id="1035902883#o8tr93_18"><p>Requires Hive with Iceberg Support. </p><ul class="list" id="1035902883#o8tr93_19" start="1"><li class="list-item" id="1035902883#o8tr93_20"><p>CDP Private Cloud Base 7.1.9 Hive does NOT support Iceberg.</p></li><li class="list-item" id="1035902883#o8tr93_21"><p>CDP Private Cloud CDW 1.5.1 Hive does support Iceberg. You need CDW Data Services 1.5.1 or higher.</p></li><li class="list-item" id="1035902883#o8tr93_22"><p>CDP Public Cloud Hive does support Iceberg as of August 2023 (Datahub and CDW)</p></li></ul></li></ul></section><section><h2 id="1035902883#caution" data-toc="caution#hms-mirror-iceberg_migration.md-caution">Caution</h2><p id="1035902883#o8tr93_23">Make sure you know the component limitations with Iceberg Tables here (<a href="https://docs.cloudera.com/cdp-public-cloud/cloud/cdp-iceberg/topics/iceberg-in-cdp.html">https://docs.cloudera.com/cdp-public-cloud/cloud/cdp-iceberg/topics/iceberg-in-cdp.html</a>) so you aren't caught by surprise.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1731590434">Index of Settings</h1><section><h2 id="-1731590434#copy-avro-schema-urls" data-toc="copy-avro-schema-urls#Index-of-Settings.md-copy-avro-schema-urls">Copy Avro Schema Urls</h2><p id="-1731590434#m8c1g2_18"><span class="inline-code" id="-1731590434#m8c1g2_20">copyAvroSchemaUrls</span> is a boolean value that determines if the Avro schema URLs should be copied from the source to the target. This is useful if you're using Avro schemas in your source and target environments and want to maintain the same schema URLs in both environments. The default value is <span class="inline-code" id="-1731590434#m8c1g2_21">false</span>.</p><div class="detached code-block" id="-1731590434#m8c1g2_19"><pre><code class="language-yaml">copyAvroSchemaUrls: true|false</code></pre></div></section><section><h2 id="-1731590434#data-strategy" data-toc="data-strategy#Index-of-Settings.md-data-strategy">Data Strategy</h2><p id="-1731590434#m8c1g2_22"><span class="inline-code" id="-1731590434#m8c1g2_25">dataStrategy</span> identifies how/what will be migrated between clusters. The following values are supported:</p><ul class="list" id="-1731590434#m8c1g2_23" start="1"><li class="list-item" id="-1731590434#m8c1g2_26"><p>SCHEMA_ONLY</p></li><li class="list-item" id="-1731590434#m8c1g2_27"><p>SQL</p></li><li class="list-item" id="-1731590434#m8c1g2_28"><p>EXPORT_IMPORT</p></li><li class="list-item" id="-1731590434#m8c1g2_29"><p>HYBRID</p></li><li class="list-item" id="-1731590434#m8c1g2_30"><p>DUMP</p></li><li class="list-item" id="-1731590434#m8c1g2_31"><p>STORAGE_MIGRATION</p></li><li class="list-item" id="-1731590434#m8c1g2_32"><p>COMMON</p></li><li class="list-item" id="-1731590434#m8c1g2_33"><p>LINKED</p></li></ul><div class="detached code-block" id="-1731590434#m8c1g2_24"><pre><code class="language-yaml">dataStrategy: &quot;SCHEMA_ONLY|SQL|EXPORT_IMPORT|HYBRID|DUMP|STORAGE_MIGRATION|COMMON|LINKED&quot;</code></pre></div></section><section><h2 id="-1731590434#database-only" data-toc="database-only#Index-of-Settings.md-database-only">Database Only</h2><p id="-1731590434#m8c1g2_34"><span class="inline-code" id="-1731590434#m8c1g2_36">databaseOnly</span> is a boolean value that determines if only the database objects should be migrated. The default value is <span class="inline-code" id="-1731590434#m8c1g2_37">false</span>.</p><div class="detached code-block" id="-1731590434#m8c1g2_35"><pre><code class="language-yaml">databaseOnly: true|false</code></pre></div></section><section><h2 id="-1731590434#dump-test-data" data-toc="dump-test-data#Index-of-Settings.md-dump-test-data">Dump Test Data</h2><p id="-1731590434#m8c1g2_38"><span class="inline-code" id="-1731590434#m8c1g2_40">dumpTestData</span> is a boolean value that determines if test data should be dumped to the target. The default value is <span class="inline-code" id="-1731590434#m8c1g2_41">false</span>.</p><div class="detached code-block" id="-1731590434#m8c1g2_39"><pre><code class="language-yaml">dumpTestData: true|false</code></pre></div></section><section><h2 id="-1731590434#load-test-data-file" data-toc="load-test-data-file#Index-of-Settings.md-load-test-data-file">Load Test Data File</h2><p id="-1731590434#m8c1g2_42"><span class="inline-code" id="-1731590434#m8c1g2_44">loadTestDataFile</span> is a string value that identifies the file containing the test data to be loaded.</p><div class="detached code-block" id="-1731590434#m8c1g2_43"><pre><code class="language-yaml">loadTestDataFile: &quot;&lt;path_to_test_data_file&gt;&quot;</code></pre></div></section><section><h2 id="-1731590434#skip-link-check" data-toc="skip-link-check#Index-of-Settings.md-skip-link-check">Skip Link Check</h2><p id="-1731590434#m8c1g2_45"><span class="inline-code" id="-1731590434#m8c1g2_47">skipLinkCheck</span> is a boolean value that determines if the link check should be skipped. The default value is <span class="inline-code" id="-1731590434#m8c1g2_48">false</span>. Each cluster identifies an HCFS namespace and the link check will verify that the namespace is accessible from the <span class="inline-code" id="-1731590434#m8c1g2_49">hms-mirror</span> host. In addition, if the <span class="inline-code" id="-1731590434#m8c1g2_50">targetNamespace</span> is defined, the link check will check that as well.</p><div class="detached code-block" id="-1731590434#m8c1g2_46"><pre><code class="language-yaml">skipLinkCheck: true|false</code></pre></div></section><section><h2 id="-1731590434#databases" data-toc="databases#Index-of-Settings.md-databases">Databases</h2><p id="-1731590434#m8c1g2_51"><span class="inline-code" id="-1731590434#m8c1g2_53">databases</span> is a list of databases to be migrated. It works in concert with 'Warehouse Plans' to provide a list of databases to be migrated.</p><div class="detached code-block" id="-1731590434#m8c1g2_52"><pre><code class="language-yaml">databases:
  - db_name
  - db_name2</code></pre></div></section><section><h2 id="-1731590434#database-prefix" data-toc="database-prefix#Index-of-Settings.md-database-prefix">Database Prefix</h2><p id="-1731590434#m8c1g2_54"><span class="inline-code" id="-1731590434#m8c1g2_56">dbPrefix</span> is a value to pre-pend to the database name when creating the database in the target cluster. This is way to avoid conflicts with existing databases in the target cluster. The default value is an empty string.</p><div class="detached code-block" id="-1731590434#m8c1g2_55"><pre><code class="language-yaml">dbPrefix: &quot;&lt;prefix&gt;&quot;</code></pre></div></section><section><h2 id="-1731590434#database-rename" data-toc="database-rename#Index-of-Settings.md-database-rename">Database Rename</h2><p id="-1731590434#m8c1g2_57"><span class="inline-code" id="-1731590434#m8c1g2_59">dbRename</span> is a string value that identifies the new name of the database in the target cluster. This is useful for testing a single database migration to an alternate database in the target cluster. This is only valid for a single database migration.</p><div class="detached code-block" id="-1731590434#m8c1g2_58"><pre><code class="language-yaml">dbRename: &quot;&lt;new_db_name&gt;&quot;</code></pre></div></section><section><h2 id="-1731590434#execute" data-toc="execute#Index-of-Settings.md-execute">Execute</h2><p id="-1731590434#m8c1g2_60"><span class="inline-code" id="-1731590434#m8c1g2_63">execute</span> is a boolean value that determines if the migration should be executed. The default value is <span class="inline-code" id="-1731590434#m8c1g2_64">false</span>, which is the dry-run mode. In the 'dry-run' mode, all the reports are generated and none of the actual migration is done.</p><blockquote class="prompt flex bordered-element-rounded tip detached">
  <svg xmlns="http://www.w3.org/2000/svg" class="prompt-icon">
    <path d="M12.946 3.552L21.52 18.4c.424.735.33 1.6-.519 1.6H3.855c-.85 0-1.817-.865-1.392-1.6l8.573-14.848a1.103 1.103 0 0 1 1.91 0zm.545 12.948a1.5 1.5 0 1 0-1.5 1.5 1.5 1.5 0 0 0 1.5-1.5zM13 8h-2v5h2z"></path>
  </svg>
  <div class="prompt-content prompt-content-p"><p>You should ALWAYS run a 'dry-run' before executing a migration. This will give you a good idea of what will be done and provide you with the reports to review.</p></div>
</blockquote>
<div class="detached code-block" id="-1731590434#m8c1g2_62"><pre><code class="language-yaml">execute: true|false</code></pre></div></section><section><h2 id="-1731590434#migrate-non-native-tables" data-toc="migrate-non-native-tables#Index-of-Settings.md-migrate-non-native-tables">Migrate Non-Native Tables</h2><p id="-1731590434#m8c1g2_65"><span class="inline-code" id="-1731590434#m8c1g2_67">migrateNonNative</span> is a boolean value that determines if non-native tables should be migrated. The default is <span class="inline-code" id="-1731590434#m8c1g2_68">false</span>. A non-native table is a table that doesn't have a LOCATION element in the table definition. This is typical of tables in Hive that rely on other technologies to store the data. EG: HBase, Kafka, JDBC Federation, etc.</p><div class="detached code-block" id="-1731590434#m8c1g2_66"><pre><code class="language-yaml">migrateNonNative: true|false</code></pre></div></section><section><h2 id="-1731590434#output-directory" data-toc="output-directory#Index-of-Settings.md-output-directory">Output Directory</h2><p id="-1731590434#m8c1g2_69"><span class="inline-code" id="-1731590434#m8c1g2_71">outputDirectory</span> is a string value that identifies the directory where the reports will be written. The default is <span class="inline-code" id="-1731590434#m8c1g2_72">$HOME/.hms-mirror/reports</span>. When this value is defined, the reports will be written to the specified output directory with the timestamp as the 'name' of the report.</p><div class="detached code-block" id="-1731590434#m8c1g2_70"><pre><code class="language-yaml">outputDirectory: &quot;&lt;path_to_output_directory&gt;&quot;</code></pre></div></section><section><h2 id="-1731590434#encrypted-passwords" data-toc="encrypted-passwords#Index-of-Settings.md-encrypted-passwords">Encrypted Passwords</h2><p id="-1731590434#m8c1g2_73"><span class="inline-code" id="-1731590434#m8c1g2_75">encryptedPasswords</span> is a boolean value that determines if the passwords in the configuration file are encrypted.</p><div class="detached code-block" id="-1731590434#m8c1g2_74"><pre><code class="language-yaml">encryptedPasswords: true|false</code></pre></div></section><section><h2 id="-1731590434#read-only" data-toc="read-only#Index-of-Settings.md-read-only">Read-Only</h2><p id="-1731590434#m8c1g2_76"><span class="inline-code" id="-1731590434#m8c1g2_78">readOnly</span> is a boolean value that determines if the migration should be read-only. The default value is <span class="inline-code" id="-1731590434#m8c1g2_79">false</span>.  When this value is set to <span class="inline-code" id="-1731590434#m8c1g2_81">true</span>, table schema's created will not have a 'purge' flag set to ensure they can't drop data. This is useful for testing migrations and for DR scenarios where you want to limit the exposure of potential changes on the target cluster.</p><div class="detached code-block" id="-1731590434#m8c1g2_77"><pre><code class="language-yaml">readOnly: true|false</code></pre></div></section><section><h2 id="-1731590434#skip-features" data-toc="skip-features#Index-of-Settings.md-skip-features">Skip Features</h2><p id="-1731590434#m8c1g2_82"><span class="inline-code" id="-1731590434#m8c1g2_84">skipFeatures</span> is a boolean value that is <span class="inline-code" id="-1731590434#m8c1g2_85">false</span> by default so feature check will be made. Features are a framework of checks that examine a table definition and make corrections to it to ensure it's compatible with the target cluster. We've found several circumstances where definitions extracted from the source cluster can NOT be replayed on the target cluster for some reason. These features attempt to correct those issues during the migration.</p><div class="detached code-block" id="-1731590434#m8c1g2_83"><pre><code class="language-yaml">skipFeatures: true|false</code></pre></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1862306058">Filter</h1><p id="1862306058#-wlsom2_3">Start typing here...</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-332740970">Cluster</h1><p id="-332740970#meqqjo_3">Start typing here...</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1345526795">Transfer</h1><section><h2 id="1345526795#target-namespace" data-toc="target-namespace#Transfer.md-target-namespace">Target Namespace</h2><div class="detached code-block" id="1345526795#-ab8rsh_5"><pre><code class="language-yaml">transfer:
  storageMigration:</code></pre></div></section><section><h2 id="1345526795#intermediate-storage" data-toc="intermediate-storage#Transfer.md-intermediate-storage">Intermediate Storage</h2><div class="detached code-block" id="1345526795#-ab8rsh_6"><pre><code class="language-yaml">transfer:
  storageMigration:
    intermediateStorage: ...</code></pre></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="605563258">Storage Migration</h1><section><h2 id="605563258#location-translation-strategy" data-toc="location-translation-strategy#Transfer-Storage-Migration.md-location-translation-strategy">Location Translation Strategy</h2><div class="detached code-block" id="605563258#eqv680_6"><pre><code class="language-yaml">transfer:
  storageMigration:
    translationType: &quot;ALIGNED|RELATIVE&quot;</code></pre></div></section><section><h2 id="605563258#data-movement-strategy" data-toc="data-movement-strategy#Transfer-Storage-Migration.md-data-movement-strategy">Data Movement Strategy</h2><div class="detached code-block" id="605563258#eqv680_7"><pre><code class="language-yaml">transfer:
  storageMigration:
    dataMovementStrategy: &quot;SQL|DISTCP&quot;</code></pre></div></section><section><h2 id="605563258#consolidate-source-tables" data-toc="consolidate-source-tables#Transfer-Storage-Migration.md-consolidate-source-tables">Consolidate Source Tables</h2><p id="605563258#eqv680_8">Used to help define how a <span class="inline-code" id="605563258#eqv680_11">distcp</span> plan will be built when asked for. The default is <span class="inline-code" id="605563258#eqv680_12">false</span> which means that will <span class="control" id="605563258#eqv680_13">NOT</span> be consolidating table locations.</p><p id="605563258#eqv680_9">If this is set to <span class="inline-code" id="605563258#eqv680_14">true</span>, the <span class="inline-code" id="605563258#eqv680_15">distcp</span> plan will <span class="control" id="605563258#eqv680_16">remove the table location</span> from the source (which is generally the database location) and use it for all transfers. This looks more simple, but could lead to copying more data than you expect since there's no guarantee that there isn't a lot of other 'extra' data in the source location.</p><div class="detached code-block" id="605563258#eqv680_10"><pre><code class="language-yaml">transfer:
  storageMigration:
    consolidateTablesForDistcp: true|false</code></pre></div></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1272712614">Troubleshooting</h1><section><h2 id="-1272712614#application-doesn-t-seem-to-be-making-progress" data-toc="application-doesn-t-seem-to-be-making-progress#troubleshooting.md-application-doesn-t-seem-to-be-making-progress">Application doesn't seem to be making progress</h2><p id="-1272712614#has4k6_16">All the counters for table processing aren't moving (review the hms-mirror.log) or (1.6.1.0+) the on screen logging of what tables are being added and metadata collected for has stopped.</p><p id="-1272712614#has4k6_17"><span class="control" id="-1272712614#has4k6_22">Solution</span></p><p id="-1272712614#has4k6_18">The application creates a pool of connection to the HiveServer2 instances on the LEFT and RIGHT to be used for processing. When the HiveServer2 doesn't support or doesn't have available the number of connections being requested from <span class="inline-code" id="-1272712614#has4k6_23">hms-mirror</span>, the application will 'wait' forever on getting the connections requested.</p><p id="-1272712614#has4k6_19">Stop the application and lower the concurrency value to a value that can be supported.</p><div class="detached code-block" id="-1272712614#has4k6_20"><pre><code class="language-yaml">transfer:
  concurrency: 4</code></pre></div><p id="-1272712614#has4k6_21">Or, you could modify the HiveServer2 instance to handle the number of connections being requested.</p></section><section><h2 id="-1272712614#application-won-t-start-noclassdeffounderror" data-toc="application-won-t-start-noclassdeffounderror#troubleshooting.md-application-won-t-start-noclassdeffounderror">Application won't start NoClassDefFoundError</h2><p id="-1272712614#has4k6_25">Error</p><div class="detached code-block" id="-1272712614#has4k6_26"><pre><code class="language-none">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError:
java/sql/Driver at java.base/java.lang.ClassLoader.defineClass1</code></pre></div><p id="-1272712614#has4k6_27"><span class="inline-code" id="-1272712614#has4k6_31">hms-mirror</span> uses a classloader to separate the various jdbc classes (and versions) used to manage migrations between two different clusters. The application also has a requirement to run on older platforms, so the most common denominator is Java 8. Our method of loading and separating these libraries doesn't work in Java 9+.</p><p id="-1272712614#has4k6_28"><span class="control" id="-1272712614#has4k6_32">Solution</span></p><p id="-1272712614#has4k6_29">Please use Java 8 to run <span class="inline-code" id="-1272712614#has4k6_33">hms-mirror</span>.</p></section><section><h2 id="-1272712614#cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect" data-toc="cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect#troubleshooting.md-cdp-hive-standalone-driver-for-cdp-7-1-8-chf-x-cummulative-hot-fix-won-t-connect">CDP Hive Standalone Driver for CDP 7.1.8 CHF x (Cummulative Hot Fix) won't connect</h2><p id="-1272712614#has4k6_34">If you are attempting to connect to a CDP 7.1.8 clusters Hive Server 2 with the CDP Hive Standalone Driver identified in the clusters <span class="inline-code" id="-1272712614#has4k6_44">jarFile</span> property, you may not be able to connect. A security item addressed in these drivers changed the required classes.</p><p id="-1272712614#has4k6_35">If you see:</p><div class="detached code-block" id="-1272712614#has4k6_36"><pre><code class="language-none">java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/log4j/Level</code></pre></div><p id="-1272712614#has4k6_37">You will need to include additional jars in the <span class="inline-code" id="-1272712614#has4k6_45">jarFile</span> property. The following jars are required:</p><div class="detached code-block" id="-1272712614#has4k6_38"><pre><code class="language-none">log4j-1.2-api-2.18.0.ja
log4j-api-2.18.0.jar
log4j-core-2.18.0.jar</code></pre></div><p id="-1272712614#has4k6_39">The feature enhancement that allows multiple jars to be specified in the <span class="inline-code" id="-1272712614#has4k6_46">jarFile</span> property is available in <span class="inline-code" id="-1272712614#has4k6_47">hms-mirror</span> 1.6.0.0 and later. See Issue #47 (<a href="https://github.com/cloudera-labs/hms-mirror/issues/67">https://github.com/cloudera-labs/hms-mirror/issues/67</a>)</p><p id="-1272712614#has4k6_40"><span class="control" id="-1272712614#has4k6_49">Solution</span></p><p id="-1272712614#has4k6_41">Using <span class="inline-code" id="-1272712614#has4k6_50">hms-mirror</span> v1.6.0.0 or later, specify the additional jars in the <span class="inline-code" id="-1272712614#has4k6_51">jarFile</span> property. For example: <span class="inline-code" id="-1272712614#has4k6_52">jarFile: &quot;&lt;absolute_path_to&gt;/hive-jdbc-3.1.3000.7.1.8.28-1-standalone.jar:&lt;absolute_path_to&gt;/log4j-1.2-api-2.18.0.jar:&lt;absolute_path_to&gt;/log4j-api-2.18.0.jar:&lt;absolute_path_to&gt;/log4j-core-2.18.0.jar&quot;</span></p><p id="-1272712614#has4k6_42">These jar files can be found on the CDP edge node in <span class="inline-code" id="-1272712614#has4k6_53">/opt/cloudera/parcels/CDH/jars/</span>.</p><p id="-1272712614#has4k6_43">Ensure that the standalone driver is list 'FIRST' in the <span class="inline-code" id="-1272712614#has4k6_54">jarFile</span> property.</p></section><section><h2 id="-1272712614#failed-avro-table-creation" data-toc="failed-avro-table-creation#troubleshooting.md-failed-avro-table-creation">Failed AVRO Table Creation</h2><div class="detached code-block" id="-1272712614#has4k6_55"><pre><code class="language-none">Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Encountered AvroSerdeException determining schema. Returning signal schema to indicate problem: Unable to read schema from given path: /user/dstreev/test.avsc)</code></pre></div><p id="-1272712614#has4k6_56"><span class="control" id="-1272712614#has4k6_58">Solution</span></p><p id="-1272712614#has4k6_57">Validate that the 'schema' file has been copied over to the new cluster. If that has been done, check the permissions. In a non-impersonation environment (doas=false), the <span class="inline-code" id="-1272712614#has4k6_59">hive</span> user must have access to the file.</p></section><section><h2 id="-1272712614#table-processing-completed-with-error" data-toc="table-processing-completed-with-error#troubleshooting.md-table-processing-completed-with-error">Table processing completed with ERROR.</h2><p id="-1272712614#has4k6_61">We make various checks as we perform the migrations, and when those checks don't pass, the result is an error.</p><p id="-1272712614#has4k6_62"><span class="control" id="-1272712614#has4k6_66">Solution</span></p><p id="-1272712614#has4k6_63">In tips (<a href="#-1057781433">Tips</a>) we suggest running with <span class="inline-code" id="-1272712614#has4k6_68">dry-run</span> first (default). This will catch the potential issues first, without taking a whole lot of time. Use this to remediate issues before executing.</p><p id="-1272712614#has4k6_64">If the scenario that causes the <span class="inline-code" id="-1272712614#has4k6_69">ERROR</span> is known, a remediation summary will be in the output report under <span class="control" id="-1272712614#has4k6_70">Issues</span> for that table. Follow those instructions, then rerun the process with <span class="inline-code" id="-1272712614#has4k6_71">--retry.</span> NOTE: <span class="inline-code" id="-1272712614#has4k6_72">--retry</span> is currently tech preview and not thoroughly tested.</p></section><section><h2 id="-1272712614#connecting-to-hs2-via-kerberos" data-toc="connecting-to-hs2-via-kerberos#troubleshooting.md-connecting-to-hs2-via-kerberos">Connecting to HS2 via Kerberos</h2><p id="-1272712614#has4k6_73">Connecting to an HDP cluster running 2.6.5 with Binary protocol and Kerberos triggers an incompatibility issue: <span class="inline-code" id="-1272712614#has4k6_78">Unrecognized Hadoop major version number: 3.1.1.7.1....</span></p><p id="-1272712614#has4k6_74"><span class="control" id="-1272712614#has4k6_79">Solution</span></p><p id="-1272712614#has4k6_75">The application is built with CDP libraries (excluding the Hive Standalone Driver). When <span class="inline-code" id="-1272712614#has4k6_80">Kerberos is the </span>auth` protocol to connect to <span class="control" id="-1272712614#has4k6_81">Hive 1</span>, it will get the application libs which will NOT be compatible with the older cluster.</p><p id="-1272712614#has4k6_76">Kerberos connections are only supported to the CDP cluster.</p><p id="-1272712614#has4k6_77">When connecting via <span class="inline-code" id="-1272712614#has4k6_82">Kerberos, you will need to include the </span>--hadoop-classpath<span class="inline-code" id="-1272712614#has4k6_83"> when launching </span>hms-mirror`.</p></section><section><h2 id="-1272712614#auto-partition-discovery-not-working" data-toc="auto-partition-discovery-not-working#troubleshooting.md-auto-partition-discovery-not-working">Auto Partition Discovery not working</h2><p id="-1272712614#has4k6_84">I've set the <span class="inline-code" id="-1272712614#has4k6_89">partitionDiscovery:auto</span> to <span class="inline-code" id="-1272712614#has4k6_90">true,</span> but the partitions aren't getting discovered.</p><p id="-1272712614#has4k6_85"><span class="control" id="-1272712614#has4k6_91">Solution</span></p><p id="-1272712614#has4k6_86">In CDP Base/PVC versions &lt; 7.1.6 have not set the housekeeping thread that runs to activate this feature.</p><p id="-1272712614#has4k6_87">In the Hive metastore configuration in Cloudera Manager, set <span class="inline-code" id="-1272712614#has4k6_92">metastore.housekeeping.threads.on=true</span> in the <span class="emphasis" id="-1272712614#has4k6_93">Hive Service Advanced Configuration Snippet (Safety Valve) for hive-site.xml</span></p><div class="container"><figure class="image-container"><img class="center image image-size" id="-1272712614#has4k6_88" alt="pic" title="pic" src="/Users/dstreev/projects/david/hms-mirror/Writerside/topics/images/hms_housekeeping_thread.png" width="1544" height="432"><figcaption class="center-text">pic</figcaption></figure></div></section><section><h2 id="-1272712614#hive-sql-exception-hdfs-permissions-issues" data-toc="hive-sql-exception-hdfs-permissions-issues#troubleshooting.md-hive-sql-exception-hdfs-permissions-issues">Hive SQL Exception / HDFS Permissions Issues</h2><div class="detached code-block" id="-1272712614#has4k6_94"><pre><code class="language-none">Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException:Permission denied: user [dstreev] does not have [ALL] privilege on [hdfs://HDP50/apps/hive/warehouse/tpcds_bin_partitioned_orc_10.db/web_site]</code></pre></div><p id="-1272712614#has4k6_95">This error is a permission error to HDFS. For HYBRID, EXPORT_IMPORT, SQL, and SCHEMA_ONLY (with <span class="inline-code" id="-1272712614#has4k6_100">-ams</span> enabled), this could be an issue with cross-cluster HDFS access.</p><p id="-1272712614#has4k6_96">Review the output report for details of where this error occurred (LEFT or RIGHT cluster).</p><p id="-1272712614#has4k6_97">When dealing with CREATE DDL statements submitted through HS2 with a <span class="inline-code" id="-1272712614#has4k6_101">LOCATION</span> element in them, the submitting <span class="emphasis" id="-1272712614#has4k6_102">user</span> <span class="control" id="-1272712614#has4k6_103">AND</span> the HS2 <span class="emphasis" id="-1272712614#has4k6_104">service account</span> must have permissions to the directory. Remember, with cross-cluster access, the user identity will originate on the RIGHT cluster and will be <span class="control" id="-1272712614#has4k6_105">EVALUATED</span> on the LEFT clusters storage layer.</p><p id="-1272712614#has4k6_98">For migrations, the <span class="inline-code" id="-1272712614#has4k6_106">hms-mirror</span> running user (JDBC) and keytab user (HDFS) should be privileged users.</p><section><h3 id="-1272712614#example-and-ambari-hints" data-toc="example-and-ambari-hints#troubleshooting.md-example-and-ambari-hints">Example and Ambari Hints</h3><p id="-1272712614#has4k6_107">After checking permissions of 'dstreev': Found that the 'dstreev' user was NOT the owner of the files in these directories on the LEFT cluster. The user running the process needs to be in 'dfs.permissions.superusergroup' for the lower clusters 'hdfs' service. Ambari 2.6 has issues setting this property: https://jira.cloudera.com/browse/EAR-7805</p><p id="-1272712614#has4k6_108">Follow the workaround above or add the user to the 'hdfs' group. Or use Ranger to allow all access. On my cluster, with no Ranger, I had to use '/var/lib/ambari-server/resources/scripts/configs.py' to set it manually for Ambari.</p><p id="-1272712614#has4k6_109"><span class="inline-code" id="-1272712614#has4k6_110">sudo ./configs.py --host=k01.streever.local --port=8080 -u admin -p admin -n hdp50 -c hdfs-site -a set -k dfs.permissions.superusergroup -v hdfs_admin</span></p></section></section><section><h2 id="-1272712614#yarn-submission-stuck-in-accepted-phase" data-toc="yarn-submission-stuck-in-accepted-phase#troubleshooting.md-yarn-submission-stuck-in-accepted-phase">YARN Submission stuck in ACCEPTED phase</h2><p id="-1272712614#has4k6_111">The process uses a connection pool to hive. If the concurrency value for the cluster is too high, you may have reached the maximum ratio of AM (Application Masters) for the YARN queue.</p><p id="-1272712614#has4k6_112">Review the ACCEPTED jobs and review the jobs <span class="emphasis" id="-1272712614#has4k6_116">Diagnostics</span> status for details on <span class="emphasis" id="-1272712614#has4k6_117">why</span> the jobs is stuck.</p><p id="-1272712614#has4k6_113"><span class="control" id="-1272712614#has4k6_118">Solution</span></p><p id="-1272712614#has4k6_114">Either of:</p><ol class="list list-decimal" id="-1272712614#has4k6_115" type="1" start="1"><li class="list-item" id="-1272712614#has4k6_119"><p>Reduce the concurrency in the configuration file for <span class="inline-code" id="-1272712614#has4k6_121">hms-mirror</span></p></li><li class="list-item" id="-1272712614#has4k6_120"><p>Increase the AM ratio or Queue size to allow the jobs to be submitted. This can be done while the process is running.</p></li></ol></section><section><h2 id="-1272712614#spark-dfs-access" data-toc="spark-dfs-access#troubleshooting.md-spark-dfs-access">Spark DFS Access</h2><p id="-1272712614#has4k6_122">If you have problems accessing HDFS from <span class="inline-code" id="-1272712614#has4k6_124">spark-shell</span> or <span class="inline-code" id="-1272712614#has4k6_125">spark-submit</span> try adding the following configuration to spark:</p><div class="detached code-block" id="-1272712614#has4k6_123"><pre><code class="language-none">--conf spark.yarn.access.hadoopFileSystems=hdfs://&lt;NEW_NAMESPACE&gt;,hdfs://&lt;OLD_NAMESPACE&gt;</code></pre></div></section><section><h2 id="-1272712614#permission-issues" data-toc="permission-issues#troubleshooting.md-permission-issues">Permission Issues</h2><p id="-1272712614#has4k6_126"><span class="inline-code" id="-1272712614#has4k6_132">HiveAccessControlException Permission denied user: [xxxx] does not have [ALL] privileges on ['location'] [state=42000,code=40000]</span></p><p id="-1272712614#has4k6_127">and possibly</p><p id="-1272712614#has4k6_128">In HS2 Logs: <span class="inline-code" id="-1272712614#has4k6_133">Unauthorized connection for super-user</span></p><p id="-1272712614#has4k6_129"><span class="control" id="-1272712614#has4k6_134">Solution</span></p><p id="-1272712614#has4k6_130">Caused by the following:</p><ul class="list" id="-1272712614#has4k6_131" start="1"><li class="list-item" id="-1272712614#has4k6_135"><p>The 'user' doesn't have access to the location as indicated in the message. Verify through 'hdfs' that this is true or not. If the user does NOT have access, grant them access and try again.</p></li><li class="list-item" id="-1272712614#has4k6_136"><p>The 'hive' service account running HS2 does NOT have access to the location. The message will mask this and present it as a 'user' issue, when it is in fact an issue with the 'hive' service account. Grant the account the appropriate access.</p></li><li class="list-item" id="-1272712614#has4k6_137"><p>The 'hive' service does NOT have proxy permissions to the storage layer. </p><ul class="list" id="-1272712614#has4k6_138" start="1"><li class="list-item" id="-1272712614#has4k6_139"><p>Check the <span class="inline-code" id="-1272712614#has4k6_140">hadoop.proxyuser.hive.hosts|groups</span> setting in <span class="inline-code" id="-1272712614#has4k6_141">core-site.xml</span>. If you are running into this <span class="inline-code" id="-1272712614#has4k6_142">super-user</span> error on the RIGHT cluster, while trying to access a storage location on the <span class="emphasis" id="-1272712614#has4k6_143">LEFT</span> cluster, ensure the proxy settings include the rights values in the RIGHT clusters <span class="inline-code" id="-1272712614#has4k6_144">core-site.xml</span>, since that is where HS2 will pick it up from.</p></li></ul></li></ul></section><section><h2 id="-1272712614#must-use-hiveinputformat-to-read-acid-tables" data-toc="must-use-hiveinputformat-to-read-acid-tables#troubleshooting.md-must-use-hiveinputformat-to-read-acid-tables">Must use HiveInputFormat to read ACID tables</h2><p id="-1272712614#has4k6_145">We've seen this while attempting to migrate ACID tables from older clusters (HDP 2.6). The error occurs when we try to extract the ACID table data to a 'transfer' external table on the LEFT cluster, which is 'legacy'.</p><p id="-1272712614#has4k6_146"><span class="control" id="-1272712614#has4k6_149">Solution</span></p><p id="-1272712614#has4k6_147">HDP 2.6.5, the lowest supported cluster version intended for this process, should be using the 'tez' execution engine <span class="inline-code" id="-1272712614#has4k6_150">set hive.execution.engine=tez</span>. If the cluster has been upgraded from an older HDP version OR they've simply decided NOT to use the <span class="inline-code" id="-1272712614#has4k6_151">tez</span> execution engine', you may get this error.</p><p id="-1272712614#has4k6_148">In <span class="inline-code" id="-1272712614#has4k6_152">hms-mirror</span> releases 1.3.0.5 and above, we will explicitly run <span class="inline-code" id="-1272712614#has4k6_153">set hive.execution.engine=tez</span> on the LEFT cluster when identified as a 'legacy' cluster. For version 1.3.0.4 (the first version to support ACID transfers), you'll need to set the hive environment for the HS2 instance you're connecting to use <span class="inline-code" id="-1272712614#has4k6_154">tez</span> as the execution engine.</p></section><section><h2 id="-1272712614#acl-issues-across-cross-while-using-lower-clusters-storage" data-toc="acl-issues-across-cross-while-using-lower-clusters-storage#troubleshooting.md-acl-issues-across-cross-while-using-lower-clusters-storage">ACL issues across cross while using LOWER clusters storage</h2><p id="-1272712614#has4k6_155">Are you seeing something like this?</p><div class="detached code-block" id="-1272712614#has4k6_156"><pre><code class="language-none">org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=hive, access=WRITE, inode=&quot;/apps/hive/warehouse/merge_files.db/merge_files_part_a_small_replacement&quot;:dstreev:hadoop:drwxr-xr-x at</code></pre></div><p id="-1272712614#has4k6_157">This is caused when trying to <span class="inline-code" id="-1272712614#has4k6_160">CREATE</span> a table on the <span class="control" id="-1272712614#has4k6_161">RIGHT</span> cluster that references data on the <span class="control" id="-1272712614#has4k6_162">LEFT</span> cluster. When the LEFT cluster is setup differently with regard to impersonation (doas) than the RIGHT, transfer tables are created with POSIX permissions that may not allow the RIGHT cluster/user to access that location.</p><p id="-1272712614#has4k6_158"><span class="control" id="-1272712614#has4k6_163">Solution</span></p><p id="-1272712614#has4k6_159">Using Ranger on the LEFT cluster, open up the permissions to allow the requesting user access as identified.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="166757441">License APLv2</h1><div class="detached code-block" id="166757441#-7d810x_3"><pre><code class="language-none">Apache License
   Version 2.0, January 2004
http://www.apache.org/licenses/</code></pre></div><p id="166757441#-7d810x_4">TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p><ol class="list list-decimal" id="166757441#-7d810x_5" type="1" start="1"><li class="list-item" id="166757441#-7d810x_13"><p id="166757441#-7d810x_22">Definitions.</p><p id="166757441#-7d810x_23">&quot;License&quot; shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p><p id="166757441#-7d810x_24">&quot;Licensor&quot; shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p><p id="166757441#-7d810x_25">&quot;Legal Entity&quot; shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, &quot;control&quot; means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p><p id="166757441#-7d810x_26">&quot;You&quot; (or &quot;Your&quot;) shall mean an individual or Legal Entity exercising permissions granted by this License.</p><p id="166757441#-7d810x_27">&quot;Source&quot; form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p><p id="166757441#-7d810x_28">&quot;Object&quot; form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p><p id="166757441#-7d810x_29">&quot;Work&quot; shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p><p id="166757441#-7d810x_30">&quot;Derivative Works&quot; shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p><p id="166757441#-7d810x_31">&quot;Contribution&quot; shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, &quot;submitted&quot; means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as &quot;Not a Contribution.&quot;</p><p id="166757441#-7d810x_32">&quot;Contributor&quot; shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p></li><li class="list-item" id="166757441#-7d810x_14"><p id="166757441#-7d810x_33">Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p></li><li class="list-item" id="166757441#-7d810x_15"><p id="166757441#-7d810x_34">Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p></li><li class="list-item" id="166757441#-7d810x_16"><p id="166757441#-7d810x_35">Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p><p id="166757441#-7d810x_36">(a) You must give any other recipients of the Work or Derivative Works a copy of this License; and</p><p id="166757441#-7d810x_37">(b) You must cause any modified files to carry prominent notices stating that You changed the files; and</p><p id="166757441#-7d810x_38">(c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</p><p id="166757441#-7d810x_39">(d) If the Work includes a &quot;NOTICE&quot; text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</p><p id="166757441#-7d810x_40">You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p></li><li class="list-item" id="166757441#-7d810x_17"><p id="166757441#-7d810x_41">Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p></li><li class="list-item" id="166757441#-7d810x_18"><p id="166757441#-7d810x_42">Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p></li><li class="list-item" id="166757441#-7d810x_19"><p id="166757441#-7d810x_43">Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p></li><li class="list-item" id="166757441#-7d810x_20"><p id="166757441#-7d810x_44">Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p></li><li class="list-item" id="166757441#-7d810x_21"><p id="166757441#-7d810x_45">Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p></li></ol><p id="166757441#-7d810x_6">END OF TERMS AND CONDITIONS</p><p id="166757441#-7d810x_7">APPENDIX: How to apply the Apache License to your work.</p><div class="detached code-block" id="166757441#-7d810x_8"><pre><code class="language-none">To apply the Apache License to your work, attach the following
boilerplate notice, with the fields enclosed by brackets &quot;[]&quot;
replaced with your own identifying information. (Don't include
the brackets!)  The text should be enclosed in the appropriate
comment syntax for the file format. We also recommend that a
file or class name and description of purpose be included on the
same &quot;printed page&quot; as the copyright notice for easier
identification within third-party archives.</code></pre></div><p id="166757441#-7d810x_9">Copyright [yyyy] [name of copyright owner]</p><p id="166757441#-7d810x_10">Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p><div class="detached code-block" id="166757441#-7d810x_11"><pre><code class="language-none">http://www.apache.org/licenses/LICENSE-2.0</code></pre></div><p id="166757441#-7d810x_12">Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1632746627">Use Cases</h1></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="-1522123162">On Prem Legacy Hive to Non-Legacy Hive</h1><section><h2 id="-1522123162#environments" data-toc="environments#hms-mirror-legacy-to-non-legacy.md-environments">Environments</h2><p id="-1522123162#nsppqi_6">HDP 2.6 -&gt; CDP CDH 5.x -&gt; CDP CDH 6.x -&gt; CDP</p><p id="-1522123162#nsppqi_7">LEFT clusters are the LEGACY clusters. RIGHT clusters are the NON-LEGACY clusters (hive3).</p></section><section><h2 id="-1522123162#assumptions" data-toc="assumptions#hms-mirror-legacy-to-non-legacy.md-assumptions">Assumptions</h2><ul class="list" id="-1522123162#nsppqi_8" start="1"><li class="list-item" id="-1522123162#nsppqi_9"><p>LEFT HS2 is NOT Kerberized. Since this is a Legacy cluster (which is kerberized) we need to establish a 'non' kerberized HS2 endpoint. Use KNOX or setup an additional HS2 in the management console that isn't Kerberized. hms-mirror is built (default) with Hadoop 3, of which the libraries are NOT backwardly compatible.</p></li><li class="list-item" id="-1522123162#nsppqi_10"><p>Standalone JDBC jar files for the LEGACY and NON-LEGACY clusters are available to the running host as specified in the 'configuration'.</p></li><li class="list-item" id="-1522123162#nsppqi_11"><p><span class="inline-code" id="-1522123162#nsppqi_16">hms-mirror</span> is run from an EdgeNode on CDP </p><ul class="list" id="-1522123162#nsppqi_17" start="1"><li class="list-item" id="-1522123162#nsppqi_18"><p>The edgenode has network access to the Legacy HS2 endpoint</p></li></ul></li><li class="list-item" id="-1522123162#nsppqi_12"><p>No ACID tables (HDP)</p></li><li class="list-item" id="-1522123162#nsppqi_13"><p>No VIEWs</p></li><li class="list-item" id="-1522123162#nsppqi_14"><p>No Non-Native tables (Hive tables backed by HBase, JDBC, Kafka)</p></li><li class="list-item" id="-1522123162#nsppqi_15"><p>The HiveServer2's on each cluster have enough concurrency to support the configured connections <span class="inline-code" id="-1522123162#nsppqi_19">transfer-&gt;concurrency</span>. If not specified, the default is 4.</p></li></ul></section><section><h2 id="-1522123162#notes" data-toc="notes#hms-mirror-legacy-to-non-legacy.md-notes">NOTES</h2><p id="-1522123162#nsppqi_20"><span class="inline-code" id="-1522123162#nsppqi_23">hms-mirror</span> runs in DRY-RUN mode by default. Add <span class="inline-code" id="-1522123162#nsppqi_24">-e|--execute</span> to your command to actually run the process on the clusters. Use <span class="inline-code" id="-1522123162#nsppqi_25">--accept</span> to avoid the verification questions (but don't deny their meaning).</p><p id="-1522123162#nsppqi_21">All actions performed by <span class="inline-code" id="-1522123162#nsppqi_26">hms-mirror</span> are recorded in the *_execute.sql files. Review them to understand the orchestration and process.</p><p id="-1522123162#nsppqi_22">Review the report markdown files (html version also available) for details about the job. Explanations regarding steps, issues, and failure reasons can be found there.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="728268470">Scenarios</h1><section><h2 id="728268470#one-time-migration-of-schema-s-from-left-to-right" data-toc="one-time-migration-of-schema-s-from-left-to-right#hms-mirror-leg-non-leg-scenarios.md-one-time-migration-of-schema-s-from-left-to-right">One-time migration of SCHEMA's from LEFT to RIGHT.</h2><p id="728268470#-bk4fgc_15">This is done with the basic <span class="inline-code" id="728268470#-bk4fgc_19">SCHEMA_ONLY</span> data strategy (default) and will extract the schema's from the LEFT and replay them on the RIGHT cluster. In this mode, NO DATA is moved.</p><p id="728268470#-bk4fgc_16"><span class="inline-code" id="728268470#-bk4fgc_20">hms-mirror -db tpcds_bin_partitioned_orc_10 -o temp</span></p><p id="728268470#-bk4fgc_17">Examine the reports place in the relative <span class="inline-code" id="728268470#-bk4fgc_21">temp</span> directory specified with the <span class="inline-code" id="728268470#-bk4fgc_22">-o</span> option. A report set is generated to each database.</p><p id="728268470#-bk4fgc_18">Since no data is transferred in this scenario, the expectation is that the data is managed by another process, like <span class="inline-code" id="728268470#-bk4fgc_23">distcp</span>. The output of <span class="inline-code" id="728268470#-bk4fgc_24">hms-mirror</span> will create some template distcp calls with the appropriate location details. You can use this as a starting point to managed this transfer.</p></section><section><h2 id="728268470#dump-of-clusters-schema" data-toc="dump-of-clusters-schema#hms-mirror-leg-non-leg-scenarios.md-dump-of-clusters-schema">DUMP of clusters SCHEMA</h2><p id="728268470#-bk4fgc_26">We want a simple extraction of a database schema. An optional <span class="inline-code" id="728268470#-bk4fgc_31">-ds LEFT|RIGHT</span> option can be specified to target which configured cluster to use. The default is <span class="inline-code" id="728268470#-bk4fgc_32">LEFT</span>. Note: When you specify <span class="inline-code" id="728268470#-bk4fgc_33">RIGHT</span>, the DUMP output with still show up in the <span class="inline-code" id="728268470#-bk4fgc_34">LEFT</span> output report.</p><p id="728268470#-bk4fgc_27">No translations are done in this scenario. So if the DUMP is taken from a 'legacy' cluster, beware of the implications of 'replaying' it on a non-legacy cluster.</p><p id="728268470#-bk4fgc_28">This will use the LEFT cluster configuration for the schema DUMP of <span class="inline-code" id="728268470#-bk4fgc_35">tpcds_bin_partititoned_orc_10</span>. <span class="inline-code" id="728268470#-bk4fgc_36">hms-mirror -db tpcds_bin_partitioned_orc_10 -o temp -d DUMP</span></p><p id="728268470#-bk4fgc_29">This will use the RIGHT cluster configuration for the schema DUMP of <span class="inline-code" id="728268470#-bk4fgc_37">tpcds_bin_partititoned_orc_10</span>. <span class="inline-code" id="728268470#-bk4fgc_38">hms-mirror -db tpcds_bin_partitioned_orc_10 -o temp -d DUMP -ds RIGHT</span></p></section><section><h2 id="728268470#data-migration-for-non-acid-tables-using-sql" data-toc="data-migration-for-non-acid-tables-using-sql#hms-mirror-leg-non-leg-scenarios.md-data-migration-for-non-acid-tables-using-sql">Data Migration for Non-ACID tables using SQL</h2><p id="728268470#-bk4fgc_40">This approach assumes the clusters are linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). The SQL data strategy uses the RIGHT's clusters view into the HDFS filesystem of the LEFT cluster to facilitate data movement.</p><p id="728268470#-bk4fgc_41"><span class="inline-code" id="728268470#-bk4fgc_44">hms-mirror -d SQL -db tpcds_bin_partitioned_orc_10 -o temp</span></p></section><section><h2 id="728268470#data-migration-for-acid-tables-using-sql-or-hybrid" data-toc="data-migration-for-acid-tables-using-sql-or-hybrid#hms-mirror-leg-non-leg-scenarios.md-data-migration-for-acid-tables-using-sql-or-hybrid">Data Migration for ACID tables using SQL or HYBRID</h2><p id="728268470#-bk4fgc_46">This approach assumes the clusters are linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). The SQL data strategy uses the RIGHT's clusters view into the HDFS filesystem of the LEFT cluster to facilitate data movement.</p><p id="728268470#-bk4fgc_47"><span class="inline-code" id="728268470#-bk4fgc_52">hms-mirror -d SQL -db tpcds_bin_partitioned_orc_10 -o temp -ma</span></p><p id="728268470#-bk4fgc_48">Use <span class="inline-code" id="728268470#-bk4fgc_53">-mao</span> to migrate ONLY ACID tables. <span class="inline-code" id="728268470#-bk4fgc_54">-ma</span> will migrate ACID and Non-ACID tables.</p></section><section><h2 id="728268470#schema-migration-of-specific-tables-using-regex" data-toc="schema-migration-of-specific-tables-using-regex#hms-mirror-leg-non-leg-scenarios.md-schema-migration-of-specific-tables-using-regex">Schema Migration of specific tables using RegEx</h2><p id="728268470#-bk4fgc_55">Using a RegEx pattern, filter the tables in the db to migrate.</p><p id="728268470#-bk4fgc_56"><span class="inline-code" id="728268470#-bk4fgc_58">hms-mirror -db tpcds_bin_partitioned_orc_10 -tf call_*. -o temp</span></p><p id="728268470#-bk4fgc_57"><span class="inline-code" id="728268470#-bk4fgc_59">-tf|--table-filter</span> followed by a RegEx to filter tables.</p></section><section><h2 id="728268470#migrate-views-for-a-specific-database" data-toc="migrate-views-for-a-specific-database#hms-mirror-leg-non-leg-scenarios.md-migrate-views-for-a-specific-database">Migrate VIEWS for a specific database</h2><p id="728268470#-bk4fgc_60">View migration requires the underlying referenced tables exist BEFORE the 'view' can be created. This isn't a requirement of <span class="inline-code" id="728268470#-bk4fgc_62">hms-mirror</span> rather a Hive requirement. Therefore, you should migrate the tables first as shown above and in a followup process run the following.</p><p id="728268470#-bk4fgc_61"><span class="inline-code" id="728268470#-bk4fgc_63">hms-mirror -db tpcds_bin_partitioned_orc_10 -v</span></p></section><section><h2 id="728268470#create-schema-in-right-cluster-using-the-left-clusters-data" data-toc="create-schema-in-right-cluster-using-the-left-clusters-data#hms-mirror-leg-non-leg-scenarios.md-create-schema-in-right-cluster-using-the-left-clusters-data">Create schema in RIGHT cluster using the LEFT clusters data</h2><p id="728268470#-bk4fgc_64">This is a helpful scenario for 'testing' workflows on the RIGHT cluster. The tables on the right cluster will NOT be configured with 'PURGE' to avoid the deletion of data on the LEFT cluster. These tables should be considered READ-ONLY. Test this against a sample dataset to THOROUGHLY understand the relationships here. This is NOT intended for 'production' use and should be used only as a validation mechanism for the RIGHT cluster.</p><p id="728268470#-bk4fgc_65">The clusters must be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). Only legacy managed tables, external tables, and views can be linked. ACID tables can NOT be linked.</p><p id="728268470#-bk4fgc_66"><span class="inline-code" id="728268470#-bk4fgc_70">hms-mirror -d LINKED -db tpcds_bin_partitioned_orc_10 -o temp</span></p><p id="728268470#-bk4fgc_67">The tables created on the RIGHT cluster will use the data located on the LEFT cluster. The 'database' will be created to match the database in the LEFT cluster.</p><p id="728268470#-bk4fgc_68">WARNING: If the LOCATION element is specified in the database definition AND you use <span class="inline-code" id="728268470#-bk4fgc_71">DROP DATABASE ... CASCADE</span> from the RIGHT cluster, YOU WILL DROP THE DATA ON THE LEFT CLUSTER even though the tables are NOT purgeable. This is the DEFAULT behavior of hive 'DROP DATABASE'. So BE CAREFUL!!!!</p></section><section><h2 id="728268470#migrate-schema-s-and-data-using-sql" data-toc="migrate-schema-s-and-data-using-sql#hms-mirror-leg-non-leg-scenarios.md-migrate-schema-s-and-data-using-sql">Migrate SCHEMA's and Data using SQL</h2><p id="728268470#-bk4fgc_73">The clusters must be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). In this scenario, we'll use the connected clusters and SQL to migrate data from the LEFT cluster to the RIGHT.</p><p id="728268470#-bk4fgc_74">There are limits regarding partitioned tables. For SQL migrations, the default is 500. Meaning that tables with more than 500 partitions will NOT attempt this transfer. This can be changed in the 'config' file by adding/changing <span class="inline-code" id="728268470#-bk4fgc_80">hybrid-&gt;sqlPartitionLimit</span>. This was put in place as a general safeguard against attempts at tables with a partition count that may fail. It doesn't mean they'll always fail, it's just a place holder.</p><p id="728268470#-bk4fgc_75"><span class="inline-code" id="728268470#-bk4fgc_81">hms-mirror -d SQL -db tpcds_bin_partitioned_orc_10 -o temp</span></p><p id="728268470#-bk4fgc_76">To transfer ACID tables, add <span class="inline-code" id="728268470#-bk4fgc_82">-ma|-mao</span>.</p><p id="728268470#-bk4fgc_77">This is a one time transfer. Incremental updates aren't supported with this configuration. For incremental schema updates, see:</p></section><section><h2 id="728268470#migrate-schema-s-and-data-using-export-import" data-toc="migrate-schema-s-and-data-using-export-import#hms-mirror-leg-non-leg-scenarios.md-migrate-schema-s-and-data-using-export-import">Migrate SCHEMA's and Data using EXPORT_IMPORT</h2><p id="728268470#-bk4fgc_84">EXPORT/IMPORT is a basic Hive process used to package table schemas and data into a transferable unit that can be replayed on the new cluster. For <span class="inline-code" id="728268470#-bk4fgc_90">hms-mirror</span> there is a defined prefix for a transfer directory in the configuration <span class="inline-code" id="728268470#-bk4fgc_91">transfer-&gt;exportBaseDirPrefix</span>. If this isn't defined, the default is <span class="inline-code" id="728268470#-bk4fgc_92">/apps/hive/warehouse/export_</span>.</p><p id="728268470#-bk4fgc_85">There are performance implications to using EXPORT_IMPORT with partitioned tables. The IMPORT process is quite slow at loading partitions. We've defined limits in the config (which can be changed) <span class="inline-code" id="728268470#-bk4fgc_93">hybrid-&gt;exportImportPartitionLimit</span>. The default is 100. If the number of partitions exceeds this value, we will NOT attempt the transfer and will note this in the output report.</p><p id="728268470#-bk4fgc_86">The clusters must be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>). The before mentioned prefix directory on the LEFT cluster is accessed by the IMPORT process that runs on the RIGHT cluster. If the namespace (and permissions) aren't correct, the IMPORT process will fail.</p><p id="728268470#-bk4fgc_87"><span class="inline-code" id="728268470#-bk4fgc_95">hms-mirror -d EXPORT_IMPORT -db tpcds_bin_partitioned_orc_10 -o temp</span></p><p id="728268470#-bk4fgc_88">EXPORT_IMPORT will NOT work for ACIDv1 -&gt; ACIDv2 (Hive 1/2 to 3) conversions. Use <span class="inline-code" id="728268470#-bk4fgc_96">SQL</span> or <span class="inline-code" id="728268470#-bk4fgc_97">HYBRID</span> instead.</p></section><section><h2 id="728268470#migrate-schema-s-and-data-using-hybrid" data-toc="migrate-schema-s-and-data-using-hybrid#hms-mirror-leg-non-leg-scenarios.md-migrate-schema-s-and-data-using-hybrid">Migrate SCHEMA's and Data using HYBRID</h2><p id="728268470#-bk4fgc_99">The <span class="inline-code" id="728268470#-bk4fgc_105">HYBRID</span> data strategy is a combination of the <span class="inline-code" id="728268470#-bk4fgc_106">SQL</span> and <span class="inline-code" id="728268470#-bk4fgc_107">EXPORT_IMPORT</span> data strategies. It uses basic rules to choose the more appropriate method for the table in question.</p><p id="728268470#-bk4fgc_100">The clusters must be linked (<a href="#1958103884">Linking Cluster Storage Layers</a>).</p><p id="728268470#-bk4fgc_101">The process will first consider using <span class="inline-code" id="728268470#-bk4fgc_109">EXPORT_IMPORT</span> unless:</p><ul class="list" id="728268470#-bk4fgc_102" start="1"><li class="list-item" id="728268470#-bk4fgc_110"><p>The table is ACIDv1 and you're migrating to ACIDv2 (Legacy to Non-Legacy Clusters)</p></li><li class="list-item" id="728268470#-bk4fgc_111"><p>The number of partitions exceed the value set by: <span class="inline-code" id="728268470#-bk4fgc_112">hybrid-&gt;exportImportPartitionLimit</span>. The default is 100. When exceeded, the <span class="inline-code" id="728268470#-bk4fgc_113">SQL</span> method will be used. The <span class="inline-code" id="728268470#-bk4fgc_114">SQL</span> method will fail is the partition count exceeds the value of <span class="inline-code" id="728268470#-bk4fgc_115">hybrid-&gt;sqlPartitionLimit</span>. The default is 500.</p></li></ul><p id="728268470#-bk4fgc_103"><span class="inline-code" id="728268470#-bk4fgc_116">hms-mirror -d HYBRID -db tpcds_bin_partitioned_orc_10 -o temp</span></p></section><section><h2 id="728268470#disaster-recovery-right-cluster-is-dr-and-read-only" data-toc="disaster-recovery-right-cluster-is-dr-and-read-only#hms-mirror-leg-non-leg-scenarios.md-disaster-recovery-right-cluster-is-dr-and-read-only">Disaster Recovery (RIGHT Cluster is DR and READ-ONLY)</h2><p id="728268470#-bk4fgc_117">The DR scenario will transfer schemas and subsequent runs will update the 'schema' if changes are made. This process does NOT move data. The process will generate a <span class="inline-code" id="728268470#-bk4fgc_123">distcp</span> work plan for you to get started. You should modify that to use 'snapshot diffs' and managed the data migration through <span class="inline-code" id="728268470#-bk4fgc_124">distcp</span>.</p><p id="728268470#-bk4fgc_118">You should first run the process in <span class="inline-code" id="728268470#-bk4fgc_125">DRY-RUN</span> mode to get the <span class="inline-code" id="728268470#-bk4fgc_126">distcp</span> plan. The data must be transferred first! This ensures that the database and table/partition directories are created BEFORE the schemas are replayed. If the schemas are applied before the <span class="inline-code" id="728268470#-bk4fgc_127">distcp</span> with SNAPSHOT diffs, then <span class="inline-code" id="728268470#-bk4fgc_128">hive</span> will own the directories and the <span class="inline-code" id="728268470#-bk4fgc_129">distcp</span> with snapshots will fail.</p><p id="728268470#-bk4fgc_119">WARNING: Do not attempt to <span class="inline-code" id="728268470#-bk4fgc_130">DROP DATABASE ... CASCADE</span> on the RIGHT cluster, this will modify the filesystem and cause the incremental <span class="inline-code" id="728268470#-bk4fgc_131">distcp</span> with snapshots to fail.</p><p id="728268470#-bk4fgc_120"><span class="inline-code" id="728268470#-bk4fgc_132">hms-mirror -d SCHEMA_ONLY -db tpcds_bin_partitioned_orc_10 -ro -sync</span></p><p id="728268470#-bk4fgc_121">This process will review the tables on the LEFT cluster with the RIGHT and either update the schema when it's changed (by dropping and recreating), add missing tables, or drop tables that don't exist anymore.</p><p id="728268470#-bk4fgc_122">Tables that are migrated this way will NOT have the <span class="inline-code" id="728268470#-bk4fgc_133">PURGE</span> flag set on the RIGHT cluster. This allows us to <span class="inline-code" id="728268470#-bk4fgc_134">DROP</span> a table without affecting the data for the <span class="inline-code" id="728268470#-bk4fgc_135">-sync</span> process.</p></section><section><h2 id="728268470#downgrade-and-replace-acid-tables" data-toc="downgrade-and-replace-acid-tables#hms-mirror-leg-non-leg-scenarios.md-downgrade-and-replace-acid-tables">Downgrade and Replace ACID tables</h2><p id="728268470#-bk4fgc_136">In this scenario, you're choosing to downgrade ACID tables that are migrated, as well as the current tables on the source cluster.</p><p id="728268470#-bk4fgc_137"><span class="inline-code" id="728268470#-bk4fgc_140">hms-mirror -db tpcds_bin_partitioned_orc_10 -mao -da -r</span></p><p id="728268470#-bk4fgc_138">This will migrate ACID tables only (-mao vs. -ma), downgrade them to EXTERNAL/PURGE tables, and 'replace' the current ACID table with a MANAGED Non-Transactional table in legacy environments OR a EXTERNAL/PURGE table in Hive3+ environments.</p><p id="728268470#-bk4fgc_139">Using the DRY-RUN mode, experiment with options <span class="inline-code" id="728268470#-bk4fgc_141">-is</span> and <span class="inline-code" id="728268470#-bk4fgc_142">-cs</span> for various implementations of this conversion.</p></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="1005763858">Cloud to Cloud DR (AWS)</h1><p id="1005763858#lv1aqm_3">We'll cover how to manage a DR scenario where the source cluster is in the cloud and the target cluster is also in the cloud. The main elements to consider are:</p><ul class="list" id="1005763858#lv1aqm_4" start="1"><li class="list-item" id="1005763858#lv1aqm_9"><p>Hive Metadata (Tables, Databases, Views)</p></li><li class="list-item" id="1005763858#lv1aqm_10"><p>Data on S3</p></li></ul><section><h2 id="1005763858#requirements" data-toc="requirements#cloud_to_cloud_dr.md-requirements">Requirements</h2><ul class="list" id="1005763858#lv1aqm_11" start="1"><li class="list-item" id="1005763858#lv1aqm_12"><p>The source and target clusters on AWS are running CDP Cloud with an available HS2 endpoint.</p></li><li class="list-item" id="1005763858#lv1aqm_13"><p>Provide a mechanism to migrate Hive metadata from the source cluster to the target cluster, to include making adjustments to the metadata to account for differences in the clusters storage locations.</p></li><li class="list-item" id="1005763858#lv1aqm_14"><p>Establish the RPO and RTO for the DR scenario and ensure the migration process can meet those requirements.</p></li><li class="list-item" id="1005763858#lv1aqm_15"><p>We'll only target <span class="control" id="1005763858#lv1aqm_16">external</span> tables for this scenario. Managed tables will require additional considerations, since the data and metadata are intermingled and can't be supported through a simple copy operation.</p></li></ul></section><section><h2 id="1005763858#assumptions" data-toc="assumptions#cloud_to_cloud_dr.md-assumptions">Assumptions</h2><ul class="list" id="1005763858#lv1aqm_17" start="1"><li class="list-item" id="1005763858#lv1aqm_18"><p>The data on S3 is already replicated to the target cluster through some other mechanism (e.g. S3 replication, etc.).</p></li><li class="list-item" id="1005763858#lv1aqm_19"><p>The S3 replication will meet the RPO and RTO requirements.</p></li><li class="list-item" id="1005763858#lv1aqm_20"><p>The data replication is in place before DR is invoked and the scripts are run to build the schemas on the target cluster.</p></li><li class="list-item" id="1005763858#lv1aqm_21"><p>There are no managed tables being migrated. I would recommend setting the database property <span class="inline-code" id="1005763858#lv1aqm_25">&lsquo;EXTERNAL_TABLES_ONLY&rsquo;=&rsquo;TRUE&rsquo;</span> with: <span class="inline-code" id="1005763858#lv1aqm_26">ALTER DATABASE &lt;db_name&gt; SET TBLPROPERTIES ('EXTERNAL_TABLES_ONLY'='TRUE');</span> to ensure only external tables can be created.</p></li><li class="list-item" id="1005763858#lv1aqm_22"><p>Partitions follow standard naming conventions regarding directory names/structures. Tables with non-standard partitioning will require additional considerations. <span class="inline-code" id="1005763858#lv1aqm_27">hms-mirror</span> doesn't translate partition details and relies on <span class="inline-code" id="1005763858#lv1aqm_28">MSCK REPAIR &lt;table&gt; SYNC PARTITIONS</span> to discover / rebuild a tables partitions. If the partitions are not in a standard format, the <span class="inline-code" id="1005763858#lv1aqm_29">MSCK REPAIR</span> will not work and the partitions will need to be manually created.</p></li><li class="list-item" id="1005763858#lv1aqm_23"><p>We don't support schema evolution. All tables will be created in there current state.</p></li><li class="list-item" id="1005763858#lv1aqm_24"><p>The &quot;LEFT&quot; clusters <span class="inline-code" id="1005763858#lv1aqm_30">hcfsNamespace</span> can only address a single namespace at a time. If you have multiple namespaces, you'll need to run <span class="inline-code" id="1005763858#lv1aqm_31">hms-mirror</span> multiple times, once for each namespace. The &quot;RIGHT&quot; cluster can address multiple namespaces through the <span class="inline-code" id="1005763858#lv1aqm_32">hcfsNamespace</span> element. This element is used to match and adjust the storage location of the tables on the target cluster.</p></li></ul></section><section><h2 id="1005763858#the-process" data-toc="the-process#cloud_to_cloud_dr.md-the-process">The Process</h2><p id="1005763858#lv1aqm_33">The process is fairly straight forward. We'll use <span class="inline-code" id="1005763858#lv1aqm_36">hms-mirror</span> to migrate the Hive metadata from the source cluster to the target cluster. We'll use the <span class="inline-code" id="1005763858#lv1aqm_37">--common-storage</span> or set the <span class="inline-code" id="1005763858#lv1aqm_38">hcfsNamespace</span> element for the RIGHT cluster to ensure the schemas are built with the DR bucket adjustments.</p><p id="1005763858#lv1aqm_34">You have a few options regarding the transfer:</p><ul class="list" id="1005763858#lv1aqm_35" start="1"><li class="list-item" id="1005763858#lv1aqm_39"><p>If the target is truly a DR cluster, you can run <span class="inline-code" id="1005763858#lv1aqm_41">hms-mirror</span> on the source cluster and generate the metadata files locally. Then copy the metadata files to the target cluster and build out the schemas there. This doesn't need to be done until the DR is invoked.</p></li><li class="list-item" id="1005763858#lv1aqm_40"><p>If you want/need to keep the metadata in-sync between the clusters, you can run <span class="inline-code" id="1005763858#lv1aqm_42">hms-mirror</span> with the <span class="inline-code" id="1005763858#lv1aqm_43">-ro</span> and <span class="inline-code" id="1005763858#lv1aqm_44">-sync</span> flags (and eventually with <span class="inline-code" id="1005763858#lv1aqm_45">-e</span>) to keep the metadata in-sync between the clusters. Tables created on the source cluster will require the data to be replicated to the target cluster before the table can be created in DR. While we're only migrating <span class="emphasis" id="1005763858#lv1aqm_46">external</span> tables, they may have set <span class="inline-code" id="1005763858#lv1aqm_47">external.table.purge</span> to <span class="inline-code" id="1005763858#lv1aqm_48">true</span> on the source cluster. In this case, these tables will be set to <span class="control" id="1005763858#lv1aqm_49">NON</span> purge on the target cluster. This is to prevent the table data (being managed through S3 replication) from being dropped by subsequent sync runs where the tables might have changed.</p></li></ul></section><section><h2 id="1005763858#running-hms-mirror" data-toc="running-hms-mirror#cloud_to_cloud_dr.md-running-hms-mirror">Running hms-mirror</h2><section><h3 id="1005763858#configuration" data-toc="configuration#cloud_to_cloud_dr.md-configuration">Configuration</h3><p id="1005763858#lv1aqm_54">This file should be named <span class="inline-code" id="1005763858#lv1aqm_56">$HOME/.hms-mirror/cfg/default.yaml</span></p><div class="detached code-block" id="1005763858#lv1aqm_55"><pre><code class="language-yaml">clusters:
  LEFT:
    environment: &quot;LEFT&quot;
    legacyHive: false
    hcfsNamespace: &quot;s3a://&lt;my_source_s3_bucket&gt;&quot;
    hiveServer2:
      # Recommend using a KNOX endpoint to remove need for Kerberos Authentication
      uri: &quot;jdbc:hive2://&lt;my_source_hs2_endpoint&gt;&quot;
      connectionProperties:
        user: &quot;&lt;user&gt;&quot;
        maxWaitMillis: &quot;5000&quot;
        password: &quot;*****&quot;
        maxTotal: &quot;-1&quot;
      jarFile: &quot;&lt;local_location_of_hive-jdbc_driver&gt;&quot;
  RIGHT:
    environment: &quot;RIGHT&quot;
    legacyHive: false
    hcfsNamespace: &quot;s3a://&lt;my_target_s3_bucket&gt;&quot;
    hiveServer2:
      uri: &quot;jdbc:hive2://&lt;my_target_hs2_endpoint&gt;&quot;
      connectionProperties:
        user: &quot;&lt;user&gt;&quot;
        maxWaitMillis: &quot;5000&quot;
        password: &quot;*****&quot;
        maxTotal: &quot;-1&quot;
      jarFile: &quot;&lt;local_location_of_hive-jdbc_driver&gt;&quot;
    partitionDiscovery:
      # Optional, but recommended it the cluster isn't overburdened.
      auto: true
      # Required if auto is false and/or you want to ensure the partitions are in sync after the 
      # transfer is made.
      initMSCK: true</code></pre></div></section><section><h3 id="1005763858#command-lines" data-toc="command-lines#cloud_to_cloud_dr.md-command-lines">Command Lines</h3><p id="1005763858#lv1aqm_57"><span class="inline-code" id="1005763858#lv1aqm_58">hms-mirror --hadoop-classpath -d SCHEMA_ONLY -db &lt;db_comma_separated_list&gt; -ro -sync</span></p></section></section></article></div></section><section class="topic"><div><article class="article"><h1 class="main-title" id="672024080">Hybrid Data LakeHouse</h1><section><h2 id="672024080#on-prem-to-cloud-direct" data-toc="on-prem-to-cloud-direct#hms-mirror-Hybrid-Data-LakeHouse.md-on-prem-to-cloud-direct">On-Prem to Cloud (Direct)</h2></section><section><h2 id="672024080#on-prem-to-cloud-in-direct" data-toc="on-prem-to-cloud-in-direct#hms-mirror-Hybrid-Data-LakeHouse.md-on-prem-to-cloud-in-direct">On-Prem to Cloud (In-Direct)</h2></section><section><h2 id="672024080#cloud-to-on-prem-in-direct" data-toc="cloud-to-on-prem-in-direct#hms-mirror-Hybrid-Data-LakeHouse.md-cloud-to-on-prem-in-direct">Cloud to On-Prem (In-Direct)</h2></section></article></div></section></div></body></html>