<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>David W. Streever</title>
    <link>http://blog.streever.com/</link>
      <atom:link href="http://blog.streever.com/index.xml" rel="self" type="application/rss+xml" />
    <description>David W. Streever</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019</copyright><lastBuildDate>Thu, 17 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://blog.streever.com/img/icon-192.png</url>
      <title>David W. Streever</title>
      <link>http://blog.streever.com/</link>
    </image>
    
    <item>
      <title>Hadoop CLI - Intro</title>
      <link>http://blog.streever.com/post/hadoop-cli/intro/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/intro/</guid>
      <description>&lt;p&gt;Working with Hadoop is much like working with a terminal application, as most everything you do with Hadoop is via the terminal.  If you want to launch a MapReduce job, do it from the terminal.  If you wanted to explore HDFS, run a command from the terminal.&lt;/p&gt;

&lt;p&gt;Working with the Hadoop Distributed File System (HDFS) should be like working with any other file system, at least when you&amp;rsquo;re in the terminal.  Unfortunately, it&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;To do anything with HDFS, launch the command line application &lt;code&gt;hdfs&lt;/code&gt;. &lt;code&gt;hdfs&lt;/code&gt; has several sub-applications for controlling various interactions with &amp;lsquo;HDFS&amp;rsquo;.  My focus is to make the &lt;code&gt;dfs&lt;/code&gt; sub-application more amenable for &amp;lsquo;any&amp;rsquo; user.  Running &lt;code&gt;hdfs dfs -...&lt;/code&gt; for every query isn&amp;rsquo;t the experience that leaves you wanting more.  And honestly, that&amp;rsquo;s been one of Hadoop&amp;rsquo;s issues with user acceptance.  It&amp;rsquo;s an expert system, and every native interface reinforces that 10 fold.&lt;/p&gt;

&lt;p&gt;So there you have it, we&amp;rsquo;ve got a gap.  We should be able to interact with &amp;lsquo;HDFS&amp;rsquo; the same way we interact with the file system on our local computer.&lt;/p&gt;

&lt;p&gt;Five years ago, I discovered the fledgling &amp;lsquo;first&amp;rsquo; iteration of this program written by Taylor Goetz, Apache Storm PMC Chair.  The concept was great but needed some TLC.  So I forked it and have been building and improving it ever since.&lt;/p&gt;

&lt;p&gt;Finally, at least from a terminal perspective, you have the same type of interaction model with HDFS that you have with your local file system.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s not just for basic commands.  Many of the standard HDFS command-line tools are embedded right in the interface.  I&amp;rsquo;ve added scripting, Standard IN, some new commands, and sessions that are context-aware.&lt;/p&gt;

&lt;p&gt;Check out the slides at the top of this post for a brief intro tour.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HWX Ansible</title>
      <link>http://blog.streever.com/project/hwx-ansible/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hwx-ansible/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Filter Hive Compactions</title>
      <link>http://blog.streever.com/post/2019/filter-hive-compactions/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/filter-hive-compactions/</guid>
      <description>

&lt;p&gt;From Beeline or a standard JDBC client connected to Hive, compactions can be seen with the standard SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SHOW COMPACTIONS;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this method has a couple of problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;No Filtering&lt;/li&gt;
&lt;li&gt;Timestamps are hard to interpret&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Until additional functionality is available for this built in function, we can do the following.&lt;/p&gt;

&lt;p&gt;Add links to the Metastore DB tables and create custom views to review compaction details.  See &lt;a href=&#34;http://blog.streever.com/post/the-power-of-hive-jdbc-federation&#34;&gt;The Power of Hive JDBC Federation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-out-metadata-elements&#34;&gt;Build Out Metadata Elements&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE DATABASE IF NOT EXISTS custom_sys;

DROP TABLE IF EXISTS `custom_sys`.`completed_compactions`;
DROP TABLE IF EXISTS `custom_sys`.`compaction_queue`;

CREATE EXTERNAL TABLE `custom_sys`.`completed_compactions`(
	CC_ID bigint COMMENT &#39;from deserializer&#39;,
	CC_DATABASE string COMMENT &#39;from deserializer&#39;,
	CC_TABLE string COMMENT &#39;from deserializer&#39;,
	CC_PARTITION string COMMENT &#39;from deserializer&#39;,
	CC_STATE string COMMENT &#39;from deserializer&#39;,
	CC_TYPE string COMMENT &#39;from deserializer&#39;,
	CC_TBLPROPERTIES string COMMENT &#39;from deserializer&#39;,
	CC_WORKER_ID string COMMENT &#39;from deserializer&#39;,
	CC_START bigint COMMENT &#39;from deserializer&#39;,
	CC_END bigint COMMENT &#39;from deserializer&#39;,
	CC_RUN_AS string COMMENT &#39;from deserializer&#39;,
	CC_HIGHEST_WRITE_ID bigint COMMENT &#39;from deserializer&#39;,
	CC_META_INFO string COMMENT &#39;from deserializer&#39;,
	CC_HADOOP_JOB_ID string COMMENT &#39;from deserializer&#39;
)
 ROW FORMAT SERDE                                   
   &#39;org.apache.hive.storage.jdbc.JdbcSerDe&#39;         
 STORED BY                                          
   &#39;org.apache.hive.storage.jdbc.JdbcStorageHandler&#39;  
 WITH SERDEPROPERTIES (                             
   &#39;serialization.format&#39;=&#39;1&#39;)                      
 TBLPROPERTIES (                                    
   &#39;bucketing_version&#39;=&#39;2&#39;,                         
   &#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;,            
   &#39;hive.sql.query&#39;=&#39;SELECT CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_TBLPROPERTIES, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_WRITE_ID, CC_META_INFO, CC_HADOOP_JOB_ID FROM COMPLETED_COMPACTIONS&#39;);

CREATE EXTERNAL TABLE `custom_sys`.`compaction_queue`(
	CQ_ID bigint COMMENT &#39;from deserializer&#39;,
	CQ_DATABASE string COMMENT &#39;from deserializer&#39;,
	CQ_TABLE string COMMENT &#39;from deserializer&#39;,
	CQ_PARTITION string COMMENT &#39;from deserializer&#39;,
	CQ_STATE string COMMENT &#39;from deserializer&#39;,
	CQ_TYPE string COMMENT &#39;from deserializer&#39;,
	CQ_TBLPROPERTIES string COMMENT &#39;from deserializer&#39;,
	CQ_WORKER_ID string COMMENT &#39;from deserializer&#39;,
	CQ_START bigint COMMENT &#39;from deserializer&#39;,
	CQ_RUN_AS string COMMENT &#39;from deserializer&#39;,
	CQ_HIGHEST_WRITE_ID bigint COMMENT &#39;from deserializer&#39;,
	CQ_META_INFO string COMMENT &#39;from deserializer&#39;,
	CQ_HADOOP_JOB_ID string COMMENT &#39;from deserializer&#39;
)
 ROW FORMAT SERDE                                   
   &#39;org.apache.hive.storage.jdbc.JdbcSerDe&#39;         
 STORED BY                                          
   &#39;org.apache.hive.storage.jdbc.JdbcStorageHandler&#39;  
 WITH SERDEPROPERTIES (                             
   &#39;serialization.format&#39;=&#39;1&#39;)                      
 TBLPROPERTIES (                                    
   &#39;bucketing_version&#39;=&#39;2&#39;,                         
   &#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;,            
   &#39;hive.sql.query&#39;=&#39;SELECT CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_TBLPROPERTIES, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_WRITE_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID FROM COMPACTION_QUEUE&#39;);

DROP VIEW IF EXISTS `custom_sys`.`compactions`;

-- TODO: Handle / Show Aborted Transactions (maybe) I think txn data may be transient...
CREATE VIEW IF NOT EXISTS `custom_sys`.`compactions` AS
SELECT CC_ID AS id,
       CC_DATABASE AS `database`,
       CC_TABLE AS `table`,
       CC_PARTITION AS `partition`,
       CASE CC_STATE
           WHEN &#39;s&#39; THEN &#39;SUCCEEDED&#39;
           WHEN &#39;a&#39; THEN &#39;ATTEMPTED&#39;
           WHEN &#39;f&#39; THEN &#39;FAILED&#39;
       END AS STATE,
       CASE CC_TYPE
           WHEN &#39;a&#39; THEN &#39;MAJOR&#39;
           WHEN &#39;i&#39; THEN &#39;MINOR&#39;
       END AS TYPE,
       CC_TBLPROPERTIES AS tblproperties,
       CC_WORKER_ID AS worker_id,
       to_utc_timestamp(CC_START,&#39;UTC&#39;) AS `start`,
       to_utc_timestamp(CC_END, &#39;UTC&#39;) AS `end`,
       CC_RUN_AS AS run_as,
       CC_HIGHEST_WRITE_ID AS highest_write_id,
       CC_META_INFO AS meta_info,
       CC_HADOOP_JOB_ID AS hadoop_job_id
FROM `custom_sys`.`completed_compactions`
UNION ALL
SELECT CQ_ID AS id,
       CQ_DATABASE AS `database`,
       CQ_TABLE AS `table`,
       CQ_PARTITION AS `partition`,
       CASE CQ_STATE
           WHEN &#39;i&#39; THEN &#39;INITIATED&#39;
           WHEN &#39;w&#39; THEN &#39;WORKING&#39;
           WHEN &#39;r&#39; THEN &#39;READY_FOR_CLEANING&#39;
       END AS `state`,
       CASE CQ_TYPE
           WHEN &#39;a&#39; THEN &#39;MAJOR&#39;
           WHEN &#39;i&#39; THEN &#39;MINOR&#39;
       END AS `type`,
       CQ_TBLPROPERTIES AS tblproperties,
       CQ_WORKER_ID AS worker_id,
       to_utc_timestamp(CQ_START, &#39;UTC&#39;) AS `start`,
       NULL AS `end`,
               CQ_RUN_AS AS run_as,
               CQ_HIGHEST_WRITE_ID AS highest_write_id,
               CQ_META_INFO AS meta_info,
               CQ_HADOOP_JOB_ID AS hadoop_job_id
FROM `custom_sys`.`compaction_queue`;

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;see-the-last-10-compaction-events&#34;&gt;See the last 10 compaction events&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
	`database`, `table`, `partition`, `state`, `type`, `start`, `end`, hadoop_job_id 
FROM
	`custom_sys`.`compactions` 
ORDER BY `start` DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;show-the-last-10-failed-compaction-event&#34;&gt;Show the last 10 failed compaction event&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- `state` options are &#39;SUCCEEDED&#39;, &#39;ATTEMPTED&#39;, &#39;FAILED&#39;, &#39;INITIATED&#39;, &#39;WORKING&#39;, and &#39;READY_FOR_CLEANING&#39;
-- `type` options are &#39;MAJOR&#39; and &#39;MINOR&#39;
select 
`database`, `table`, `partition`, `state`, `type`, `start`, `end` 
FROM
	`custom_sys`.`compactions` 
WHERE `state` = &#39;FAILED&#39; 
ORDER BY `start` DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI</title>
      <link>http://blog.streever.com/project/hadoop-cli/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hadoop-cli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HDP3 Upgrade Planning</title>
      <link>http://blog.streever.com/project/hdp3-upgrade-planning/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hdp3-upgrade-planning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hive LLAP Calculator</title>
      <link>http://blog.streever.com/project/hive-llap-calculator/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hive-llap-calculator/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Power of Hive JDBC Federation</title>
      <link>http://blog.streever.com/post/2019/jdbc-federation/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/jdbc-federation/</guid>
      <description>

&lt;p&gt;Hive jdbc-federation is a powerful mechanism to include external sources in your hive ecosystem.   Apache Software Foundation has excellent docs detailing this feature referred to as the &lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/JdbcStorageHandler&#34; target=&#34;_blank&#34;&gt;JDBCStorageHandler&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Interesting points about this StorageHandler include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Securing Passwords - &lt;strong&gt;Protects  passwords using a &amp;lsquo;jceks&amp;rsquo; file stored on &amp;lsquo;hdfs&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Partitioning - &lt;strong&gt;Could be used as an alternate to SQOOP for importing data to Hive&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;more-hive-metadata&#34;&gt;More Hive Metadata!&lt;/h2&gt;

&lt;p&gt;Hive Metadata, previously only available via &lt;code&gt;WebHCat&lt;/code&gt; which has been removed, can be retrieved through hive&amp;rsquo;s &lt;code&gt;sys&lt;/code&gt; database.   And the &lt;code&gt;sys&lt;/code&gt; db is actually using the JDBCStorageHandler with a special tblproperty &lt;code&gt;&#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;&lt;/code&gt; used with the storage handler and &lt;code&gt;&#39;hive.sql.query&#39;=&#39;SELECT ... FROM ...&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve used this technique to expose tables that support &lt;code&gt;COMPACTION&lt;/code&gt;, &lt;code&gt;LOCKS&lt;/code&gt;, and &lt;code&gt;TRANSACTIONS&lt;/code&gt; in order to fill some gaps with the current admin functions.   See the examples below for details.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;A word of caution here.  All the &amp;lsquo;extras&amp;rsquo; your create against the metastore DB can/may break with the next release.  This isn&amp;rsquo;t a supported method of accessing metadata.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t add &lt;em&gt;new&lt;/em&gt; tables to the current &lt;code&gt;sys.db&lt;/code&gt;.  That&amp;rsquo;s bad form and could cause issue for the platform during the next upgrade cycle, especially if there are naming conflicts.  Put it somewhere else!!&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI Walk-Through</title>
      <link>http://blog.streever.com/slides/hadoop-cli/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/slides/hadoop-cli/</guid>
      <description>

&lt;h2 id=&#34;the-hadoop-cli&#34;&gt;The Hadoop CLI&lt;/h2&gt;

&lt;p&gt;A quick tour of the installation and basic usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/img/hadoopcli_icon.png&#34; alt=&#34;Hadoop CLI&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;slide-controls&#34;&gt;Slide Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt(Option) + Click&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-is-the-hadoop-cli&#34;&gt;What is the Hadoop-CLI?&lt;/h2&gt;

&lt;p&gt;It is the missing CLI for HDFS.&lt;/p&gt;

&lt;p&gt;Launch a session and benefit from an interactive CLI experience (like your local filesyste) against HDFS.&lt;/p&gt;

&lt;p&gt;It does &amp;lsquo;tab&amp;rsquo; completion, has location &amp;lsquo;context&amp;rsquo;, supports &amp;lsquo;most&amp;rsquo; hdfs commands, and has a few &amp;lsquo;nice surprises&amp;rsquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;where-to-find-it&#34;&gt;Where to Find it&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli&#34; target=&#34;_blank&#34;&gt;On Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Pre-built Releases&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Download the latest &lt;code&gt;tar.gz&lt;/code&gt; from &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Releases&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;See the &amp;lsquo;Assets&amp;rsquo; associated with a release and downland.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation as &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;sudo&lt;/code&gt; to allow it to create and install global links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget &amp;lt;release-link-from-asset-section&amp;gt;
tar xvfz hadoop.cli-&amp;lt;version&amp;gt;-3.1.tar.gz
cd hadoop-cli-3.1
sudo ./setup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it.  I&amp;rsquo;m making an assumption you have &lt;code&gt;java 8&lt;/code&gt; on the host.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;start-up&#34;&gt;Start-up&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd # to return to your home directory
hadoopcli # It will be in the global path for most standard configurations

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;hadoopcli-startup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;let-s-try-a-simple-command&#34;&gt;Let&amp;rsquo;s try a simple command&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how way didn&amp;rsquo;t specify a full path (or any path) for &lt;code&gt;ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;and-with-a-relative-reference&#34;&gt;And with a &amp;lsquo;relative&amp;rsquo; reference&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;du -h data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path for &lt;code&gt;du -h&lt;/code&gt; is relative (no preceeding &amp;lsquo;/&amp;rsquo;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;context-aware&#34;&gt;Context Aware&lt;/h2&gt;

&lt;p&gt;Both HDFS and the Local Filesystem are accessible.  HDFS is considered the primary, meaning that standard commands will be applied to it.&lt;/p&gt;

&lt;p&gt;Context is tracked for each file system, so commands without a preceeding &amp;lsquo;/&amp;rsquo; will append to the current location.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;need-help&#34;&gt;Need Help&lt;/h2&gt;

&lt;p&gt;Get a complete command reference with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get command help with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help &amp;lt;command&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details&#34;&gt;Environment Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This is a valid HDFS client, using Hadoop-HDFS libraries.  All permissions are applied to the user as if they were using &lt;code&gt;hdfs dfs -&amp;lt;cmd&amp;gt; ...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;By default, the application will look for configurations in &lt;code&gt;/etc/hadoop/conf&lt;/code&gt;.  It requires &lt;code&gt;hdfs-site.xml&lt;/code&gt; and &lt;code&gt;core-site.xml&lt;/code&gt; from your cluster.&lt;/li&gt;
&lt;li&gt;Security is applied the same way as if using &lt;code&gt;hdfs dfs&lt;/code&gt;.  Via &amp;lsquo;os username&amp;rsquo; or &amp;lsquo;kerberos&amp;rsquo;.  Get a Kerberos ticket &lt;em&gt;before&lt;/em&gt; starting the cli.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details-cont&#34;&gt;Environment Details (cont.)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Support for alternative configs(location) via commandline parameter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoopcli --config &amp;lt;alt-location&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;h3 id=&#34;i-m-always-looking-for-feedback-to-make-it-better&#34;&gt;I&amp;rsquo;m always looking for feedback to make it better.&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you find an issue, log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you think of an enhancement, please log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learning Tech - The Dunning-Kruger Effect</title>
      <link>http://blog.streever.com/post/2018/dunning-kruger-effect-technology/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2018/dunning-kruger-effect-technology/</guid>
      <description>&lt;p&gt;It happens to everyone, regardless of how long you&amp;rsquo;ve been around.  No matter your experience level, there is no escaping the &amp;lsquo;Dunning-Kruger Effect&amp;rsquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://understandinginnovation.files.wordpress.com/2015/06/dunning-kruger-0011.jpg&#34; alt=&#34;Dunning-Kruger-Effect&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Learning a new technology can be rewarding, but you have to push through the &amp;lsquo;valley of despair&amp;rsquo; first.  I&amp;rsquo;ve been around a while and a few times a year, I have to go through this.  And the process is painful.  Not only for me, but those around me as well.  They could just rename the effect, &amp;ldquo;The Grumpy Effect&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In the beginning, there&amp;rsquo;s a sense of understanding.  You feel enlighten by the concepts and totally start to understand the benefits of the new venture, but this is short lived.  The next step is actually putting this new knowledge to work.  Now you need to roll up your sleeves and actually do it.  That&amp;rsquo;s when the confidence you&amp;rsquo;ve just built up, start to crash down.  At the height of this confidence, you&amp;rsquo;re king of the world.  And soon there after, your less than the dirt on the bottom of your shoe.&lt;/p&gt;

&lt;p&gt;As you wade through the valley, everything appears foreign and distant.  The most simple concept seems to elude your every attempt at understanding.  You&amp;rsquo;ll reread a passage numerous times without so much as a hint at it&amp;rsquo;s meaning or relevance.&lt;/p&gt;

&lt;p&gt;And then it happens, the swamp starts to drain, the fog lifts and the pieces just start to fall into place.  It can take hours, days or maybe months but it will happen.  That is, if you can handle the &amp;lsquo;valley&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;After 30 years in this business, I found the number 1 asset is &amp;lsquo;persistence&amp;rsquo;.  You&amp;rsquo;ll never succeed if you give up!  PRESS ON!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>http://blog.streever.com/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>http://blog.streever.com/terms/</guid>
      <description>

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;This disclaimer (&amp;ldquo;Disclaimer&amp;rdquo;, &amp;ldquo;Agreement&amp;rdquo;) is an agreement between Website Operator (&amp;ldquo;Website Operator&amp;rdquo;, &amp;ldquo;us&amp;rdquo;, &amp;ldquo;we&amp;rdquo; or &amp;ldquo;our&amp;rdquo;) and you (&amp;ldquo;User&amp;rdquo;, &amp;ldquo;you&amp;rdquo; or &amp;ldquo;your&amp;rdquo;). This Disclaimer sets forth the general guidelines, terms and conditions of your use of the blog.streever.com website and any of its products or services (collectively, &amp;ldquo;Website&amp;rdquo; or &amp;ldquo;Services&amp;rdquo;).&lt;/p&gt;

&lt;h2 id=&#34;representation&#34;&gt;Representation&lt;/h2&gt;

&lt;p&gt;Any views or opinions represented in this Website are personal and belong solely to Website Operator and do not represent those of people, institutions or organizations that the owner may or may not be associated with in professional or personal capacity unless explicitly stated. Any views or opinions are not intended to malign any religion, ethnic group, club, organization, company, or individual.&lt;/p&gt;

&lt;h2 id=&#34;content-and-postings&#34;&gt;Content and postings&lt;/h2&gt;

&lt;p&gt;You may print a copy of any part of this Website for your personal or non-commercial use.&lt;/p&gt;

&lt;h2 id=&#34;indemnification-and-warranties&#34;&gt;Indemnification and warranties&lt;/h2&gt;

&lt;p&gt;While we have made every attempt to ensure that the information contained on the Website is correct, Website Operator is not responsible for any errors or omissions, or for the results obtained from the use of this information. All information on the Website is provided &amp;ldquo;as is&amp;rdquo;, with no guarantee of completeness, accuracy, timeliness or of the results obtained from the use of this information, and without warranty of any kind, express or implied. In no event will Website Operator be liable to you or anyone else for any decision made or action taken in reliance on the information on the Website or for any consequential, special or similar damages, even if advised of the possibility of such damages. Information on the Website is for general information purposes only and is not intended to provide legal, financial, medical, or any other type of professional advice. Please seek professional assistance should you require it. Furthermore, information contained on the Website and any pages linked to and from it are subject to change at any time and without warning.&lt;/p&gt;

&lt;p&gt;We reserve the right to modify this Disclaimer relating to the Website or Services at any time, effective upon posting of an updated version of this Disclaimer on the Website. When we do we will post a notification on the main page of our Website. Continued use of the Website after any such changes shall constitute your consent to such changes. Policy was created with WebsitePolicies.&lt;/p&gt;

&lt;h2 id=&#34;acceptance-of-this-disclaimer&#34;&gt;Acceptance of this disclaimer&lt;/h2&gt;

&lt;p&gt;You acknowledge that you have read this Disclaimer and agree to all its terms and conditions. By accessing the Website you agree to be bound by this Disclaimer. If you do not agree to abide by the terms of this Disclaimer, you are not authorized to use or access the Website.&lt;/p&gt;

&lt;h2 id=&#34;contacting-us&#34;&gt;Contacting us&lt;/h2&gt;

&lt;p&gt;If you would like to contact us to understand more about this Disclaimer or wish to contact us concerning any matter relating to it, you may send an email to david+terms@streever.com&lt;/p&gt;

&lt;p&gt;This document was last updated on October 18, 2019&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RHEL/CentOS 6 to 7 in-place upgrades with HDP</title>
      <link>http://blog.streever.com/post/2018/in-place-centos6-to-centos7-hdp/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2018/in-place-centos6-to-centos7-hdp/</guid>
      <description>

&lt;p&gt;If you are an RHEL/CentOS shop, version 7 should be your target OS for Hadoop.  But if you&amp;rsquo;re cluster has been around a while, you probably have a few machines on version 6.x.&lt;/p&gt;

&lt;p&gt;Unfortunately, RHEL / CentOS don&amp;rsquo;t offer an upgrade path from 6 to 7.  The only certified option is to wipe the OS and reinstall, which is a problem when it&amp;rsquo;s part of an active cluster.&lt;/p&gt;

&lt;p&gt;There are 3 methods for dealing with this.
- Decommission the node and replace it.
- Migrate the workloads to another cluster and rebuild.
- Hot swap the OS.&lt;/p&gt;

&lt;h2 id=&#34;decommission-method&#34;&gt;Decommission Method&lt;/h2&gt;

&lt;p&gt;Decommissioning is a process which gracefully migrates data off a data node in a controlled manner.   Thereby removing the data node from the active cluster while ensuring the integrity of the data is maintained.&lt;/p&gt;

&lt;p&gt;This approach is pretty conservative and general only practically for a small number of nodes.&lt;/p&gt;

&lt;p&gt;With replication set to 3, the default, you can only decommission two nodes at a time.  Beyond that, you risk data being unavailable for a period.  See &lt;a href=&#34;http://blog.streever.com/images/upgrades/its-your-last-block.html&#34;&gt;It&amp;rsquo;s your last block&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With rack awareness, it&amp;rsquo;s possible to do this a rack at a time.  But beware, if rack awareness wasn&amp;rsquo;t initially configured, you can experience data loss/availability issues.  Always monitor the namenode for data availability issues.&lt;/p&gt;

&lt;p&gt;While the safest approach, there are three major concerns.
- Network Bandwidth - This process will force the movement of a dataset equivalent to the used hdfs storage on the device.
- Pressure on the Namenode - Namenode resources are required to manage the movement/tracking of blocks left under replicated by the decommissioning process.
- Time - This is a lengthy process.&lt;/p&gt;

&lt;p&gt;For large clusters with hundreds of nodes, it&amp;rsquo;s unlikely you&amp;rsquo;ll be able to keep up a pace needed to replace each node within a reasonable window.&lt;/p&gt;

&lt;h2 id=&#34;migration&#34;&gt;Migration&lt;/h2&gt;

&lt;p&gt;Depending on how much hardware you have around for this process, it&amp;rsquo;s not a likely option.  If you happen to have an abundance of hardware around, you could migrate workloads from one cluster to another, over time.&lt;/p&gt;

&lt;p&gt;The difficulties with this approach:
- Requires a duplicate set of hardware
- SLDC processes for an application that are using/ingesting and building the data need to be altered.
- Consumers needs to adjust so they can use the new cluster.
- Transitions in Big Data are not &amp;ldquo;atomic&amp;rdquo;.  The volume ingested and changed on the source is usually too high to be synchronized atomically.  Further leading to risk.&lt;/p&gt;

&lt;h2 id=&#34;hot-swapping-the-os&#34;&gt;Hot swapping the OS&lt;/h2&gt;

&lt;p&gt;Sounds sexy, doesn&amp;rsquo;t it?  Well, in geek-speak it kind of is.  If you&amp;rsquo;re familiar with how Hadoop manages its filesystem, you know there are three things you need to preserve:  fsimage, edits and block pools.  When it comes down to it, if these things are safe, you can restore HDFS.&lt;/p&gt;

&lt;p&gt;To hot swap the OS, you need to have followed some best practices around component/data layouts on your hardware.  The first and most important part is that the data directories for the Namenodes, Journal Nodes and Data Nodes should be on physically separate devices from the OS.  These drives should NOT be part of any LVM controlled by the OS.  They should be simple JBOD devices, formatted as either ext4 or xfs drives.&lt;/p&gt;

&lt;p&gt;As for the remaining services on your cluster, ensure that all the persisted data elements of each service aren&amp;rsquo;t co-located with the OS drive partition.  Which includes (but not limited to): Zeppelin Notebooks, AMS, ATS, Application Data, NFS, etc..  Check for specific OS level customizations like CRONTAB, fstab, etc. on hosts and be prepared to reinstate those after rebuilding the OS.&lt;/p&gt;

&lt;p&gt;To run this process against your master nodes and continue to support operations in your cluster, the master services for HDFS, YARN, HBase, Oozie, Druid, and Hive should be configured with HA.&lt;/p&gt;

&lt;p&gt;The cluster metadata stores for Ambari, Hive, Oozie, Ranger, and Druid should be on separately managed RDBMS host(s) and aren&amp;rsquo;t a part of the OS upgrade process we&amp;rsquo;ll cover here.&lt;/p&gt;

&lt;p&gt;If the above isn&amp;rsquo;t true, you&amp;rsquo;ll need to make it accurate before you can continue with this upgrade method.&lt;/p&gt;

&lt;p&gt;This process relies on fast and consistent OS (re)builds.  For that, I suggest that you invest some time in DevOps automation in the form of Ansible, Puppet, Chef or some other provisioning toolset.  You need to be able to rebuild and host and have it ready to be added back to the cluster in under 15 minutes.  That may be aggressive, but consider what happens when a data node is marked &amp;lsquo;dead&amp;rsquo;, which will occur after 10 minutes.  A lot of traffic and resources will start to be consumed, attempting to repair the missing replicas in the system left by the missing block pool that was managed by the host you&amp;rsquo;re upgrading.  So our goal is to reduce the amount of time that the block pool on the host is out of the cluster.&lt;/p&gt;

&lt;p&gt;Some considerations before shutting the host down:
- If you want to manage YARN gracefully, you can decommission yarn through Ambari.
- We will NOT &amp;lsquo;decommission&amp;rsquo; the Data Node.
- If this is a Master with an Active Component, fail it over to the other server before starting this process.  This provides you with a bit more control over the process.&lt;/p&gt;

&lt;p&gt;With the chosen host, go ahead and shut it down. And start the OS migration process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_not_connected.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As I mentioned earlier, all persisted state information needs to be on a different physical drive(s) than the OS.  The reason for this is that we&amp;rsquo;re going to rebuild the OS partition completely.  This means formatting &lt;strong&gt;ONLY THE ROOT DEVICE&lt;/strong&gt; and reinstalling the new OS on it.  As long as the persisted stores are on other devices, our data is safe.&lt;/p&gt;

&lt;p&gt;The process for rebuilding the OS should include all major pre-requisites for a Hadoop cluster, but not limited to this list:
- THP (Transparent Huge Pages) disabled
- NNTP installed and active
- IDM integration (sssd or other)
- JDK 8 - latest version installed
- OS Patches applied
- SELinux - disabled
- Swappiness=1 for worker nodes&lt;/p&gt;

&lt;p&gt;Consult the Hortonworks &lt;a href=&#34;https://docs.hortonworks.com/HDPDocuments/Ambari-2.6.1.5/bk_ambari-installation/content/prepare_the_environment.html&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; for further details.&lt;/p&gt;

&lt;p&gt;It is &lt;strong&gt;CRITICAL&lt;/strong&gt; that the host retains the same &lt;strong&gt;FQDN&lt;/strong&gt; as before the upgrade.  If this changes, we will NOT be able to use Ambari to complete the process.  The hosts &amp;lsquo;fstab&amp;rsquo; configuration should match what was previously configured, regarding the data drives.  Don&amp;rsquo;t change mount points at this time, it will only complicate the process and setup.&lt;/p&gt;

&lt;p&gt;In addition to these, you&amp;rsquo;ll need to install the &lt;em&gt;Ambari-Agent&lt;/em&gt; on the host and configure it for the &lt;em&gt;Ambari-Server&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo ambari-agent reset ${AMBARI_SERVER_HOST}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the host rebuilt with the basics and an &lt;em&gt;Ambari-Agent&lt;/em&gt; up and running, we should be able to see the host connected to Ambari via the Hosts Page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_connected.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you have a host that has been re-associated to Ambari successfully.  In the host details section, you&amp;rsquo;ll notice how the host now identifies with version 7 of the OS.  And at this point, the host will have none of the HDP libraries installed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_detail_os7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While none of the HDP libraries are installed, you&amp;rsquo;ll notice that Ambari knows what services should be on the host.  That&amp;rsquo;s because the host FQDN matches the previous OS version of the host and when the &amp;lsquo;ambari-agent&amp;rsquo; registered back with the &amp;lsquo;ambari-server&amp;rsquo;, the host assumed the former identity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_available_not_installed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This host will also NOT have the previous system accounts for HDP, yet.  IE: hdfs, yarn, hive, etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;Ambari Server has a new feature, added in 2.6, that allows us to recover a host in this state.  From the &amp;lsquo;host&amp;rsquo; window there&amp;rsquo;s a &amp;lsquo;Host Action&amp;rsquo; button in the upper right corner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_action_button.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Select this and &amp;lsquo;Recover Host&amp;rsquo;.  Ambari will reinstall and configure all the missing services on this host, as they were before.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/recover_host_action.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re almost there, do NOT start the services yet!!!  The remounted data directories that still contain all our data (that was the whole point of this exercise) need to be fixed.  Most likely that the POSIX UID&amp;rsquo;s for the user and group assigned in the previous OS, do NOT match or even exist in the new OS.  So the data drive permissions will not be properly configured.  We need to fix this before starting the services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_data_dir_permissions_broke.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Locate each of the data directories and adjust (recursively) the ownership to match the new uid&amp;rsquo;s.  For example:
The NN data directory at &lt;code&gt;/data/0/hadoop/hdfs/namenode&lt;/code&gt;.  We need to ensure hdfs:hadoop owns the directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chown -R hdfs:hadoop /data/0/hadoop/hdfs/namenode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/images/upgrades/host_data_dir_permissions_fixed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Make these adjustments for each of the data directories on the host including, but not limited to:
- Namenode
- Secondary Namenode (when applicable)
- Journal Node
- ZooKeeper
- ATS
- Yarn (local and logs)
- Datanode (Block Pools)
- Log Directories (if these were on separate drives and survived the OS rebuild)
- Pid Directories (if these were on separate drives and survived the OS rebuild)&lt;/p&gt;

&lt;p&gt;Now that we&amp;rsquo;ve fixed the permissions, we can &lt;strong&gt;restart&lt;/strong&gt; the services on the host.&lt;/p&gt;

&lt;h3 id=&#34;validate&#34;&gt;Validate&lt;/h3&gt;

&lt;p&gt;At this point, you should validate the services are operating as expected.  The validation should be thorough to ensure the host is a healthy member of the cluster &lt;strong&gt;BEFORE&lt;/strong&gt; proceeding on to the next host.&lt;/p&gt;

&lt;h3 id=&#34;some-known-gotcha-s&#34;&gt;Some known gotcha&amp;rsquo;s&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Not all running jobs are tolerant to failed nodes.  For example: a long running Spark task that dies because the host was removed, may not recover very well.  In which case, the job may need to be restarted.&lt;/li&gt;
&lt;li&gt;When cycling through the master services, dropping a Hive Server 2 host will cause job failures for those connected to that host.  If you consumers are using the ZooKeeper discovery protocol and you have multiple HiveServer2 instances, they&amp;rsquo;ll be routed to a working HiveServer2.  Stagger the replacement of these HiveServer2 nodes to avoid chasing active connections down and killing them in short order.&lt;/li&gt;
&lt;li&gt;Some master services that aren&amp;rsquo;t HA will experience downtime as they go through this process.  I suggest that the process be managed more carefully across the master nodes, opposed to the worker nodes.&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;re running LLAP, ensure the queue has enough room to support a missing host.  LLAP daemons will attempt to restart, when a host is brought down and will need a place to go.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;automation&#34;&gt;Automation&lt;/h3&gt;

&lt;p&gt;Now that you&amp;rsquo;ve seen how this can be done, it&amp;rsquo;s time to automate this.  Using your companies DevOps automation tools and practices, you should be able to cycle through a host upgrade every 15-30 minutes.  You should be able to cycle through all the worker nodes in record time.&lt;/p&gt;

&lt;p&gt;We briefly discussed a DevOps model to automate the OS rebuild.  Another automation measure to consider is through Ambari.&lt;/p&gt;

&lt;p&gt;If you have several hundred nodes to do this to, you&amp;rsquo;ll want to investigate the Ambari REST API and automate the actions we&amp;rsquo;ve performed through the Ambari UI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s Your Last Block!</title>
      <link>http://blog.streever.com/post/2018/its-your-last-block/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2018/its-your-last-block/</guid>
      <description>&lt;p&gt;In a few other articles, I&amp;rsquo;ll talk about bringing hosts down to facilitate upgrades.&lt;/p&gt;

&lt;p&gt;If you do this at rack at a time, it may also lead to data loss if any other drive in the system fails during the operation.  That&amp;rsquo;s because the rules for replication when setting to 3 will place the second and third blocks on the same rack.  The first block becomes the only viable source.  If a drive fails, that contains this single block, the file will become corrupt.  Restoring the data directories will fix the block issue, restoring the integrity of the file.  But, jobs that depend on this file during the time it was corrupt/unavailable will fail.&lt;/p&gt;

&lt;p&gt;If critical jobs can&amp;rsquo;t handle this risk, identify the supporting files/directories and increase the replication factor for the duration of the upgrade process to ensure you have adequate replicas available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://blog.streever.com/post/hadoop-cli/scripting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/scripting/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
