<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devops | David W. Streever</title>
    <link>http://blog.streever.com/categories/devops/</link>
      <atom:link href="http://blog.streever.com/categories/devops/index.xml" rel="self" type="application/rss+xml" />
    <description>devops</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019</copyright><lastBuildDate>Sat, 19 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://blog.streever.com/img/icon-192.png</url>
      <title>devops</title>
      <link>http://blog.streever.com/categories/devops/</link>
    </image>
    
    <item>
      <title>Hadoop CLI for HDFS Directory Cleanup</title>
      <link>http://blog.streever.com/post/2019/cleaning-up-hdfs/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/cleaning-up-hdfs/</guid>
      <description>

&lt;p&gt;Refer to the &amp;lsquo;project&amp;rsquo; and &amp;lsquo;slides&amp;rsquo; above for details on &amp;lsquo;Hadoop CLI&amp;rsquo;.  See the &amp;lsquo;related&amp;rsquo; posts at the end of this post for other information.&lt;/p&gt;

&lt;p&gt;With pipelining in &amp;lsquo;hadoopcli&amp;rsquo; we can combine search functions with cleanup functions.  So what use to take hours to prep and setup, can be accomplished in seconds.&lt;/p&gt;

&lt;h3 id=&#34;recommendation&#34;&gt;Recommendation&lt;/h3&gt;

&lt;p&gt;These operations are &amp;lsquo;destructive&amp;rsquo;, so take precautions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Test the pipeline results before call a destructive operation.&lt;/li&gt;
&lt;li&gt;Create an &amp;lsquo;hdfs snapshot&amp;rsquo; of the target base directory in case something doesn&amp;rsquo;t go as planned. See &lt;a href=&#34;http://blog.streever.com/terms&#34;&gt;Terms&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sample-use-case&#34;&gt;Sample Use case&lt;/h3&gt;

&lt;p&gt;Cleaning up hive directories left behind by some hive bugs in &amp;lsquo;stat&amp;rsquo; collection or &lt;code&gt;insert overwrite&lt;/code&gt; operations.&lt;/p&gt;

&lt;p&gt;In the &amp;lsquo;hadoopcli&amp;rsquo; application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Let&#39;s test the command.
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path
/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db/catalog_sales/.hive-staging_hive_2019-03-07_11-43-26_995_5066067470456199362-32
/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db/catalog_sales/.hive-staging_hive_2019-03-07_12-26-53_012_3155309838210900907-4

# Now we see the results, let&#39;s action on it with a pipe to rm -r -f
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/
tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path | rm -r -f

# That would&#39;ve deleted the directories listed above.
# Now let&#39;s try it again, without the pipe and validate the directories are gone.
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few notes about this.&lt;br /&gt;
- Since the &amp;lsquo;.hive-staging.*&amp;rsquo; directories are prefixed with a &amp;lsquo;.&amp;rsquo; they&amp;rsquo;re considered &lt;em&gt;invisible&lt;/em&gt;.  So use the &lt;code&gt;-i&lt;/code&gt; option to find them.
- Use the &lt;code&gt;-R&lt;/code&gt; to support recursion into the directory tree.
- Output just the &amp;lsquo;path&amp;rsquo; with the &lt;code&gt;-f&lt;/code&gt; option because the whole output line is what will be passed into the &lt;code&gt;rm -r -f&lt;/code&gt; command.
- Use the &lt;code&gt;-do&lt;/code&gt; &amp;lsquo;directory-only&amp;rsquo; option to output just directories.&lt;/p&gt;

&lt;p&gt;If you want to do a little more investigation before &lt;code&gt;rm&lt;/code&gt;, Try piping to &lt;code&gt;count -h&lt;/code&gt; for a count and size of Directories and Files.&lt;/p&gt;

&lt;h3 id=&#34;other-use-cases&#34;&gt;Other Use Cases&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Cleaning up Job History&lt;/li&gt;
&lt;li&gt;Cleaning up Spark History&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;last-note&#34;&gt;Last Note&lt;/h2&gt;

&lt;p&gt;Doing things in mass is great for productivity.  But equally as dangerous when things go wrong.  Protect yourself!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI &#39;lsp&#39; Function</title>
      <link>http://blog.streever.com/post/hadoop-cli/lsp/</link>
      <pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/lsp/</guid>
      <description>

&lt;p&gt;Like &amp;lsquo;ls&amp;rsquo;, you can fetch many details about a file.  But with this, you can also add information about the file that includes:
- Block Size
- Access Time
- Ratio of File Size to Block
- Datanode information for the files blocks (Host and Block Id)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lsp&lt;/code&gt; can be used to search hdfs, similar to the &lt;code&gt;find&lt;/code&gt; linux program.  Although the syntax is a bit different.  Use options &lt;code&gt;-F&lt;/code&gt;,&lt;code&gt;-Fe&lt;/code&gt;, and &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lsp&lt;/code&gt; can be used to output a formatted row for files and directories using the &lt;code&gt;-f&lt;/code&gt; option.  When the &lt;code&gt;datanode_info&lt;/code&gt; option is specified, the output will contain details for each replicated block of a file.&lt;/p&gt;

&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;usage: lsp [OPTION ...] [ARGS ...]
Options:
 -c,--comment &amp;lt;comment&amp;gt;                  Add comment to output
 -d,--maxDepth &amp;lt;maxDepth&amp;gt;                Depth of Recursion (default 5),
                                         use &#39;-1&#39; for unlimited
 -do,--dir-only                          Show Directories Only
 -f,--format &amp;lt;output-format&amp;gt;             Comma separated list of one or
                                         more:
                                         permissions_long,replication,user
                                         ,group,size,block_size,ratio,mod,
                                         access,path,file,datanode_info
                                         (default all of the above)
 -F,--filter &amp;lt;filter&amp;gt;                    Regex Filter of Content. Can be
                                         &#39;Quoted&#39;
 -Fe,--filter-element &amp;lt;filter element&amp;gt;   Filter on &#39;element&#39;.  One of
                                         &#39;--format&#39;
 -i,--invisible                          Process Invisible
                                         Files/Directories
 -n,--newline &amp;lt;newline&amp;gt;                  New Line
 -o,--output &amp;lt;output directory&amp;gt;          Output Directory (HDFS) (default
                                         System.out)
 -R,--recursive                          Process Path Recursively
 -r,--relative                           Show Relative Path Output
 -s,--separator &amp;lt;separator&amp;gt;              Field Separator
 -sp,--show-parent                       For Test, show parent
 -t,--test                               Test for existence
 -v,--invert                             Invert Regex Filter of Content
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;actions&#34;&gt;Actions&lt;/h2&gt;

&lt;h3 id=&#34;recursion&#34;&gt;Recursion&lt;/h3&gt;

&lt;p&gt;Use the &lt;code&gt;-R&lt;/code&gt; to recurse through directories.  Use the &lt;code&gt;-d&lt;/code&gt; option to specify the depth of the recursion.  The default is 5.  Use -1 for no-limit, but be careful because this could iterate through the whole filesystem.  And that&amp;rsquo;s not productive on &amp;lsquo;large&amp;rsquo; filesystem.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Control the depth of the recursion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -d 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;filtering-and-output&#34;&gt;Filtering and Output&lt;/h3&gt;

&lt;p&gt;Find files matching a certain pattern, recursively from path context. The &lt;code&gt;-F&lt;/code&gt; option takes a &amp;lsquo;regex&amp;rsquo; expression.  By default, the &amp;lsquo;path&amp;rsquo; &lt;code&gt;-f&lt;/code&gt; option is searched.  If you&amp;rsquo;d like to search another element use &lt;code&gt;-Fe&lt;/code&gt; and specify one of the valid &lt;code&gt;-f&lt;/code&gt; options.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output will include the standard output &lt;code&gt;-f&lt;/code&gt;, which maybe quite verbose.&lt;/p&gt;

&lt;p&gt;Limit the result output like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.* -f path
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Try using the &lt;code&gt;-do&lt;/code&gt; option to list ONLY directories.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.* -f path -do
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-comments&#34;&gt;Using Comments&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;-c&lt;/code&gt; option will prepend any output with the comment.  It&amp;rsquo;s a great way to drive scripts from the results.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -c &#39;count -h&#39; -F .*trans.* -f path -do
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;how-about-an-inverted-search&#34;&gt;How about an Inverted Search&lt;/h3&gt;

&lt;p&gt;There are times you want to find files that do &lt;em&gt;NOT&lt;/em&gt; match a pattern. Use the &lt;code&gt;-v&lt;/code&gt; option to reverse match on the filter.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -F *.trans.* -v
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;default-output&#34;&gt;Default Output&lt;/h3&gt;

&lt;p&gt;When not argument is specified, it will use the current directory.&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Using the default format, output a listing to the files in `/user/dstreev/perf` to `/tmp/test.out`
lsp -o /tmp/test.out /user/dstreev/perf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output with the default format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;permissions_long,replication,user,group,size,block_size,ratio,mod,access,path,datanode_info
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   rw-------,3,dstreev,hdfs,429496700,134217728,3.200,2015-10-24 12:26:39.689,2015-10-24 12:23:27.406,/user/dstreev/perf/teragen_27/part-m-00004,10.0.0.166,d2.hdp.local,blk_1073747900
   rw-------,3,dstreev,hdfs,429496700,134217728,3.200,2015-10-24 12:26:39.689,2015-10-24 12:23:27.406,/user/dstreev/perf/teragen_27/part-m-00004,10.0.0.167,d3.hdp.local,blk_1073747900
   rw-------,3,dstreev,hdfs,33,134217728,2.459E-7,2015-10-24 12:27:09.134,2015-10-24 12:27:06.560,/user/dstreev/perf/terasort_27/_partition.lst,10.0.0.166,d2.hdp.local,blk_1073747909
   rw-------,3,dstreev,hdfs,33,134217728,2.459E-7,2015-10-24 12:27:09.134,2015-10-24 12:27:06.560,/user/dstreev/perf/terasort_27/_partition.lst,10.0.0.167,d3.hdp.local,blk_1073747909
   rw-------,1,dstreev,hdfs,543201700,134217728,4.047,2015-10-24 12:29:28.706,2015-10-24 12:29:20.882,/user/dstreev/perf/terasort_27/part-r-00002,10.0.0.167,d3.hdp.local,blk_1073747920
   rw-------,1,dstreev,hdfs,543201700,134217728,4.047,2015-10-24 12:29:28.706,2015-10-24 12:29:20.882,/user/dstreev/perf/terasort_27/part-r-00002,10.0.0.167,d3.hdp.local,blk_1073747921
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the file in HDFS, you can build a &lt;a href=&#34;./src/main/hive/lsp.ddl&#34;&gt;hive table&lt;/a&gt; on top of it to do some analysis.  One of the reasons I created this was to be able to review a directory used by some process and get a baring on the file construction and distribution across the cluster.&lt;/p&gt;

&lt;h4 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;The ratio can be used to identify files that are below the block size (small files).&lt;/li&gt;
&lt;li&gt;With the Datanode information, you can determine if a dataset is hot-spotted on a cluster.  All you need is a full list of hosts to join the results with.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI - Intro</title>
      <link>http://blog.streever.com/post/hadoop-cli/intro/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/intro/</guid>
      <description>&lt;p&gt;Working with Hadoop is much like working with a terminal application, as most everything you do with Hadoop is via the terminal.  If you want to launch a MapReduce job, do it from the terminal.  If you wanted to explore HDFS, run a command from the terminal.&lt;/p&gt;

&lt;p&gt;Working with the Hadoop Distributed File System (HDFS) should be like working with any other file system, at least when you&amp;rsquo;re in the terminal.  Unfortunately, it&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;To do anything with HDFS, launch the command line application &lt;code&gt;hdfs&lt;/code&gt;. &lt;code&gt;hdfs&lt;/code&gt; has several sub-applications for controlling various interactions with &amp;lsquo;HDFS&amp;rsquo;.  My focus is to make the &lt;code&gt;dfs&lt;/code&gt; sub-application more amenable for &amp;lsquo;any&amp;rsquo; user.  Running &lt;code&gt;hdfs dfs -...&lt;/code&gt; for every query isn&amp;rsquo;t the experience that leaves you wanting more.  And honestly, that&amp;rsquo;s been one of Hadoop&amp;rsquo;s issues with user acceptance.  It&amp;rsquo;s an expert system, and every native interface reinforces that 10 fold.&lt;/p&gt;

&lt;p&gt;So there you have it, we&amp;rsquo;ve got a gap.  We should be able to interact with &amp;lsquo;HDFS&amp;rsquo; the same way we interact with the file system on our local computer.&lt;/p&gt;

&lt;p&gt;Five years ago, I discovered the fledgling &amp;lsquo;first&amp;rsquo; iteration of this program written by Taylor Goetz, Apache Storm PMC Chair.  The concept was great but needed some TLC.  So I forked it and have been building and improving it ever since.&lt;/p&gt;

&lt;p&gt;Finally, at least from a terminal perspective, you have the same type of interaction model with HDFS that you have with your local file system.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s not just for basic commands.  Many of the standard HDFS command-line tools are embedded right in the interface.  I&amp;rsquo;ve added scripting, Standard IN, some new commands, and sessions that are context-aware.&lt;/p&gt;

&lt;p&gt;Check out the slides at the top of this post for a brief intro tour.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HWX Ansible</title>
      <link>http://blog.streever.com/project/hwx-ansible/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hwx-ansible/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Filter Hive Compactions</title>
      <link>http://blog.streever.com/post/2019/filter-hive-compactions/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/filter-hive-compactions/</guid>
      <description>

&lt;p&gt;From Beeline or a standard JDBC client connected to Hive, compactions can be seen with the standard SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SHOW COMPACTIONS;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this method has a couple of problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;No Filtering&lt;/li&gt;
&lt;li&gt;Timestamps are hard to interpret&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Until additional functionality is available for this built in function, we can do the following.&lt;/p&gt;

&lt;p&gt;Add links to the Metastore DB tables and create custom views to review compaction details.  See &lt;a href=&#34;http://blog.streever.com/post/the-power-of-hive-jdbc-federation&#34;&gt;The Power of Hive JDBC Federation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-out-metadata-elements&#34;&gt;Build Out Metadata Elements&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;A word of caution here.  All the &amp;lsquo;extras&amp;rsquo; you create against the metastore DB can/may break with the next release.  This isn&amp;rsquo;t a supported method of accessing metadata.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t add &lt;em&gt;new&lt;/em&gt; tables to the current &lt;code&gt;sys.db&lt;/code&gt;.  That&amp;rsquo;s bad form and could cause issues for the platform during the next upgrade cycle, especially if there are naming conflicts.  Put it somewhere else!!&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE DATABASE IF NOT EXISTS custom_sys;

DROP TABLE IF EXISTS `custom_sys`.`completed_compactions`;
DROP TABLE IF EXISTS `custom_sys`.`compaction_queue`;

CREATE EXTERNAL TABLE `custom_sys`.`completed_compactions`(
	CC_ID bigint COMMENT &#39;from deserializer&#39;,
	CC_DATABASE string COMMENT &#39;from deserializer&#39;,
	CC_TABLE string COMMENT &#39;from deserializer&#39;,
	CC_PARTITION string COMMENT &#39;from deserializer&#39;,
	CC_STATE string COMMENT &#39;from deserializer&#39;,
	CC_TYPE string COMMENT &#39;from deserializer&#39;,
	CC_TBLPROPERTIES string COMMENT &#39;from deserializer&#39;,
	CC_WORKER_ID string COMMENT &#39;from deserializer&#39;,
	CC_START bigint COMMENT &#39;from deserializer&#39;,
	CC_END bigint COMMENT &#39;from deserializer&#39;,
	CC_RUN_AS string COMMENT &#39;from deserializer&#39;,
	CC_HIGHEST_WRITE_ID bigint COMMENT &#39;from deserializer&#39;,
	CC_META_INFO string COMMENT &#39;from deserializer&#39;,
	CC_HADOOP_JOB_ID string COMMENT &#39;from deserializer&#39;
)
 ROW FORMAT SERDE                                   
   &#39;org.apache.hive.storage.jdbc.JdbcSerDe&#39;         
 STORED BY                                          
   &#39;org.apache.hive.storage.jdbc.JdbcStorageHandler&#39;  
 WITH SERDEPROPERTIES (                             
   &#39;serialization.format&#39;=&#39;1&#39;)                      
 TBLPROPERTIES (                                    
   &#39;bucketing_version&#39;=&#39;2&#39;,                         
   &#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;,            
   &#39;hive.sql.query&#39;=&#39;SELECT CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_TBLPROPERTIES, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_WRITE_ID, CC_META_INFO, CC_HADOOP_JOB_ID FROM COMPLETED_COMPACTIONS&#39;);

CREATE EXTERNAL TABLE `custom_sys`.`compaction_queue`(
	CQ_ID bigint COMMENT &#39;from deserializer&#39;,
	CQ_DATABASE string COMMENT &#39;from deserializer&#39;,
	CQ_TABLE string COMMENT &#39;from deserializer&#39;,
	CQ_PARTITION string COMMENT &#39;from deserializer&#39;,
	CQ_STATE string COMMENT &#39;from deserializer&#39;,
	CQ_TYPE string COMMENT &#39;from deserializer&#39;,
	CQ_TBLPROPERTIES string COMMENT &#39;from deserializer&#39;,
	CQ_WORKER_ID string COMMENT &#39;from deserializer&#39;,
	CQ_START bigint COMMENT &#39;from deserializer&#39;,
	CQ_RUN_AS string COMMENT &#39;from deserializer&#39;,
	CQ_HIGHEST_WRITE_ID bigint COMMENT &#39;from deserializer&#39;,
	CQ_META_INFO string COMMENT &#39;from deserializer&#39;,
	CQ_HADOOP_JOB_ID string COMMENT &#39;from deserializer&#39;
)
 ROW FORMAT SERDE                                   
   &#39;org.apache.hive.storage.jdbc.JdbcSerDe&#39;         
 STORED BY                                          
   &#39;org.apache.hive.storage.jdbc.JdbcStorageHandler&#39;  
 WITH SERDEPROPERTIES (                             
   &#39;serialization.format&#39;=&#39;1&#39;)                      
 TBLPROPERTIES (                                    
   &#39;bucketing_version&#39;=&#39;2&#39;,                         
   &#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;,            
   &#39;hive.sql.query&#39;=&#39;SELECT CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_TBLPROPERTIES, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_WRITE_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID FROM COMPACTION_QUEUE&#39;);

DROP VIEW IF EXISTS `custom_sys`.`compactions`;

-- TODO: Handle / Show Aborted Transactions (maybe) I think txn data may be transient...
CREATE VIEW IF NOT EXISTS `custom_sys`.`compactions` AS
SELECT CC_ID AS id,
       CC_DATABASE AS `database`,
       CC_TABLE AS `table`,
       CC_PARTITION AS `partition`,
       CASE CC_STATE
           WHEN &#39;s&#39; THEN &#39;SUCCEEDED&#39;
           WHEN &#39;a&#39; THEN &#39;ATTEMPTED&#39;
           WHEN &#39;f&#39; THEN &#39;FAILED&#39;
       END AS STATE,
       CASE CC_TYPE
           WHEN &#39;a&#39; THEN &#39;MAJOR&#39;
           WHEN &#39;i&#39; THEN &#39;MINOR&#39;
       END AS TYPE,
       CC_TBLPROPERTIES AS tblproperties,
       CC_WORKER_ID AS worker_id,
       to_utc_timestamp(CC_START,&#39;UTC&#39;) AS `start`,
       to_utc_timestamp(CC_END, &#39;UTC&#39;) AS `end`,
       CC_RUN_AS AS run_as,
       CC_HIGHEST_WRITE_ID AS highest_write_id,
       CC_META_INFO AS meta_info,
       CC_HADOOP_JOB_ID AS hadoop_job_id
FROM `custom_sys`.`completed_compactions`
UNION ALL
SELECT CQ_ID AS id,
       CQ_DATABASE AS `database`,
       CQ_TABLE AS `table`,
       CQ_PARTITION AS `partition`,
       CASE CQ_STATE
           WHEN &#39;i&#39; THEN &#39;INITIATED&#39;
           WHEN &#39;w&#39; THEN &#39;WORKING&#39;
           WHEN &#39;r&#39; THEN &#39;READY_FOR_CLEANING&#39;
       END AS `state`,
       CASE CQ_TYPE
           WHEN &#39;a&#39; THEN &#39;MAJOR&#39;
           WHEN &#39;i&#39; THEN &#39;MINOR&#39;
       END AS `type`,
       CQ_TBLPROPERTIES AS tblproperties,
       CQ_WORKER_ID AS worker_id,
       to_utc_timestamp(CQ_START, &#39;UTC&#39;) AS `start`,
       NULL AS `end`,
               CQ_RUN_AS AS run_as,
               CQ_HIGHEST_WRITE_ID AS highest_write_id,
               CQ_META_INFO AS meta_info,
               CQ_HADOOP_JOB_ID AS hadoop_job_id
FROM `custom_sys`.`compaction_queue`;

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;see-the-last-10-compaction-events&#34;&gt;See the last 10 compaction events&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
	`database`, `table`, `partition`, `state`, `type`, `start`, `end`, hadoop_job_id 
FROM
	`custom_sys`.`compactions` 
ORDER BY `start` DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;show-the-last-10-failed-compaction-event&#34;&gt;Show the last 10 failed compaction event&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- `state` options are &#39;SUCCEEDED&#39;, &#39;ATTEMPTED&#39;, &#39;FAILED&#39;, &#39;INITIATED&#39;, &#39;WORKING&#39;, and &#39;READY_FOR_CLEANING&#39;
-- `type` options are &#39;MAJOR&#39; and &#39;MINOR&#39;
select 
`database`, `table`, `partition`, `state`, `type`, `start`, `end` 
FROM
	`custom_sys`.`compactions` 
WHERE `state` = &#39;FAILED&#39; 
ORDER BY `start` DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI</title>
      <link>http://blog.streever.com/project/hadoop-cli/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hadoop-cli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hive LLAP Calculator</title>
      <link>http://blog.streever.com/project/hive-llap-calculator/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hive-llap-calculator/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Power of Hive JDBC Federation</title>
      <link>http://blog.streever.com/post/2019/jdbc-federation/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/jdbc-federation/</guid>
      <description>

&lt;p&gt;Hive jdbc-federation is a powerful mechanism to include external sources in your hive ecosystem.   Apache Software Foundation has excellent docs detailing this feature referred to as the &lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/JdbcStorageHandler&#34; target=&#34;_blank&#34;&gt;JDBCStorageHandler&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Interesting points about this StorageHandler include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Securing Passwords - &lt;strong&gt;Protects  passwords using a &amp;lsquo;jceks&amp;rsquo; file stored on &amp;lsquo;hdfs&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Partitioning - &lt;strong&gt;Could be used as an alternate to SQOOP for importing data to Hive&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;more-hive-metadata&#34;&gt;More Hive Metadata!&lt;/h2&gt;

&lt;p&gt;Hive Metadata, previously only available via &lt;code&gt;WebHCat&lt;/code&gt; which has been removed, can be retrieved through hive&amp;rsquo;s &lt;code&gt;sys&lt;/code&gt; database.   And the &lt;code&gt;sys&lt;/code&gt; db is actually using the JDBCStorageHandler with a special tblproperty &lt;code&gt;&#39;hive.sql.database.type&#39;=&#39;METASTORE&#39;&lt;/code&gt; used with the storage handler and &lt;code&gt;&#39;hive.sql.query&#39;=&#39;SELECT ... FROM ...&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve used this technique to expose tables that support &lt;code&gt;COMPACTION&lt;/code&gt;, &lt;code&gt;LOCKS&lt;/code&gt;, and &lt;code&gt;TRANSACTIONS&lt;/code&gt; in order to fill some gaps with the current admin functions.   See the examples below for details.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;A word of caution here.  All the &amp;lsquo;extras&amp;rsquo; you create against the metastore DB can/may break with the next release.  This isn&amp;rsquo;t a supported method of accessing metadata.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t add &lt;em&gt;new&lt;/em&gt; tables to the current &lt;code&gt;sys.db&lt;/code&gt;.  That&amp;rsquo;s bad form and could cause issues for the platform during the next upgrade cycle, especially if there are naming conflicts.  Put it somewhere else!!&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI Walk-Through</title>
      <link>http://blog.streever.com/slides/hadoop-cli/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/slides/hadoop-cli/</guid>
      <description>

&lt;h2 id=&#34;the-hadoop-cli&#34;&gt;The Hadoop CLI&lt;/h2&gt;

&lt;p&gt;A quick tour of the installation and basic usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/img/hadoopcli_icon.png&#34; alt=&#34;Hadoop CLI&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;slide-controls&#34;&gt;Slide Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt(Option) + Click&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-is-the-hadoop-cli&#34;&gt;What is the Hadoop-CLI?&lt;/h2&gt;

&lt;p&gt;It is the missing CLI for HDFS.&lt;/p&gt;

&lt;p&gt;Launch a session and benefit from an interactive CLI experience (like your local filesyste) against HDFS.&lt;/p&gt;

&lt;p&gt;It does &amp;lsquo;tab&amp;rsquo; completion, has location &amp;lsquo;context&amp;rsquo;, supports &amp;lsquo;most&amp;rsquo; hdfs commands, and has a few &amp;lsquo;nice surprises&amp;rsquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;where-to-find-it&#34;&gt;Where to Find it&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli&#34; target=&#34;_blank&#34;&gt;On Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Pre-built Releases&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Download the latest &lt;code&gt;tar.gz&lt;/code&gt; from &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Releases&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;See the &amp;lsquo;Assets&amp;rsquo; associated with a release and downland.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation as &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;sudo&lt;/code&gt; to allow it to create and install global links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget &amp;lt;release-link-from-asset-section&amp;gt;
tar xvfz hadoop.cli-&amp;lt;version&amp;gt;-3.1.tar.gz
cd hadoop-cli-3.1
sudo ./setup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it.  I&amp;rsquo;m making an assumption you have &lt;code&gt;java 8&lt;/code&gt; on the host.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;start-up&#34;&gt;Start-up&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd # to return to your home directory
hadoopcli # It will be in the global path for most standard configurations

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;hadoopcli-startup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;let-s-try-a-simple-command&#34;&gt;Let&amp;rsquo;s try a simple command&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how way didn&amp;rsquo;t specify a full path (or any path) for &lt;code&gt;ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;and-with-a-relative-reference&#34;&gt;And with a &amp;lsquo;relative&amp;rsquo; reference&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;du -h data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path for &lt;code&gt;du -h&lt;/code&gt; is relative (no preceeding &amp;lsquo;/&amp;rsquo;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;context-aware&#34;&gt;Context Aware&lt;/h2&gt;

&lt;p&gt;Both HDFS and the Local Filesystem are accessible.  HDFS is considered the primary, meaning that standard commands will be applied to it.&lt;/p&gt;

&lt;p&gt;Context is tracked for each file system, so commands without a preceeding &amp;lsquo;/&amp;rsquo; will append to the current location.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;need-help&#34;&gt;Need Help&lt;/h2&gt;

&lt;p&gt;Get a complete command reference with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get command help with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help &amp;lt;command&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details&#34;&gt;Environment Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This is a valid HDFS client, using Hadoop-HDFS libraries.  All permissions are applied to the user as if they were using &lt;code&gt;hdfs dfs -&amp;lt;cmd&amp;gt; ...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;By default, the application will look for configurations in &lt;code&gt;/etc/hadoop/conf&lt;/code&gt;.  It requires &lt;code&gt;hdfs-site.xml&lt;/code&gt; and &lt;code&gt;core-site.xml&lt;/code&gt; from your cluster.&lt;/li&gt;
&lt;li&gt;Security is applied the same way as if using &lt;code&gt;hdfs dfs&lt;/code&gt;.  Via &amp;lsquo;os username&amp;rsquo; or &amp;lsquo;kerberos&amp;rsquo;.  Get a Kerberos ticket &lt;em&gt;before&lt;/em&gt; starting the cli.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details-cont&#34;&gt;Environment Details (cont.)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Support for alternative configs(location) via commandline parameter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoopcli --config &amp;lt;alt-location&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;h3 id=&#34;i-m-always-looking-for-feedback-to-make-it-better&#34;&gt;I&amp;rsquo;m always looking for feedback to make it better.&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you find an issue, log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you think of an enhancement, please log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s Your Last Block!</title>
      <link>http://blog.streever.com/post/2018/its-your-last-block/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2018/its-your-last-block/</guid>
      <description>&lt;p&gt;In a few other articles, I&amp;rsquo;ll talk about bringing hosts down to facilitate upgrades.&lt;/p&gt;

&lt;p&gt;If you do this at rack at a time, it may also lead to data loss if any other drive in the system fails during the operation.  That&amp;rsquo;s because the rules for replication when setting to 3 will place the second and third blocks on the same rack.  The first block becomes the only viable source.  If a drive fails, that contains this single block, the file will become corrupt.  Restoring the data directories will fix the block issue, restoring the integrity of the file.  But, jobs that depend on this file during the time it was corrupt/unavailable will fail.&lt;/p&gt;

&lt;p&gt;If critical jobs can&amp;rsquo;t handle this risk, identify the supporting files/directories and increase the replication factor for the duration of the upgrade process to ensure you have adequate replicas available.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
