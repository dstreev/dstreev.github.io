<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>navigation | David W. Streever</title>
    <link>http://blog.streever.com/categories/navigation/</link>
      <atom:link href="http://blog.streever.com/categories/navigation/index.xml" rel="self" type="application/rss+xml" />
    <description>navigation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019</copyright><lastBuildDate>Fri, 18 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://blog.streever.com/img/icon-192.png</url>
      <title>navigation</title>
      <link>http://blog.streever.com/categories/navigation/</link>
    </image>
    
    <item>
      <title>Hadoop CLI &#39;lsp&#39; Function</title>
      <link>http://blog.streever.com/post/hadoop-cli/lsp/</link>
      <pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/lsp/</guid>
      <description>

&lt;p&gt;Like &amp;lsquo;ls&amp;rsquo;, you can fetch many details about a file.  But with this, you can also add information about the file that includes:
- Block Size
- Access Time
- Ratio of File Size to Block
- Datanode information for the files blocks (Host and Block Id)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lsp&lt;/code&gt; can be used to search hdfs, similar to the &lt;code&gt;find&lt;/code&gt; linux program.  Although the syntax is a bit different.  Use options &lt;code&gt;-F&lt;/code&gt;,&lt;code&gt;-Fe&lt;/code&gt;, and &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lsp&lt;/code&gt; can be used to output a formatted row for files and directories using the &lt;code&gt;-f&lt;/code&gt; option.  When the &lt;code&gt;datanode_info&lt;/code&gt; option is specified, the output will contain details for each replicated block of a file.&lt;/p&gt;

&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;usage: lsp [OPTION ...] [ARGS ...]
Options:
 -c,--comment &amp;lt;comment&amp;gt;                  Add comment to output
 -d,--maxDepth &amp;lt;maxDepth&amp;gt;                Depth of Recursion (default 5),
                                         use &#39;-1&#39; for unlimited
 -do,--dir-only                          Show Directories Only
 -f,--format &amp;lt;output-format&amp;gt;             Comma separated list of one or
                                         more:
                                         permissions_long,replication,user
                                         ,group,size,block_size,ratio,mod,
                                         access,path,file,datanode_info
                                         (default all of the above)
 -F,--filter &amp;lt;filter&amp;gt;                    Regex Filter of Content. Can be
                                         &#39;Quoted&#39;
 -Fe,--filter-element &amp;lt;filter element&amp;gt;   Filter on &#39;element&#39;.  One of
                                         &#39;--format&#39;
 -i,--invisible                          Process Invisible
                                         Files/Directories
 -n,--newline &amp;lt;newline&amp;gt;                  New Line
 -o,--output &amp;lt;output directory&amp;gt;          Output Directory (HDFS) (default
                                         System.out)
 -R,--recursive                          Process Path Recursively
 -r,--relative                           Show Relative Path Output
 -s,--separator &amp;lt;separator&amp;gt;              Field Separator
 -sp,--show-parent                       For Test, show parent
 -t,--test                               Test for existence
 -v,--invert                             Invert Regex Filter of Content
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;actions&#34;&gt;Actions&lt;/h2&gt;

&lt;h3 id=&#34;recursion&#34;&gt;Recursion&lt;/h3&gt;

&lt;p&gt;Use the &lt;code&gt;-R&lt;/code&gt; to recurse through directories.  Use the &lt;code&gt;-d&lt;/code&gt; option to specify the depth of the recursion.  The default is 5.  Use -1 for no-limit, but be careful because this could iterate through the whole filesystem.  And that&amp;rsquo;s not productive on &amp;lsquo;large&amp;rsquo; filesystem.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Control the depth of the recursion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -d 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;filtering-and-output&#34;&gt;Filtering and Output&lt;/h3&gt;

&lt;p&gt;Find files matching a certain pattern, recursively from path context. The &lt;code&gt;-F&lt;/code&gt; option takes a &amp;lsquo;regex&amp;rsquo; expression.  By default, the &amp;lsquo;path&amp;rsquo; &lt;code&gt;-f&lt;/code&gt; option is searched.  If you&amp;rsquo;d like to search another element use &lt;code&gt;-Fe&lt;/code&gt; and specify one of the valid &lt;code&gt;-f&lt;/code&gt; options.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output will include the standard output &lt;code&gt;-f&lt;/code&gt;, which maybe quite verbose.&lt;/p&gt;

&lt;p&gt;Limit the result output like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.* -f path
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Try using the &lt;code&gt;-do&lt;/code&gt; option to list ONLY directories.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -F .*trans.* -f path -do
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-comments&#34;&gt;Using Comments&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;-c&lt;/code&gt; option will prepend any output with the comment.  It&amp;rsquo;s a great way to drive scripts from the results.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -R -c &#39;count -h&#39; -F .*trans.* -f path -do
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;how-about-an-inverted-search&#34;&gt;How about an Inverted Search&lt;/h3&gt;

&lt;p&gt;There are times you want to find files that do &lt;em&gt;NOT&lt;/em&gt; match a pattern. Use the &lt;code&gt;-v&lt;/code&gt; option to reverse match on the filter.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lsp -F *.trans.* -v
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;default-output&#34;&gt;Default Output&lt;/h3&gt;

&lt;p&gt;When not argument is specified, it will use the current directory.&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Using the default format, output a listing to the files in `/user/dstreev/perf` to `/tmp/test.out`
lsp -o /tmp/test.out /user/dstreev/perf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output with the default format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;permissions_long,replication,user,group,size,block_size,ratio,mod,access,path,datanode_info
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   rw-------,3,dstreev,hdfs,429496700,134217728,3.200,2015-10-24 12:26:39.689,2015-10-24 12:23:27.406,/user/dstreev/perf/teragen_27/part-m-00004,10.0.0.166,d2.hdp.local,blk_1073747900
   rw-------,3,dstreev,hdfs,429496700,134217728,3.200,2015-10-24 12:26:39.689,2015-10-24 12:23:27.406,/user/dstreev/perf/teragen_27/part-m-00004,10.0.0.167,d3.hdp.local,blk_1073747900
   rw-------,3,dstreev,hdfs,33,134217728,2.459E-7,2015-10-24 12:27:09.134,2015-10-24 12:27:06.560,/user/dstreev/perf/terasort_27/_partition.lst,10.0.0.166,d2.hdp.local,blk_1073747909
   rw-------,3,dstreev,hdfs,33,134217728,2.459E-7,2015-10-24 12:27:09.134,2015-10-24 12:27:06.560,/user/dstreev/perf/terasort_27/_partition.lst,10.0.0.167,d3.hdp.local,blk_1073747909
   rw-------,1,dstreev,hdfs,543201700,134217728,4.047,2015-10-24 12:29:28.706,2015-10-24 12:29:20.882,/user/dstreev/perf/terasort_27/part-r-00002,10.0.0.167,d3.hdp.local,blk_1073747920
   rw-------,1,dstreev,hdfs,543201700,134217728,4.047,2015-10-24 12:29:28.706,2015-10-24 12:29:20.882,/user/dstreev/perf/terasort_27/part-r-00002,10.0.0.167,d3.hdp.local,blk_1073747921
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the file in HDFS, you can build a &lt;a href=&#34;./src/main/hive/lsp.ddl&#34;&gt;hive table&lt;/a&gt; on top of it to do some analysis.  One of the reasons I created this was to be able to review a directory used by some process and get a baring on the file construction and distribution across the cluster.&lt;/p&gt;

&lt;h4 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;The ratio can be used to identify files that are below the block size (small files).&lt;/li&gt;
&lt;li&gt;With the Datanode information, you can determine if a dataset is hot-spotted on a cluster.  All you need is a full list of hosts to join the results with.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI - Intro</title>
      <link>http://blog.streever.com/post/hadoop-cli/intro/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/intro/</guid>
      <description>&lt;p&gt;Working with Hadoop is much like working with a terminal application, as most everything you do with Hadoop is via the terminal.  If you want to launch a MapReduce job, do it from the terminal.  If you wanted to explore HDFS, run a command from the terminal.&lt;/p&gt;

&lt;p&gt;Working with the Hadoop Distributed File System (HDFS) should be like working with any other file system, at least when you&amp;rsquo;re in the terminal.  Unfortunately, it&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;To do anything with HDFS, launch the command line application &lt;code&gt;hdfs&lt;/code&gt;. &lt;code&gt;hdfs&lt;/code&gt; has several sub-applications for controlling various interactions with &amp;lsquo;HDFS&amp;rsquo;.  My focus is to make the &lt;code&gt;dfs&lt;/code&gt; sub-application more amenable for &amp;lsquo;any&amp;rsquo; user.  Running &lt;code&gt;hdfs dfs -...&lt;/code&gt; for every query isn&amp;rsquo;t the experience that leaves you wanting more.  And honestly, that&amp;rsquo;s been one of Hadoop&amp;rsquo;s issues with user acceptance.  It&amp;rsquo;s an expert system, and every native interface reinforces that 10 fold.&lt;/p&gt;

&lt;p&gt;So there you have it, we&amp;rsquo;ve got a gap.  We should be able to interact with &amp;lsquo;HDFS&amp;rsquo; the same way we interact with the file system on our local computer.&lt;/p&gt;

&lt;p&gt;Five years ago, I discovered the fledgling &amp;lsquo;first&amp;rsquo; iteration of this program written by Taylor Goetz, Apache Storm PMC Chair.  The concept was great but needed some TLC.  So I forked it and have been building and improving it ever since.&lt;/p&gt;

&lt;p&gt;Finally, at least from a terminal perspective, you have the same type of interaction model with HDFS that you have with your local file system.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s not just for basic commands.  Many of the standard HDFS command-line tools are embedded right in the interface.  I&amp;rsquo;ve added scripting, Standard IN, some new commands, and sessions that are context-aware.&lt;/p&gt;

&lt;p&gt;Check out the slides at the top of this post for a brief intro tour.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI</title>
      <link>http://blog.streever.com/project/hadoop-cli/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/project/hadoop-cli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hadoop CLI Walk-Through</title>
      <link>http://blog.streever.com/slides/hadoop-cli/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/slides/hadoop-cli/</guid>
      <description>

&lt;h2 id=&#34;the-hadoop-cli&#34;&gt;The Hadoop CLI&lt;/h2&gt;

&lt;p&gt;A quick tour of the installation and basic usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/img/hadoopcli_icon.png&#34; alt=&#34;Hadoop CLI&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;slide-controls&#34;&gt;Slide Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt(Option) + Click&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-is-the-hadoop-cli&#34;&gt;What is the Hadoop-CLI?&lt;/h2&gt;

&lt;p&gt;It is the missing CLI for HDFS.&lt;/p&gt;

&lt;p&gt;Launch a session and benefit from an interactive CLI experience (like your local filesyste) against HDFS.&lt;/p&gt;

&lt;p&gt;It does &amp;lsquo;tab&amp;rsquo; completion, has location &amp;lsquo;context&amp;rsquo;, supports &amp;lsquo;most&amp;rsquo; hdfs commands, and has a few &amp;lsquo;nice surprises&amp;rsquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;where-to-find-it&#34;&gt;Where to Find it&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli&#34; target=&#34;_blank&#34;&gt;On Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Pre-built Releases&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Download the latest &lt;code&gt;tar.gz&lt;/code&gt; from &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Releases&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;See the &amp;lsquo;Assets&amp;rsquo; associated with a release and downland.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation as &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;sudo&lt;/code&gt; to allow it to create and install global links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget &amp;lt;release-link-from-asset-section&amp;gt;
tar xvfz hadoop.cli-&amp;lt;version&amp;gt;-3.1.tar.gz
cd hadoop-cli-3.1
sudo ./setup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it.  I&amp;rsquo;m making an assumption you have &lt;code&gt;java 8&lt;/code&gt; on the host.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;start-up&#34;&gt;Start-up&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd # to return to your home directory
hadoopcli # It will be in the global path for most standard configurations

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;hadoopcli-startup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;let-s-try-a-simple-command&#34;&gt;Let&amp;rsquo;s try a simple command&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how way didn&amp;rsquo;t specify a full path (or any path) for &lt;code&gt;ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;and-with-a-relative-reference&#34;&gt;And with a &amp;lsquo;relative&amp;rsquo; reference&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;du -h data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path for &lt;code&gt;du -h&lt;/code&gt; is relative (no preceeding &amp;lsquo;/&amp;rsquo;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;context-aware&#34;&gt;Context Aware&lt;/h2&gt;

&lt;p&gt;Both HDFS and the Local Filesystem are accessible.  HDFS is considered the primary, meaning that standard commands will be applied to it.&lt;/p&gt;

&lt;p&gt;Context is tracked for each file system, so commands without a preceeding &amp;lsquo;/&amp;rsquo; will append to the current location.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;need-help&#34;&gt;Need Help&lt;/h2&gt;

&lt;p&gt;Get a complete command reference with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get command help with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help &amp;lt;command&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details&#34;&gt;Environment Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This is a valid HDFS client, using Hadoop-HDFS libraries.  All permissions are applied to the user as if they were using &lt;code&gt;hdfs dfs -&amp;lt;cmd&amp;gt; ...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;By default, the application will look for configurations in &lt;code&gt;/etc/hadoop/conf&lt;/code&gt;.  It requires &lt;code&gt;hdfs-site.xml&lt;/code&gt; and &lt;code&gt;core-site.xml&lt;/code&gt; from your cluster.&lt;/li&gt;
&lt;li&gt;Security is applied the same way as if using &lt;code&gt;hdfs dfs&lt;/code&gt;.  Via &amp;lsquo;os username&amp;rsquo; or &amp;lsquo;kerberos&amp;rsquo;.  Get a Kerberos ticket &lt;em&gt;before&lt;/em&gt; starting the cli.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details-cont&#34;&gt;Environment Details (cont.)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Support for alternative configs(location) via commandline parameter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoopcli --config &amp;lt;alt-location&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;h3 id=&#34;i-m-always-looking-for-feedback-to-make-it-better&#34;&gt;I&amp;rsquo;m always looking for feedback to make it better.&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you find an issue, log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you think of an enhancement, please log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI for HDFS Directory Cleanup</title>
      <link>http://blog.streever.com/post/2019/cleaning-up-hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/2019/cleaning-up-hdfs/</guid>
      <description>

&lt;p&gt;Refer to the &amp;lsquo;project&amp;rsquo; and &amp;lsquo;slides&amp;rsquo; above for details on &amp;lsquo;Hadoop CLI&amp;rsquo;.  See the &amp;lsquo;related&amp;rsquo; posts at the end of this post for other information.&lt;/p&gt;

&lt;p&gt;With pipelining in &amp;lsquo;hadoopcli&amp;rsquo; we can combine search functions with cleanup functions.  So what use to take hours to prep and setup, can be accomplished in seconds.&lt;/p&gt;

&lt;h3 id=&#34;recommendation&#34;&gt;Recommendation&lt;/h3&gt;

&lt;p&gt;These operations are &amp;lsquo;destructive&amp;rsquo;, so take precautions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Test the pipeline results before call a destructive operation.&lt;/li&gt;
&lt;li&gt;Create an &amp;lsquo;hdfs snapshot&amp;rsquo; of the target base directory in case something doesn&amp;rsquo;t go as planned. See &lt;a href=&#34;http://blog.streever.com/terms&#34;&gt;Terms&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sample-use-case&#34;&gt;Sample Use case&lt;/h3&gt;

&lt;p&gt;Cleaning up hive directories left behind by some hive bugs in &amp;lsquo;stat&amp;rsquo; collection or &lt;code&gt;insert overwrite&lt;/code&gt; operations.&lt;/p&gt;

&lt;p&gt;In the &amp;lsquo;hadoopcli&amp;rsquo; application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Let&#39;s test the command.
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path
/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db/catalog_sales/.hive-staging_hive_2019-03-07_11-43-26_995_5066067470456199362-32
/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db/catalog_sales/.hive-staging_hive_2019-03-07_12-26-53_012_3155309838210900907-4

# Now we see the results, let&#39;s action on it with a pipe to rm -r -f
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/
tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path | rm -r -f

# That would&#39;ve deleted the directories listed above.
# Now let&#39;s try it again, without the pipe and validate the directories are gone.
REMOTE: hdfs://HOME90/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_300.db		LOCAL: file:/home/dstreev
hdfs-cli:$ lsp -R -i -do -F .*.hive-staging.* -f path


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few notes about this.&lt;br /&gt;
- Since the &amp;lsquo;.hive-staging.*&amp;rsquo; directories are prefixed with a &amp;lsquo;.&amp;rsquo; they&amp;rsquo;re considered &lt;em&gt;invisible&lt;/em&gt;.  So use the &lt;code&gt;-i&lt;/code&gt; option to find them.
- Use the &lt;code&gt;-R&lt;/code&gt; to support recursion into the directory tree.
- Output just the &amp;lsquo;path&amp;rsquo; with the &lt;code&gt;-f&lt;/code&gt; option because the whole output line is what will be passed into the &lt;code&gt;rm -r -f&lt;/code&gt; command.
- Use the &lt;code&gt;-do&lt;/code&gt; &amp;lsquo;directory-only&amp;rsquo; option to output just directories.&lt;/p&gt;

&lt;p&gt;If you want to do a little more investigation before &lt;code&gt;rm&lt;/code&gt;, Try piping to &lt;code&gt;count -h&lt;/code&gt; for a count and size of Directories and Files.&lt;/p&gt;

&lt;h3 id=&#34;other-use-cases&#34;&gt;Other Use Cases&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Cleaning up Job History&lt;/li&gt;
&lt;li&gt;Cleaning up Spark History&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;last-note&#34;&gt;Last Note&lt;/h2&gt;

&lt;p&gt;Doing things in mass is great for productivity.  But equally as dangerous when things go wrong.  Protect yourself!!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
