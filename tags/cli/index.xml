<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cli | David W. Streever</title>
    <link>http://blog.streever.com/tags/cli/</link>
      <atom:link href="http://blog.streever.com/tags/cli/index.xml" rel="self" type="application/rss+xml" />
    <description>cli</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019</copyright><lastBuildDate>Thu, 17 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://blog.streever.com/img/icon-192.png</url>
      <title>cli</title>
      <link>http://blog.streever.com/tags/cli/</link>
    </image>
    
    <item>
      <title>Hadoop CLI - Intro</title>
      <link>http://blog.streever.com/post/hadoop-cli/intro/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/post/hadoop-cli/intro/</guid>
      <description>&lt;p&gt;Working with Hadoop is much like working with a terminal application, as most everything you do with Hadoop is via the terminal.  If you want to launch a MapReduce job, do it from the terminal.  If you wanted to explore HDFS, run a command from the terminal.&lt;/p&gt;

&lt;p&gt;Working with the Hadoop Distributed File System (HDFS) should be like working with any other file system, at least when you&amp;rsquo;re in the terminal.  Unfortunately, it&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;To do anything with HDFS, launch the command line application &lt;code&gt;hdfs&lt;/code&gt;. &lt;code&gt;hdfs&lt;/code&gt; has several sub-applications for controlling various interactions with &amp;lsquo;HDFS&amp;rsquo;.  My focus is to make the &lt;code&gt;dfs&lt;/code&gt; sub-application more amenable for &amp;lsquo;any&amp;rsquo; user.  Running &lt;code&gt;hdfs dfs -...&lt;/code&gt; for every query isn&amp;rsquo;t the experience that leaves you wanting more.  And honestly, that&amp;rsquo;s been one of Hadoop&amp;rsquo;s issues with user acceptance.  It&amp;rsquo;s an expert system, and every native interface reinforces that 10 fold.&lt;/p&gt;

&lt;p&gt;So there you have it, we&amp;rsquo;ve got a gap.  We should be able to interact with &amp;lsquo;HDFS&amp;rsquo; the same way we interact with the file system on our local computer.&lt;/p&gt;

&lt;p&gt;Five years ago, I discovered the fledgling &amp;lsquo;first&amp;rsquo; iteration of this program written by Taylor Goetz, Apache Storm PMC Chair.  The concept was great but needed some TLC.  So I forked it and have been building and improving it ever since.&lt;/p&gt;

&lt;p&gt;Finally, at least from a terminal perspective, you have the same type of interaction model with HDFS that you have with your local file system.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s not just for basic commands.  Many of the standard HDFS command-line tools are embedded right in the interface.  I&amp;rsquo;ve added scripting, Standard IN, some new commands, and sessions that are context-aware.&lt;/p&gt;

&lt;p&gt;Check out the slides at the top of this post for a brief intro tour.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop CLI Walk-Through</title>
      <link>http://blog.streever.com/slides/hadoop-cli/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://blog.streever.com/slides/hadoop-cli/</guid>
      <description>

&lt;h2 id=&#34;the-hadoop-cli&#34;&gt;The Hadoop CLI&lt;/h2&gt;

&lt;p&gt;A quick tour of the installation and basic usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.streever.com/img/hadoopcli_icon.png&#34; alt=&#34;Hadoop CLI&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;slide-controls&#34;&gt;Slide Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt(Option) + Click&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-is-the-hadoop-cli&#34;&gt;What is the Hadoop-CLI?&lt;/h2&gt;

&lt;p&gt;It is the missing CLI for HDFS.&lt;/p&gt;

&lt;p&gt;Launch a session and benefit from an interactive CLI experience (like your local filesyste) against HDFS.&lt;/p&gt;

&lt;p&gt;It does &amp;lsquo;tab&amp;rsquo; completion, has location &amp;lsquo;context&amp;rsquo;, supports &amp;lsquo;most&amp;rsquo; hdfs commands, and has a few &amp;lsquo;nice surprises&amp;rsquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;where-to-find-it&#34;&gt;Where to Find it&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli&#34; target=&#34;_blank&#34;&gt;On Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Pre-built Releases&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Download the latest &lt;code&gt;tar.gz&lt;/code&gt; from &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/releases&#34; target=&#34;_blank&#34;&gt;Releases&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;See the &amp;lsquo;Assets&amp;rsquo; associated with a release and downland.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation as &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;sudo&lt;/code&gt; to allow it to create and install global links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget &amp;lt;release-link-from-asset-section&amp;gt;
tar xvfz hadoop.cli-&amp;lt;version&amp;gt;-3.1.tar.gz
cd hadoop-cli-3.1
sudo ./setup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it.  I&amp;rsquo;m making an assumption you have &lt;code&gt;java 8&lt;/code&gt; on the host.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;start-up&#34;&gt;Start-up&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd # to return to your home directory
hadoopcli # It will be in the global path for most standard configurations

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;hadoopcli-startup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;let-s-try-a-simple-command&#34;&gt;Let&amp;rsquo;s try a simple command&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how way didn&amp;rsquo;t specify a full path (or any path) for &lt;code&gt;ls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;and-with-a-relative-reference&#34;&gt;And with a &amp;lsquo;relative&amp;rsquo; reference&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;du -h data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path for &lt;code&gt;du -h&lt;/code&gt; is relative (no preceeding &amp;lsquo;/&amp;rsquo;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;step_02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;context-aware&#34;&gt;Context Aware&lt;/h2&gt;

&lt;p&gt;Both HDFS and the Local Filesystem are accessible.  HDFS is considered the primary, meaning that standard commands will be applied to it.&lt;/p&gt;

&lt;p&gt;Context is tracked for each file system, so commands without a preceeding &amp;lsquo;/&amp;rsquo; will append to the current location.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;need-help&#34;&gt;Need Help&lt;/h2&gt;

&lt;p&gt;Get a complete command reference with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get command help with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help &amp;lt;command&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details&#34;&gt;Environment Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This is a valid HDFS client, using Hadoop-HDFS libraries.  All permissions are applied to the user as if they were using &lt;code&gt;hdfs dfs -&amp;lt;cmd&amp;gt; ...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;By default, the application will look for configurations in &lt;code&gt;/etc/hadoop/conf&lt;/code&gt;.  It requires &lt;code&gt;hdfs-site.xml&lt;/code&gt; and &lt;code&gt;core-site.xml&lt;/code&gt; from your cluster.&lt;/li&gt;
&lt;li&gt;Security is applied the same way as if using &lt;code&gt;hdfs dfs&lt;/code&gt;.  Via &amp;lsquo;os username&amp;rsquo; or &amp;lsquo;kerberos&amp;rsquo;.  Get a Kerberos ticket &lt;em&gt;before&lt;/em&gt; starting the cli.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;environment-details-cont&#34;&gt;Environment Details (cont.)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Support for alternative configs(location) via commandline parameter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoopcli --config &amp;lt;alt-location&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;h3 id=&#34;i-m-always-looking-for-feedback-to-make-it-better&#34;&gt;I&amp;rsquo;m always looking for feedback to make it better.&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you find an issue, log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you think of an enhancement, please log it &lt;a href=&#34;https://github.com/dstreev/hadoop-cli/issues&#34; target=&#34;_blank&#34;&gt;on my project issues page&lt;/a&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
